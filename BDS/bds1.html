<!DOCTYPE html>
<!-- saved from url=(0105)https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-1/145258 -->
<html xmlns:fb="http://ogp.me/ns/fb#" lang="en-gb" dir="ltr" class="secondary-14px wf-proximanova-n7-active wf-proximanova-i7-active wf-proximanova-n4-active wf-raleway-n1-active wf-raleway-n7-active wf-raleway-n4-active wf-raleway-n5-active wf-raleway-n3-active wf-raleway-n8-active wf-raleway-n9-active wf-raleway-n2-active wf-raleway-n6-active wf-proximanova-i4-active wf-active"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><meta name="format-detection" content="telephone=no"><script type="text/javascript" src="./bds1_files/display.js.下载"></script><script type="text/javascript" src="./bds1_files/pro"></script><script type="text/javascript" src="./bds1_files/l.js.下载"></script><script type="text/javascript" src="./bds1_files/l.js(1).下载"></script><script type="text/javascript" src="./bds1_files/l.js(2).下载"></script><script type="text/javascript" src="./bds1_files/l.js(3).下载"></script><script type="text/javascript" src="./bds1_files/pageload"></script><script type="text/javascript" async="" src="./bds1_files/analytics.js.下载"></script><script type="text/javascript" async="" src="./bds1_files/atatus.js.下载"></script><script src="./bds1_files/fMy5LNtdDqis6adCpEbCXQHA47I.js.下载"></script><script src="./bds1_files/js"></script><link rel="canonical" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-1/145258"><link rel="stylesheet" type="text/css" href="./bds1_files/bc-course.min_092917.css"><link rel="stylesheet" type="text/css" href="./bds1_files/bc-style-092917.css"><!--[if lt IE 9]> <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script> <script src="//css3-mediaqueries-js.googlecode.com/svn/trunk/css3-mediaqueries.js"></script> <![endif]--><link rel="shortcut icon" href="https://www.braincert.com/images/favicon.ico"> <script type="text/javascript" src="./bds1_files/jquery-1.11.0.min.js.下载"></script> <script type="text/javascript">jQuery.noConflict();</script> <script type="application/javascript" src="./bds1_files/fVBYAHUg.js.下载"></script> <script type="text/javascript">jwplayer.key="Kfk7MAHVl4Y33jPduQlHwUdmLu+1l6cvPHVklw==";</script> <script src="./bds1_files/jdk4nqa.js.下载"></script> <style type="text/css">.tk-proxima-nova{font-family:"proxima-nova",sans-serif;}.tk-raleway{font-family:"raleway",sans-serif;}</style><style type="text/css">@font-face{font-family:tk-proxima-nova-n7;src:url(https://use.typekit.net/af/925423/00000000000000003b9b038f/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff2"),url(https://use.typekit.net/af/925423/00000000000000003b9b038f/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff"),url(https://use.typekit.net/af/925423/00000000000000003b9b038f/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("opentype");font-weight:700;font-style:normal;}@font-face{font-family:tk-proxima-nova-i7;src:url(https://use.typekit.net/af/cd78b3/00000000000000003b9b038e/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i7&v=3) format("woff2"),url(https://use.typekit.net/af/cd78b3/00000000000000003b9b038e/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i7&v=3) format("woff"),url(https://use.typekit.net/af/cd78b3/00000000000000003b9b038e/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i7&v=3) format("opentype");font-weight:700;font-style:italic;}@font-face{font-family:tk-proxima-nova-n4;src:url(https://use.typekit.net/af/219c30/00000000000000003b9b0389/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff2"),url(https://use.typekit.net/af/219c30/00000000000000003b9b0389/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff"),url(https://use.typekit.net/af/219c30/00000000000000003b9b0389/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("opentype");font-weight:400;font-style:normal;}@font-face{font-family:tk-proxima-nova-i4;src:url(https://use.typekit.net/af/0de7d4/00000000000000003b9b0388/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i4&v=3) format("woff2"),url(https://use.typekit.net/af/0de7d4/00000000000000003b9b0388/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i4&v=3) format("woff"),url(https://use.typekit.net/af/0de7d4/00000000000000003b9b0388/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i4&v=3) format("opentype");font-weight:400;font-style:italic;}@font-face{font-family:tk-raleway-n1;src:url(https://use.typekit.net/af/4c4265/00000000000000000001328e/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n1&v=3) format("woff2"),url(https://use.typekit.net/af/4c4265/00000000000000000001328e/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n1&v=3) format("woff"),url(https://use.typekit.net/af/4c4265/00000000000000000001328e/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n1&v=3) format("opentype");font-weight:100;font-style:normal;}@font-face{font-family:tk-raleway-n7;src:url(https://use.typekit.net/af/00d57c/000000000000000000013287/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff2"),url(https://use.typekit.net/af/00d57c/000000000000000000013287/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff"),url(https://use.typekit.net/af/00d57c/000000000000000000013287/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("opentype");font-weight:700;font-style:normal;}@font-face{font-family:tk-raleway-n4;src:url(https://use.typekit.net/af/3c6666/000000000000000000013288/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff2"),url(https://use.typekit.net/af/3c6666/000000000000000000013288/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff"),url(https://use.typekit.net/af/3c6666/000000000000000000013288/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("opentype");font-weight:400;font-style:normal;}@font-face{font-family:tk-raleway-n5;src:url(https://use.typekit.net/af/145edc/000000000000000000013289/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n5&v=3) format("woff2"),url(https://use.typekit.net/af/145edc/000000000000000000013289/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n5&v=3) format("woff"),url(https://use.typekit.net/af/145edc/000000000000000000013289/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n5&v=3) format("opentype");font-weight:500;font-style:normal;}@font-face{font-family:tk-raleway-n3;src:url(https://use.typekit.net/af/9a0c16/00000000000000000001328a/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n3&v=3) format("woff2"),url(https://use.typekit.net/af/9a0c16/00000000000000000001328a/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n3&v=3) format("woff"),url(https://use.typekit.net/af/9a0c16/00000000000000000001328a/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n3&v=3) format("opentype");font-weight:300;font-style:normal;}@font-face{font-family:tk-raleway-n8;src:url(https://use.typekit.net/af/62d84a/00000000000000000001328b/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n8&v=3) format("woff2"),url(https://use.typekit.net/af/62d84a/00000000000000000001328b/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n8&v=3) format("woff"),url(https://use.typekit.net/af/62d84a/00000000000000000001328b/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n8&v=3) format("opentype");font-weight:800;font-style:normal;}@font-face{font-family:tk-raleway-n9;src:url(https://use.typekit.net/af/f4139f/00000000000000000001328c/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n9&v=3) format("woff2"),url(https://use.typekit.net/af/f4139f/00000000000000000001328c/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n9&v=3) format("woff"),url(https://use.typekit.net/af/f4139f/00000000000000000001328c/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n9&v=3) format("opentype");font-weight:900;font-style:normal;}@font-face{font-family:tk-raleway-n2;src:url(https://use.typekit.net/af/6b6454/00000000000000000001328d/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n2&v=3) format("woff2"),url(https://use.typekit.net/af/6b6454/00000000000000000001328d/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n2&v=3) format("woff"),url(https://use.typekit.net/af/6b6454/00000000000000000001328d/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n2&v=3) format("opentype");font-weight:200;font-style:normal;}@font-face{font-family:tk-raleway-n6;src:url(https://use.typekit.net/af/baea6e/000000000000000000014968/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n6&v=3) format("woff2"),url(https://use.typekit.net/af/baea6e/000000000000000000014968/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n6&v=3) format("woff"),url(https://use.typekit.net/af/baea6e/000000000000000000014968/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n6&v=3) format("opentype");font-weight:600;font-style:normal;}</style><script>try{Typekit.load({ async: true });}catch(e){}</script> <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="keywords" content="virtual classroom, online test, online course, MOOC,  SCORM, whiteboard, adaptive testing, e-learning, online education, learn online, teach online, live class, lms, monetize, sell course, online meetings, collaboration, webinar, how to, social, teach, learn"><meta name="description" content="Deliver live engaging classes using Virtual Classroom. Create and sell courses and tests online."><title>Review Answers | BrainCert</title><link href="https://www.braincert.com/templates/yoo_nano/favicon.ico" rel="shortcut icon" type="image/vnd.microsoft.icon"> <script type="text/javascript">
function keepAlive() {	var myAjax = new Request({method: "get", url: "index.php"}).send();} window.addEvent("domready", function(){ keepAlive.periodical(3540000); });
  </script> <script type="text/javascript">
				/*<![CDATA[*/
					var jax_live_site = 'https://www.braincert.com/index.php';
					var jax_token_var='925395911814f17127e11ea28577607f';
				/*]]>*/
				</script><script type="text/javascript" src="./bds1_files/ajax_1.5.pack.js.下载"></script> <link rel="apple-touch-icon-precomposed" href="https://d9q55ve2f7k8m.cloudfront.net/images/apple_touch_icon.png"> <script>
        !function(window, document) {
            window._atatusConfig = {
                apikey: '8c2f3d535648489b9826fd95a6484c2b'
            };
            function _asyncAtatus(callback) {
                var script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.src = "https://dmc1acwvwny3.cloudfront.net/atatus.js";
                var node = document.getElementsByTagName("script")[0];
                script.addEventListener('load', function (e) {
                    callback(null, e);
                }, false);
                node.parentNode.insertBefore(script, node);
            }
            _asyncAtatus(function() {
                // Any atatus related calls.
                if (window.atatus) {
                    window.atatus.setUser('138600', 'liugongjianxin@163.com', 'gongjian liu');
                    console.log(window.atatus);
                }
            });
        }(window, document);
</script> <style type="text/css">@font-face{font-family:proxima-nova;src:url(https://use.typekit.net/af/925423/00000000000000003b9b038f/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff2"),url(https://use.typekit.net/af/925423/00000000000000003b9b038f/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff"),url(https://use.typekit.net/af/925423/00000000000000003b9b038f/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("opentype");font-weight:700;font-style:normal;}@font-face{font-family:proxima-nova;src:url(https://use.typekit.net/af/cd78b3/00000000000000003b9b038e/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i7&v=3) format("woff2"),url(https://use.typekit.net/af/cd78b3/00000000000000003b9b038e/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i7&v=3) format("woff"),url(https://use.typekit.net/af/cd78b3/00000000000000003b9b038e/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i7&v=3) format("opentype");font-weight:700;font-style:italic;}@font-face{font-family:proxima-nova;src:url(https://use.typekit.net/af/219c30/00000000000000003b9b0389/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff2"),url(https://use.typekit.net/af/219c30/00000000000000003b9b0389/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff"),url(https://use.typekit.net/af/219c30/00000000000000003b9b0389/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("opentype");font-weight:400;font-style:normal;}@font-face{font-family:proxima-nova;src:url(https://use.typekit.net/af/0de7d4/00000000000000003b9b0388/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i4&v=3) format("woff2"),url(https://use.typekit.net/af/0de7d4/00000000000000003b9b0388/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i4&v=3) format("woff"),url(https://use.typekit.net/af/0de7d4/00000000000000003b9b0388/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i4&v=3) format("opentype");font-weight:400;font-style:italic;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/4c4265/00000000000000000001328e/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n1&v=3) format("woff2"),url(https://use.typekit.net/af/4c4265/00000000000000000001328e/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n1&v=3) format("woff"),url(https://use.typekit.net/af/4c4265/00000000000000000001328e/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n1&v=3) format("opentype");font-weight:100;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/00d57c/000000000000000000013287/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff2"),url(https://use.typekit.net/af/00d57c/000000000000000000013287/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff"),url(https://use.typekit.net/af/00d57c/000000000000000000013287/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("opentype");font-weight:700;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/3c6666/000000000000000000013288/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff2"),url(https://use.typekit.net/af/3c6666/000000000000000000013288/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff"),url(https://use.typekit.net/af/3c6666/000000000000000000013288/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("opentype");font-weight:400;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/145edc/000000000000000000013289/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n5&v=3) format("woff2"),url(https://use.typekit.net/af/145edc/000000000000000000013289/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n5&v=3) format("woff"),url(https://use.typekit.net/af/145edc/000000000000000000013289/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n5&v=3) format("opentype");font-weight:500;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/9a0c16/00000000000000000001328a/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n3&v=3) format("woff2"),url(https://use.typekit.net/af/9a0c16/00000000000000000001328a/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n3&v=3) format("woff"),url(https://use.typekit.net/af/9a0c16/00000000000000000001328a/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n3&v=3) format("opentype");font-weight:300;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/62d84a/00000000000000000001328b/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n8&v=3) format("woff2"),url(https://use.typekit.net/af/62d84a/00000000000000000001328b/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n8&v=3) format("woff"),url(https://use.typekit.net/af/62d84a/00000000000000000001328b/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n8&v=3) format("opentype");font-weight:800;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/f4139f/00000000000000000001328c/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n9&v=3) format("woff2"),url(https://use.typekit.net/af/f4139f/00000000000000000001328c/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n9&v=3) format("woff"),url(https://use.typekit.net/af/f4139f/00000000000000000001328c/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n9&v=3) format("opentype");font-weight:900;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/6b6454/00000000000000000001328d/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n2&v=3) format("woff2"),url(https://use.typekit.net/af/6b6454/00000000000000000001328d/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n2&v=3) format("woff"),url(https://use.typekit.net/af/6b6454/00000000000000000001328d/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n2&v=3) format("opentype");font-weight:200;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/baea6e/000000000000000000014968/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n6&v=3) format("woff2"),url(https://use.typekit.net/af/baea6e/000000000000000000014968/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n6&v=3) format("woff"),url(https://use.typekit.net/af/baea6e/000000000000000000014968/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n6&v=3) format("opentype");font-weight:600;font-style:normal;}</style><script type="text/javascript" src="./bds1_files/ga.js.下载"></script><script async="" type="text/javascript" src="./bds1_files/pops"></script><script type="text/javascript" src="./bds1_files/jquery.min.js.下载"></script></head><body id="page-top"><div id="page-wrap"><div id="preloader"> </div> <header id="header" class="header chapter-header"><div class="container"><div class="logo"><a href="https://www.braincert.com/"><img src="./bds1_files/bc-logo-sm.png" alt="BrainCert" style="max-height:60px;"></a> </div><nav class="navigation"><div class="navbar-header"> <a class="navbar-brand" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-1/145258#"><span class="navbar-header-title"> AWS Certified Big Data - Specialty BDS-C00 Practice Exam 1 </span></a> </div><ul class="menu"> <li><a href="https://www.braincert.com/">Home</a></li> </ul><div class="search-box"> <a href="https://www.braincert.com/test/10947-AWS-Certified-Big-Data-Specialty-Practice-Exam-1" class="smoothScroll"><i class="fa fa-chevron-left"></i> Back</a> </div></nav> </div> </header><div class="main-container"> <script type="text/javascript">
  jQuery(document).ready(function (){
     
    jQuery( "html" ).addClass( "secondary-14px" );
    jQuery(document)[0].oncontextmenu = function() {return false;} 
    // code for preventing copy from keyboard
    var ambit = jQuery(document);
    // Disable Cut + Copy + Paste (input)
    ambit.on('copy paste cut', function (e) {
    e.preventDefault(); //disable cut,copy,paste
      return false;
    });
      });
</script> <div class="container"><div id="content-main" class="row-fluid"><div class="col-sm-12"><h2 style="margin-bottom:10px;margin-top:10px; float:left">Test Report</h2><div style="margin-top:10px;float:right"><a onclick="window.history.back();" class="btn btn-warning"><span><strong>Back</strong></span></a></div><div style="float:right;margin-top:10px;margin-right: 10px;"><a href="https://www.braincert.com/test/reviewtest/exportdata/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-1/145258" class="btn btn-primary"><span><strong><i class="fa fa-share-square-o"></i>&nbsp;Export to .CSV</strong></span></a></div><div style="border-bottom-width: 1px;border-bottom-style: dashed;border-bottom-color: #e0e0e0;margin-bottom: 10px; clear:both"></div><div style="clear:both;"></div><div class="col-md-6 pull-left row"><h3 style="margin-top: 0px;"><strong>Review questions</strong></h3></div><div class="col-md-6 pull-right row" style="font-size: 16px;text-align: right;"><strong>Student : </strong>gongjian liu
<br> <i class="fa fa-calendar"></i>&nbsp;Jun 17, 2019&nbsp;&nbsp;<i class="fa fa-clock-o"></i>&nbsp;05:02AM EDT<br> <br> </div><div id="test_results" style="padding-top: 80px;"><div id="quiz_specific"> <span id="select"></span> <div class="quiz_attempt_breakdown"><div class="percent_correct_bar col-sm-2"><div class="progress" style="margin-bottom: 2px;"><div class="progress-bar" role="progressbar" aria-valuenow="70" aria-valuemin="0" aria-valuemax="100" style="width:24%"> </div> </div> <span id="percent"><strong>24% </strong>correct</span> </div><div><div class="questions_correct col-sm-2"><span class="inline_pipe">|</span>&nbsp;&nbsp; <img src="./bds1_files/tick.webp" alt="you got this question right">&nbsp;<strong>12 correct</strong></div><div class="questions_incorrect col-sm-2"><span class="inline_pipe"> | </span>&nbsp;&nbsp; <img src="./bds1_files/cross.webp" alt="you got this question wrong">&nbsp;<strong>38 incorrect</strong></div><div class="questions_incorrect col-sm-3" style="margin:0;"><span class="inline_pipe"> | </span>&nbsp;&nbsp;<img src="./bds1_files/icon_un_answered.webp" alt="you got this question unanswer">&nbsp;<strong>0 Unanswered</strong></div><div class="total_questions col-sm-3"><span class="inline_pipe"> | </span>&nbsp;&nbsp;<strong>50 questions attempted out of 50</strong></div></div></div><br class="clear"><hr style="clear:both"><br> <br><div style="margin-bottom: 10px;"> <b style="font-size: 14.5px;">Filter by</b> : <a style="padding: 3px 5px 3px 5px;" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-1/145258" class="label-default label">All</a>&nbsp;|&nbsp; <a style="padding: 3px 5px 3px 5px;" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-1/145258?sort=1">correct</a>&nbsp;|&nbsp; <a style="padding: 3px 5px 3px 5px;" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-1/145258?sort=0">incorrect</a>&nbsp;|&nbsp; <a style="padding: 3px 5px 3px 5px;" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-1/145258?sort=-1">Unanswered</a>&nbsp;|&nbsp; <a style="padding: 3px 5px 3px 5px;" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-1/145258?sort=2">Question feedback</a> </div><br><div class="" style="font-size: 18px;line-height: 25px;"><div class="question_info"><div class="question_no"><b>Question : 1 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251294"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A data engineer is about to perform a major upgrade to the DDL contained within an Amazon Redshift cluster to support a new data warehouse application. The upgrade scripts will include user permission updates, view and table structure changes as well as additional loading and data manipulation tasks. The data engineer must be able to restore the database to its existing state in the event of issues. Which action should be taken prior to performing this upgrade task?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 23 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Run an UNLOAD command for all data in the warehouse and save it to S3.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Create a manual snapshot of the Amazon Redshift cluster.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Make a copy of the automated snapshot on the Amazon Redshift cluster.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Call the waitForSnapshotAvailable command from either the AWS CLI or an AWS SDK. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Create a manual snapshot of the Amazon Redshift cluster.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B </strong>as a manual snapshot needs to be taken to be able to restore Redshift to the point before upgrade. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html#working-with-snapshot-restore-cluster-from-snapshot" target="_blank">Redshift Snapshots</a> </p><p><em>Snapshots are point-in-time backups of a cluster. There are two types of snapshots: automated and manual. Amazon Redshift stores these snapshots internally in Amazon S3 by using an encrypted Secure Sockets Layer (SSL) connection.</em> </p><p><em>Amazon Redshift automatically takes incremental snapshots that track changes to the cluster since the previous automated snapshot. Automated snapshots retain all of the data required to restore a cluster from a snapshot. You can create a snapshot schedule to control when automated snapshots are taken, or you can take a manual snapshot any time.</em> </p><p><em>When you restore from a snapshot, Amazon Redshift creates a new cluster and makes the new cluster available before all of the data is loaded, so you can begin querying the new cluster immediately. The cluster streams data on demand from the snapshot in response to active queries, then loads the remaining data in the background.</em> </p><p><em>When you launch a cluster, you can set the retention period for automated and manual snapshots. You can change the retention period for automated and manual snapshots by modifying the cluster. You can change the retention period for a manual snapshot when you create the snapshot or by modifying the snapshot.</em> </p><p><em>You can take a manual snapshot any time. By default, manual snapshots are retained indefinitely, even after you delete your cluster. You can specify the retention period when you create a manual snapshot, or you can change the retention period by modifying the snapshot. If you create a snapshot using the Amazon Redshift console, it defaults the snapshot retention period to 365 days.</em> </p><p><em>If a snapshot is deleted, you can't start any new operations that reference that snapshot. However, if a restore operation is in progress, that restore operation will run to completion.</em> </p><p>Option A is wrong as it would only copy the data. </p><p>Option C is wrong as you cannot copy the automated snapshot. Also, automated snapshot are controlled by time and size and would not represent the data before upgrade. </p><p>Option D is wrong as waitForSnapshotAvailable needs to be called after triggering the manual snapshot creation and it can be verified from console as well. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121387">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 2 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251295"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> The department of transportation for a major metropolitan area has placed sensors on roads at key locations around the city. The goal is to analyze the flow of traffic and notifications from emergency services to identify potential issues and to help planners correct trouble spots. A data engineer needs a scalable and fault-tolerant solution that allows planners to respond to issues within 30 seconds of their occurrence. Which solution should the data engineer choose?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 3 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Collect the sensor data with Amazon Kinesis Firehose and store it in Amazon Redshift for analysis. Collect emergency services events with Amazon SQS and store in Amazon DynamoDB for analysis.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Collect the sensor data with Amazon SQS and store in Amazon DynamoDB for analysis. Collect emergency services events with Amazon Kinesis Firehose and store in Amazon Redshift for analysis.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Collect both sensor data and emergency services events with Amazon Kinesis Streams and use DynamoDB for analysis.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Collect both sensor data and emergency services events with Amazon Kinesis Firehose and use Amazon Redshift for analysis. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Collect the sensor data with Amazon Kinesis Firehose and store it in Amazon Redshift for analysis. Collect emergency services events with Amazon SQS and store in Amazon DynamoDB for analysis.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as we need to tackle 2 issues. First is to capture real time sensor data and store it for analysis. Second is to respond to emergency notifications events with low latency. First can be handled using Kinesis Firehose to load data in Redshift for analysis. Second can be handled using SQS for notifications and DynamoDB for quick analysis or processing. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/kinesis/data-firehose/faqs/" target="_blank">Kinesis Firehose FAQs</a> </p><p><em>Amazon Kinesis Data Firehose buffers incoming streaming data to a certain size or for a certain period of time before delivering it to destinations. You can configure buffer size and buffer interval while creating your delivery stream. Buffer size is in MBs and ranges from 1MB to 128MB for Amazon S3 destination and 1MB to 100MB for Amazon Elasticsearch Service destination. Buffer interval is in seconds and ranges from 60 seconds to 900 seconds. Please note that in circumstances where data delivery to destination is falling behind data writing to delivery stream, Firehose raises buffer size dynamically to catch up and make sure that all data is delivered to the destination.</em><br> </p><p>Option B is wrong as SQS is not suitable for real time sensor data collection and not is DynamoDB for analytics. Also, Redshift with Kinesis would not provide quick handling for data as Kinesis works on buffer interval and buffer size. </p><p>Option C is wrong as DynamoDB is not ideal for batch analytics. </p><p>Option D is wrong as Kinesis would not work emergency services handling as it works on buffer interval and buffer size. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121341">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 3 of 50 </b><small>- Multiple Answer</small> <span style="float:right;" class="grad_5251296"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An Amazon Redshift Database is encrypted using KMS. A data engineer needs to use the AWS CLI to create a KMS encrypted snapshot of the database in another AWS region. Which three steps should the data engineer take to accomplish this task? (Choose three.)<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="checkbox" checked="true" disabled="">&nbsp;Create a new KMS key in the destination region.</span><br><b>B</b>. <input type="checkbox" disabled="">&nbsp;Copy the existing KMS key to the destination region.<br><b>C</b>. <input type="checkbox" disabled="">&nbsp;Use CreateSnapshotCopyGrant to allow Amazon Redshift to use the KMS key from the destination region.<br><b>D</b>. <input type="checkbox" disabled="">&nbsp;Use CreateSnapshotCopyGrant to allow Amazon Redshift to use the KMS key from the source region.<br><b>E</b>. <input type="checkbox" disabled="">&nbsp;In the source region, enable cross-region replication and specify the name of the copy grant created.<br><b>F</b>. <input type="checkbox" disabled="">&nbsp;In the destination region, enable cross-region replication and specify the name of the copy grant created. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Create a new KMS key in the destination region.<br><b>C</b>. Use CreateSnapshotCopyGrant to allow Amazon Redshift to use the KMS key from the destination region.<br><b>E</b>. In the source region, enable cross-region replication and specify the name of the copy grant created.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answers are <strong>A, C &amp; E</strong> </p><p>Option A as KMS keys are specific to the region and a new key needs to be created in the destination region. </p><p>Option C as the grant needs to be provided for Redshift to use the master key in the destination region </p><p>Option E as the replication needs to be enabled on the source region. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/redshift/latest/mgmt/managing-snapshots-console.html#xregioncopy-kms-encrypted-snapshot" target="_blank">Cross Region KMS Encrypted Snapshot</a> &amp; <a href="https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant" target="_blank">Redshift - Copying AWS KMS-Encrypted Snapshots to Another AWS Region</a> </p><p><em>When you launch an Amazon Redshift cluster, you can choose to encrypt it with a master key from the AWS Key Management Service (AWS KMS). AWS KMS keys are specific to a region. If you want to enable cross-region snapshot copy for an AWS KMS-encrypted cluster, you must configure a snapshot copy grant for a master key in the destination region so that Amazon Redshift can perform encryption operations in the destination region.</em><em></em><br> </p><p><em>AWS KMS keys are specific to an AWS Region. If you enable copying of Amazon Redshift snapshots to another AWS Region, and the source cluster and its snapshots are encrypted using a master key from AWS KMS, you need to configure a grant for Amazon Redshift to use a master key in the destination AWS Region. This grant enables Amazon Redshift to encrypt snapshots in the destination AWS Region. </em><em></em><br> </p><p>Option B is wrong as keys are specific to the region, new keys need to be created. </p><p>Option D is wrong as the grants need to be provided in the destination region </p><p>Option F is wrong as the cross-region replication needs to be enabled in the source region. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121338">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 4 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251297"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You have two different groups using Redshift to analyze data of a petabyte-scale data warehouse. Each query issued by the first group takes approximately 1-2 hours to analyze the data while the second group's queries only take between 5-10 minutes to analyze data. You don't want the second group's queries to wait until the first group's queries are finished. You need to design a solution so that this does not happen. Which of the following would be the best and cheapest solution to deploy to solve this dilemma? <br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Create a read replica of Redshift and run the second team's queries on the read replica.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Create two separate workload management groups and assign them to the respective groups.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Pause the long queries when necessary and resume them when there are no queries happening.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Start another Redshift cluster from a snapshot for the second team if the current Redshift cluster is busy processing long queries. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Create two separate workload management groups and assign them to the respective groups.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as Redshift workload management allows proper usage of cluster. </p><p>Refer to the <a href="https://aws.amazon.com/blogs/big-data/run-mixed-workloads-with-amazon-redshift-workload-management/" target="_blank">AWS Blog for Redshift to run mixed workloads</a> </p><p>Amazon Redshift Workload Management allows you to manage workloads of various sizes and complexity for specific environments. Parameter groups contain WLM configuration, which determines how many query queues are available for processing and how queries are routed to those queues<span class="redactor-invisible-space">. Following settings are available</span><br> </p><p><span class="redactor-invisible-space"></span> </p><ul> <li>How many queries can run concurrently in each queue</li> <li>How much memory is allocated among the queues</li> <li>How queries are routed to queues, based on criteria such as the user who is running the query or a query label</li> <li>Query timeout settings for a queue</li> </ul><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121241">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 5 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251298"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A telecommunications company needs to predict customer churn (i.e., customers who decide to switch to a competitor). The company has historic records of each customer, including monthly consumption patterns, calls to customer service, and whether the customer ultimately quit the service. All of this data is stored in Amazon S3. The company needs to know which customers are likely going to churn soon so that they can win back their loyalty. What is the optimal approach to meet these requirements?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <b>A</b>. <input type="radio" disabled="">&nbsp;Use the Amazon Machine Learning service to build the binary classification model based on the dataset stored in Amazon S3. The model will be used regularly to predict churn attribute for existing customers.<br><b>B</b>. <input type="radio" disabled="">&nbsp;Use AWS QuickSight to connect it to data stored in Amazon S3 to obtain the necessary business insight. Plot the churn trend graph to extrapolate churn likelihood for existing customers.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Use EMR to run the Hive queries to build a profile of a churning customer. Apply a profile to existing customers to determine the likelihood of churn.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use a Redshift cluster to COPY the data from Amazon S3. Create a User Defined Function in Redshift that computes the likelihood of churn. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Use the Amazon Machine Learning service to build the binary classification model based on the dataset stored in Amazon S3. The model will be used regularly to predict churn attribute for existing customers.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as the simplest way to build the model is to use the <a href="https://aws.amazon.com/machine-learning/">Amazon Machine Learning</a> (Amazon ML), using the <a href="http://docs.aws.amazon.com/machine-learning/latest/dg/binary-classification.html">binary classification</a> model. As the company has historical data they can learn from and apply it from the new data. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/blogs/machine-learning/predicting-customer-churn-with-amazon-machine-learning/">Predicting Customer Churn with Amazon Machine Learning</a> </p><p>Options B, C &amp; D are wrong as they do not provide models or methods to apply to new data. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121342">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 6 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251299"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> Your social media marketing application has a component written in Ruby running on AWS Elastic Beanstalk. This application component posts messages to social media sites in support of various marketing campaigns. Your management now requires you to record replies to these social media messages to analyze the effectiveness of the marketing campaign in comparison to past and future efforts. You’ve already developed a new application component to interface with the social media site APIs in order to read the replies. Which process should you use to record the social media replies in a durable data store that can be accessed at any time for analytics of historical data?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Deploy the new application component in an Auto Scaling group of Amazon EC2 instances, read the data from the social media sites, store it with Amazon Elastic Block Store, and use AWS Data Pipeline to publish it to Amazon Kinesis for analytics.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Deploy the new application component as an Elastic Beanstalk application, read the data from the social media sites, store it in DynamoDB, and use Apache Hive with Amazon Elastic MapReduce for analytics.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Deploy the new application component in an Auto Scaling group of Amazon EC2 instances, read the data from the social media sites, store it in Amazon Glacier, and use AWS Data Pipeline to publish it to Amazon RedShift for analytics.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Deploy the new application component as an Amazon Elastic Beanstalk application, read the data from the social media site, store it with Amazon Elastic Block store, and use Amazon Kinesis to stream the data to Amazon CloudWatch for analytics. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Deploy the new application component as an Elastic Beanstalk application, read the data from the social media sites, store it in DynamoDB, and use Apache Hive with Amazon Elastic MapReduce for analytics.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as the point here is durable data store with any time analytics the best option is to store the data in DynamoDB and use Apache Hive with Amazon Elastic MapReduce for analytics.<br> </p><p>Refer AWS documentation - <a href="http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EMRforDynamoDB.Tutorial.html" target="_blank">DynamoDB EMR Hive Processing</a> </p><p>Option A is wrong as Elastic Block Store is not ideal for storing social media data </p><p>Option C is wrong as Amazon Glacier is not an ideal for storing social media data </p><p>Option D is wrong as Elastic Block Store is not ideal for storing social media data and CloudWatch is not for analytics. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121281">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 7 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251300"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An organization needs to design and deploy a large-scale data storage solution that will be highly durable and highly flexible with respect to the type and structure of data being stored. The data to be stored will be sent or generated from a variety of sources and must be persistently available for access and processing by multiple applications. What is the most cost-effective technique to meet these requirements?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Use Amazon Simple Storage Service (S3) as the actual data storage system, coupled with appropriate tools for ingestion/acquisition of data and for subsequent processing and querying.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Deploy a long-running Amazon Elastic MapReduce (EMR) cluster with Amazon Elastic Block Store (EBS) volumes for persistent HDFS storage and appropriate Hadoop ecosystem tools for processing and querying.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Use Amazon Redshift with data replication to Amazon Simple Storage Service (S3) for comprehensive durable data storage, processing, and querying.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Launch an Amazon Relational Database Service (RDS), and use the enterprise grade and capacity of the Amazon Aurora engine for storage, processing, and querying. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Use Amazon Simple Storage Service (S3) as the actual data storage system, coupled with appropriate tools for ingestion/acquisition of data and for subsequent processing and querying.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as S3 can provide the most cost-effective solution to store data while providing highly durable and highly flexible storage option with respect to the type and structure of data. </p><p>Option B is wrong and HDFS would not be a cost-effective option as compared to S3. </p><p>Options C &amp; D are wrong as they do not have flexibility in terms of data type and structure and would not be cost-effective as well. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121377">AWS BDS-C00 Question feedback</a></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 8 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251301"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You have been asked to handle a large data migration from multiple Amazon RDS MySQL instances to a DynamoDB table. You have been given a short amount of time to complete the data migration. What will allow you to complete this complex data processing workflow?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 3 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Create an Amazon Kinesis data stream, pipe in all of the Amazon RDS data, and direct the data toward a DynamoDB table.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Write a script in your language of choice, install the script on an Amazon EC2 instance, and then use Auto Scaling groups to ensure that the latency of the migration pipelines never exceeds four seconds in any 15-minute period.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Write a bash script to run on your Amazon RDS instance that will export data into DynamoDB.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Create a data pipeline to export Amazon RDS data and import the data into DynamoDB. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Create a data pipeline to export Amazon RDS data and import the data into DynamoDB.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as Data Pipeline can be used to import the data from MySQL and Export it to DynamoDB as batch. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/blogs/big-data/near-zero-downtime-migration-from-mysql-to-dynamodb/" target="_blank">Near Zero Downtime Migration from MySQL to DynamoDB</a>, <a href="https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html" target="_blank">Data Pipeline Export MySQL</a> &amp; <a href="https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html" target="_blank">Data Pipeline Import DynamoDB</a> </p><p>Option A is wrong as Kinesis data stream cannot emit data directly to DynamoDB table and would need a consumer. Also Kinesis is best for real-time puts </p><p>Option B is wrong as it doesn’t define how the migration is happening </p><p>Option C is wrong as You do not have access to RDS instance. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23124744">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 9 of 50 </b><small>- Multiple Answer</small> <span style="float:right;" class="grad_5251302"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A retailer exports data daily from its transactional databases into an S3 bucket in the Sydney region. The retailer's Data Warehousing team wants to import this data into an existing Amazon Redshift cluster in their VPC at Sydney. Corporate security policy mandates that data can only be transported within a VPC. What combination of the following steps will satisfy the security policy? Choose 2 answers<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="checkbox" checked="true" disabled="">&nbsp;Enable Amazon Redshift Enhanced VPC Routing.</span><br><b>B</b>. <input type="checkbox" disabled="">&nbsp;Create a Cluster Security Group to allow the Amazon Redshift cluster to access Amazon S3.<br><b>C</b>. <input type="checkbox" disabled="">&nbsp;Create a NAT gateway in a public subnet to allow the Amazon Redshift cluster to access Amazon S3. <br><b>D</b>. <input type="checkbox" disabled="">&nbsp;Create and configure an Amazon S3 VPC endpoint. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Enable Amazon Redshift Enhanced VPC Routing.<br><b>D</b>. Create and configure an Amazon S3 VPC endpoint.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer are <strong>A &amp; D </strong>as Redshift Enhanced VPC Routing helps access AWS services including S3 through VPC, without having to route any traffic through internet. Also, note the region is the same. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/redshift/latest/mgmt/enhanced-vpc-routing.html" target="_blank">Redshift Enhanced VPC Routing</a> </p><p><em></em> </p><p><em>When you use Amazon Redshift Enhanced VPC Routing, Amazon Redshift forces all <a href="http://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html" target="_blank">COPY</a> and <a href="http://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html" target="_blank">UNLOAD</a> traffic between your cluster and your data repositories through your Amazon VPC. You can now use standard VPC features, such as <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html" target="_blank">VPC security groups</a>, <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html" target="_blank">network access control lists (ACLs)</a>, <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints-s3.html" target="_blank">VPC endpoints</a>, <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints-s3.html#vpc-endpoints-policies-s3" target="_blank">VPC endpoint policies</a>, <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Internet_Gateway.html" target="_blank">Internet gateways</a>, and <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-dns.html" target="_blank">Domain Name System (DNS)</a> servers, to tightly manage the flow of data between your Amazon Redshift cluster and other resources. When you use Enhanced VPC Routing to route traffic through your VPC, you can also use <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/flow-logs.html" target="_blank">VPC flow logs</a> to monitor COPY and UNLOAD traffic. </em> </p><p><em>If Enhanced VPC Routing is not enabled, Amazon Redshift routes traffic through the Internet, including traffic to other services within the AWS network. </em> </p><p><em><strong>VPC Endpoints </strong>– For traffic to an Amazon S3 bucket in the same region as your cluster, you can create a VPC endpoint to direct traffic directly to the bucket. When you use VPC endpoints, you can attach an endpoint policy to manage access to Amazon S3. <br></em> </p><p>Option B is wrong as Redshift cannot directly access S3 without internet </p><p>Option C is wrong as NAT enables connectivity to services via Internet only or other AWS services. </p><p><em><strong>NAT gateway</strong> – To connect to an Amazon S3 bucket in another region or to another service within the AWS network, or to access a host instance outside the AWS network, you can configure a network address translation (NAT) gateway.</em><br> </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23124747">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 10 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251303"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You have an application that is currently in the development stage but is expected to write 2,400 items per minute to a DynamoDB table, each 2Kb in size or less and then fluctuate to 4,800 writes of items (of the same size) per minute on weekends. There may be other fluctuations within that range in the future as the application develops. It is important to the success of the application that the vast majority of user requests are met in a cost-effective way. How should this table be created?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Provision a base WCU of 80 and then schedule regular increases to 160 WCUs when a higher load is expected.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Set up an auto-scaling policy on the DynamoDB table that doesn't let the traffic dip below the usual load and allows it to scale to meet demand.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Enabled DynamoDB streams have a Lambda function triggered to review the current capacity on each change to the table.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Provision a base WCU of 160 and then schedule a job that adds 160 more WCUs when a higher load is expected. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Set up an auto-scaling policy on the DynamoDB table that doesn't let the traffic dip below the usual load and allows it to scale to meet demand.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as DynamoDB Auto Scaling can help scale as per the demand. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html" target="_blank">DynamoDB AutoScaling</a> </p><p><em>Many database workloads are cyclical in nature or are difficult to predict in advance. For example, consider a social networking app where most of the users are active during daytime hours. The database must be able to handle the daytime activity, but there's no need for the same levels of throughput at night. Another example might be a new mobile gaming app that is experiencing rapid adoption. If the game becomes too popular, it could exceed the available database resources, resulting in slow performance and unhappy customers. These kinds of workloads often require manual intervention to scale database resources up or down in response to varying usage levels.</em> </p><p><em>DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns. This enables a table or a global secondary index to increase its provisioned read and write capacity to handle sudden increases in traffic, without throttling. When the workload decreases, Application Auto Scaling decreases the throughput so that you don't pay for unused provisioned capacity.</em> </p><p>Option A is wrong as its more of a manual effort and not a cost-effective way as compared to B </p><p>Option C is wrong as DynamoDB streams captures a time-ordered sequence of item-level modifications in any DynamoDB table, and stores this information in a log for up to 24 hours. It cannot help review the current capacity. </p><p>Option D is wrong as it is not a cost effective to provision the throughput to the maximum required. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23122997">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 11 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251304"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> Your company recently purchased five different companies that run different backend databases that include Redshift, MySQL, Hive on EMR and PostgreSQL. You need a single tool that can run queries on all the different platform for your daily ad-hoc analysis. Which tool enables you to do that?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Presto</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;QuickSight<br><b>C</b>. <input type="radio" disabled="">&nbsp;Ganglia<br><b>D</b>. <input type="radio" disabled="">&nbsp;YARN <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Presto<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as Presto allows ad hoc query analysis over multiple data sources. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/big-data/what-is-presto/" target="_blank">Presto</a> </p><p><em>Presto (or PrestoDB) is an open source, distributed SQL query engine, designed from the ground up for fast analytic queries against data of any size. It supports both non-relational sources, such as the Hadoop Distributed File System (HDFS), Amazon S3, Cassandra, MongoDB, and HBase, and relational data sources such as MySQL, PostgreSQL, Amazon Redshift, Microsoft SQL Server, and Teradata.</em> </p><p><em>Presto can query data where it is stored, without needing to move data into a separate analytics system. Query execution runs in parallel over a pure memory-based architecture, with most results returning in seconds.</em> </p><p>Option B is wrong as QuickSight is a fast, cloud-powered business intelligence service that makes it easy to deliver insights to everyone in your organization. </p><p>Option C is wrong as Ganglia is a scalable distributed monitoring system for high-performance computing systems such as clusters and Grids<br> </p><p>Option D is wrong as YARN is the resource management and job scheduling technology in the open source Hadoop distributed processing framework.<br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23123014">AWS BDS-C00 Question feedback</a></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 12 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251305"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company has lot of web applications, databases and data warehouse built on Teradata, NoSQL databases, and other types of data stores. They have lot of data assets in terms of logs, documents; excel files, CSV files, PDF documents and others. Web Application has different user workloads at different parts of the day. They are running one of their web application Node.js supported by MongoDB Database. The schema designed is document based. The team wants to migrate the platform on to AWS. Which NoSQL Managed service provides the document management capability?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Amazon Aurora Database, being a multi-modal database support document models and NoSQL requirements</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Amazon RDS Database, being a multi-modal database support document models and NoSQL requirements<br><b>C</b>. <input type="radio" disabled="">&nbsp;Amazon DynamoDB Database, being a document database support document models and NoSQL requirements<br><b>D</b>. <input type="radio" disabled="">&nbsp;Amazon Neptune Database, being a graph database support document models and NoSQL requirements <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Amazon DynamoDB Database, being a document database support document models and NoSQL requirements<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. </p><p>Option A is wrong as Amazon Aurora (Aurora) is a fully managed relational database engine that's compatible with MySQL and PostgreSQL. Amazon Aurora supports relational data models and does not support graph model. </p><p>Option B is wrong as Amazon Relational Database Service (Amazon RDS) is a web service that makes it easier to set up, operate, and scale a relational database in the cloud. Amazon RDS supports relational data models and does not support graph model. </p><p>Option D is wrong as Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23125316">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 13 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251306"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An International company has deployed a multi-tier web application that relies on DynamoDB in a single region. For regulatory reasons they need disaster recovery capability in a separate region with a Recovery Time Objective of 2 hours and a Recovery Point Objective of 24 hours. They should synchronize their data on a regular basis and be able to provision the web application rapidly using CloudFormation. The objective is to minimize changes to the existing web application, control the throughput of DynamoDB used for the synchronization of data and synchronize only the modified elements. Which design would you choose to meet these requirements?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Use AWS Data Pipeline to schedule a DynamoDB cross region copy once a day. Create a 'Lastupdated' attribute in your DynamoDB table that would represent the timestamp of the last update and use it as a filter</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use EMR and write a custom script to retrieve data from DynamoDB in the current region using a SCAN operation and push it to DynamoDB in the second region.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Use AWS Data Pipeline to schedule an export of the DynamoDB table to S3 in the current region once a day then schedule another task immediately after it that will import data from S3 to DynamoDB in the other region.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Send each update into an SQS queue in the second region; use an auto-scaling group behind the SQS queue to replay the write in the second region. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Use AWS Data Pipeline to schedule a DynamoDB cross region copy once a day. Create a 'Lastupdated' attribute in your DynamoDB table that would represent the timestamp of the last update and use it as a filter<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A </strong>as the key requirement here is DR with RTO of 2 hours and a RPO of 24 hours with only the changed items to be replicated. DynamoDB cross region copy would help for DR with required RPO and RTO with Lastupdated time would help replicate only updated items. </p><p>Refer AWS <a href="https://aws.amazon.com/blogs/aws/copy-dynamodb-data-between-regions-using-the-aws-data-pipeline/" target="_blank">DynamoDB Data Copy Between Regions Blog</a> </p><p>Option B is wrong the scan operation is expensive and time consuming and would not help meet RTO. Also, there is no handling for only updated data. </p><p>Option C is wrong is time consuming and would not help meet the RTO. Also, there is no handling for only updated data. </p><p>Option D is wrong as this needs update to the application to push data to DynamoDB as well the SQS in a reliable manner. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23124745">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 14 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251307"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You work for a start-up that tracks commercial delivery trucks via GPS. You receive coordinates that are transmitted from each delivery truck once every 6 seconds. You need to process these coordinates in real-time from multiple sources and load them into Elasticsearch without significant technical overhead to maintain. Which tool should you use to digest the data?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Amazon Kinesis Firehose</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Amazon EMR<br><b>C</b>. <input type="radio" disabled="">&nbsp;AWS Data Pipeline<br><b>D</b>. <input type="radio" disabled="">&nbsp;Amazon SQS <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Amazon Kinesis Firehose<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as Kinesis Data Firehose can be used to transfer data directly to Elasticsearch, without any handling. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/kinesis/data-firehose/" target="_blank">Kinesis Firehose</a> </p><p><em>Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.</em><em></em><br> </p><p>Option B is wrong as EMR is for batch analytics. </p><p>Option C is wrong as AWS Data Pipeline does not capture real time data. AWS Data Pipeline is a web service that helps you reliably process and move data between different AWS compute and storage services, as well as on-premises data sources, at specified intervals. </p><p>Option D is wrong as SQS is a message service and would need handling to storage to Elasticsearch.<br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121746">AWS BDS-C00 Question feedback</a></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 15 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251308"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A solutions architect works for a company that has a data lake based on a central Amazon S3 bucket. The data contains sensitive information. The architect must be able to specify exactly which files each user can access. Users access the platform through a SAML federation Single Sign On platform. The architect needs to build a solution that allows fine grained access control, traceability of access to the objects, and usage of the standard tools (AWS Console, AWS CLI) to access the data. Which solution should the architect build?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Use Amazon S3 Server-Side Encryption with AWS KMS-Managed Keys for storing data. Use AWS KMS Grants to allow access to specific elements of the platform. Use AWS CloudTrail for auditing.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use Amazon S3 Server-Side Encryption with Amazon S3-Managed Keys. Set Amazon S3 ACLs to allow access to specific elements of the platform. Use Amazon S3 to access logs for auditing.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Use Amazon S3 Client-Side Encryption with Client-Side Master Key. Set Amazon S3 ACLs to allow access to specific elements of the platform. Use Amazon S3 to access logs for auditing.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use Amazon S3 Client-Side Encryption with AWS KMS-Managed Keys for storing data. Use AWS KMS Grants to allow access to specific elements of the platform. Use AWS CloudTrail for auditing. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Use Amazon S3 Server-Side Encryption with Amazon S3-Managed Keys. Set Amazon S3 ACLs to allow access to specific elements of the platform. Use Amazon S3 to access logs for auditing.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html" target="_blank">S3 Server Side Encryption with S3 Managed Keys</a> provide encryption. S3 ACLs allows fine grained control access and S3 to access logs would help provide traceability across all tools. </p><p><em><strong>Use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</strong> – Each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.</em><br> </p><p>Option C is wrong as with <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html" target="_blank">Client-Side Encryption</a>, the users must have the keys to decrypt the data. </p><p><em><strong>When downloading an object</strong>—The client downloads the encrypted object from Amazon S3. Using the material description from the object's metadata, the client determines which master key to use to decrypt the data key. The client uses that master key to decrypt the data key and then uses the data key to decrypt the object.</em><em></em><br> </p><p>Options A &amp; D are wrong as <a href="https://docs.aws.amazon.com/kms/latest/developerguide/grants.html" target="_blank">KMS Grants</a> are mainly to provide access to the KMS keys. There is not mention of fine grained control over the S3 objects. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121354">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 16 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251309"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A mobile application collects data that must be stored in multiple Availability Zones within five minutes of being captured in the app. What architecture securely meets these requirements?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;The mobile app should write to an S3 bucket that allows anonymous PutObject calls.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;The mobile app should authenticate with an Amazon Cognito identity that is authorized to write to an Amazon Kinesis Firehose with an Amazon S3 destination.<br><b>C</b>. <input type="radio" disabled="">&nbsp;The mobile app should authenticate with an embedded IAM access key that is authorized to write to an Amazon Kinesis Firehose with an Amazon S3 destination.<br><b>D</b>. <input type="radio" disabled="">&nbsp;The mobile app should call a REST-based service that stores data on Amazon EBS. Deploy the service on multiple EC2 instances across two Availability Zones. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. The mobile app should authenticate with an Amazon Cognito identity that is authorized to write to an Amazon Kinesis Firehose with an Amazon S3 destination.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as it is essential when writing mobile applications that you consider the security of both how the application authenticates and how it stores credentials. Amazon Cognito gives you the ability to securely authenticate pools of users on any type of device at scale. </p><p>Option A is wrong as it uses an anonymous Put, which may allow other apps to write counterfeit data; </p><p>Option C is wrong as it would put credentials directly into the application, which is strongly discouraged because applications can be decompiled which can compromise the keys. </p><p>Option D is wrong as it does not meet our availability requirements: although the EC2 instances are running in different Availability Zones, the EBS volumes attached to each instance only store data in a single Availability Zone. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121321">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 17 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251310"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You are using QuickSight to identify demand trends over multiple months for your top five product lines. Which type of visualization do you choose?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 3 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Scatter Plot</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Pie Chart<br><b>C</b>. <input type="radio" disabled="">&nbsp;Pivot Table<br><b>D</b>. <input type="radio" disabled="">&nbsp;Line Chart <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Line Chart<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as you need to represent time driven data for demand trends over multiple months, Line chart would be an ideal choice. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/quicksight/latest/user/working-with-visual-types.html" target="_blank">QuickSight Visual Types</a> </p><p>Option A is wrong as Scatter plots can help to visualize two or three measures for a dimension. </p><p>Option B is wrong as Pie charts can help to compare values for items in a dimension.<br> </p><p>Option C is wrong as Pivot tables can help to show measure values for the intersection of two dimensions.<br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23123033">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 18 of 50 </b><small>- Multiple Answer</small> <span style="float:right;" class="grad_5251311"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company is storing data on Amazon Simple Storage Service (S3). The company’s security policy mandates that data be encrypted at rest. Which of the following methods can achieve this? Choose 3 answers<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="checkbox" checked="true" disabled="">&nbsp;Use Amazon S3 server-side encryption with AWS Key Management Service managed keys.</span><br><b>B</b>. <input type="checkbox" disabled="">&nbsp;Use Amazon S3 server-side encryption with customer-provided keys<br><b>C</b>. <input type="checkbox" disabled="">&nbsp;Use Amazon S3 server-side encryption with EC2 key pair.<br><b>D</b>. <input type="checkbox" disabled="">&nbsp;Use Amazon S3 bucket policies to restrict access to the data at rest.<br><b>E</b>. <input type="checkbox" disabled="">&nbsp;Encrypt the data on the client-side before ingesting to Amazon S3 using their own master key<br><b>F</b>. <input type="checkbox" disabled="">&nbsp;Use SSL to encrypt the data while in transit to Amazon S3. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Use Amazon S3 server-side encryption with AWS Key Management Service managed keys.<br><b>B</b>. Use Amazon S3 server-side encryption with customer-provided keys<br><b>E</b>. Encrypt the data on the client-side before ingesting to Amazon S3 using their own master key<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answers are <strong>A, B &amp; E</strong> </p><p>Refer to the AWS <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html" target="_blank">S3 Protecting Data using Encryption</a> </p><p>Data at rest encryption using S3 can be implemented using either Server Side or Client Side encryption. SSE can be implemented using either KMS provided keys (SSE-KMS) or Customer provided keys (SSE-C). CSE can be implemented by encrypting the data before uploading it to S3 and then decrypting the data after downloading it from S3 at client side. </p><p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html" target="_blank"></a> </p><p>Option C is wrong as server side encryption doesn't work with EC2 key pair<br> </p><p>Option D is wrong as bucket policies are just to restrict access to S3 </p><p>Option F is wrong as it targets the data in transit only. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23123036">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 19 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251312"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company that provides economics data dashboards needs to be able to develop software to display rich, interactive, data-driven graphics that run in web browsers and leverages the full stack of web standards (HTML, SVG, and CSS). Which technology provides the most appropriate support for this requirements?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;D3.js</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;IPython/Jupyter<br><b>C</b>. <input type="radio" disabled="">&nbsp;R Studio<br><b>D</b>. <input type="radio" disabled="">&nbsp;Hue <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. D3.js<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as <em><strong>D3.js</strong> is a JavaScript library for manipulating documents based on data. <strong>D3</strong> helps you bring data to life using HTML, SVG, and CSS. D3’s emphasis on web standards gives you the full capabilities of modern browsers without tying yourself to a proprietary framework, combining powerful visualization components and a data-driven approach to DOM manipulation.</em> </p><p>Option B is wrong as Jupyter is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. It is not a rich visualization tool which supports all the web standards. </p><p>Option C is wrong as RStudio makes R easier to use. It includes a code editor, debugging &amp; visualization tools.<br> </p><p>Option D is wrong as Hue is the open source analytics workbench designed for fast data discovery, intelligent query assistance, and seamless collaboration<br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121409">AWS BDS-C00 Question feedback</a></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 20 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251313"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An enterprise customer is migrating to Redshift and is considering using dense storage nodes in its Redshift cluster. The customer wants to migrate 50 TB of data. The customer’s query patterns involve performing many joins with thousands of rows. The customer needs to know how many nodes are needed in its target Redshift cluster. The customer has a limited budget and needs to avoid performing tests unless absolutely needed. Which approach should this customer use?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Start with many small nodes.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Start with fewer large nodes.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Have two separate clusters with a mix of a small and large nodes.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Insist on performing multiple tests to determine the optimal configuration. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Start with many small nodes.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as the customer is planning to use Dense Storage nodes, they can start with more number of small nodes which would be cost-effective as compared to large nodes and easier to improve query performance and storage. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html#rs-about-clusters-and-nodes" target="_blank">Redshift Cluster &amp; Nodes</a> </p><p><em>DS2 node types are optimized for large data workloads and use hard disk drive (HDD) storage. Node types are available in different sizes. DS2 nodes are available in xlarge and 8xlarge sizes.<br></em> </p><p><em>The number of nodes that you choose depends on the size of your dataset and your desired query performance. Using the dense storage node types as an example, if you have 32 TB of data, you can choose either 16 ds2.xlarge nodes or 2 ds2.8xlarge nodes. If your data grows in small increments, choosing the ds2.xlarge node size allows you to scale in increments of 2 TB. If you typically see data growth in larger increments, a ds2.8xlarge node size might be a better choice.</em> </p><p><em>Because Amazon Redshift distributes and executes queries in parallel across all of a cluster’s compute nodes, you can increase query performance by adding nodes to your cluster. Amazon Redshift also distributes your data across all compute nodes in a cluster. When you run a cluster with at least two compute nodes, data on each node will always be mirrored on disks on another node and you reduce the risk of incurring data loss.</em> </p><p>Option B is wrong as with 50TB you would need 4 large nodes and it would not be as cost effective as small nodes. </p><p>Options C &amp; D are wrong as it is not meet the customer requirement of limited budget and avoid performing tests unless absolutely needed. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121401">AWS BDS-C00 Question feedback</a></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 21 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251314"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> ABCD has developed a sensor intended to be placed inside of people's shoes, monitoring the number of steps taken every day. ABCD is expecting thousands of sensors reporting in every minute and hopes to scale to millions by the end of the year. A requirement for the project is it needs to be able to accept the data, run it through ETL to store in warehouse and archive it on Amazon Glacier, with room for a real-time dashboard for the sensor data to be added at a later date. What is the best method for architecting this application given the requirements? Choose the correct answer:<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Use Amazon Cognito to accept the data when the user pairs the sensor to the phone, and then have Cognito send the data to Dynamodb. Use Data Pipeline to create a job that takes the DynamoDB tablee and sends it to an EMR cluster for ETL, then outputs to Redshift and S3 while, using S3 lifecycle policies to archive on Glacier.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Write the sensor data directly to a scaleable DynamoDB; create a data pipeline that starts an EMR cluster using data from DynamoDB and sends the data to S3 and Redshift.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Write the sensor data to Amazon S3 with a lifecycle policy for Glacier, create an EMR cluster that uses the bucket data and runs it through ETL. It then outputs that data into Redshift data warehouse.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Write the sensor data directly to Amazon Kinesis and output the data into Amazon S3 creating a lifecycle policy for Glacier archiving. Also, have a parallel processing application that runs the data through EMR and sends to a Redshift data warehouse. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Write the sensor data directly to Amazon Kinesis and output the data into Amazon S3 creating a lifecycle policy for Glacier archiving. Also, have a parallel processing application that runs the data through EMR and sends to a Redshift data warehouse.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as the requirement is real time data ingestion and analytics, the best option is to use Kinesis for storing the real time incoming data. The data can then be moved to S3 and analyzed using EMR and Redshift. Data can then be moved to Glacier for archival. </p><p>Refer AWS documentation - <a href="http://docs.aws.amazon.com/streams/latest/dev/introduction.html" target="_blank">Kinesis</a> </p><p><em>Amazon Kinesis is a platform for streaming data on AWS, making it easy to load and analyze streaming data, and also providing the ability for you to build custom streaming data applications for specialized needs.</em> </p><ul> <li><em>Use Amazon Kinesis Streams to collect and process large streams of data records in real time.</em></li> </ul><p><em></em> </p><ul> <li><em>Use Amazon Kinesis Firehose to deliver real-time streaming data to destinations such as Amazon S3 and Amazon Redshift.<br></em></li> <li><em>Use Amazon Kinesis Analytics to process and analyze streaming data with standard SQL.</em></li> </ul><p>Option A is wrong as Cognito is not suitable for handling real time data<br> </p><p><em>Amazon Cognito lets you easily add user sign-up and sign-in and manage permissions for your mobile and web apps. You can create your own user directory within Amazon Cognito, or you can authenticate users through social identity providers such as Facebook, Twitter, or Amazon; with SAML identity solutions; or by using your own identity system. In addition, Amazon Cognito enables you to save data locally on users' devices, allowing your applications to work even when the devices are offline. You can then synchronize data across users' devices so that their app experience remains consistent regardless of the device they use.</em><em></em><br> </p><p>Option B is wrong as DynamoDB is not suitable for streaming data ingestion and handling.<br> </p><p>Option C is wrong as S3 is not an ideal solution to handle this huge amount of requests. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121261">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 22 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251315"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You need to visualize data from Spark and Hive running on an EMR cluster. Which of the options is best for an interactive and collaborative notebook for data exploration?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 3 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Hive</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;D3.js<br><b>C</b>. <input type="radio" disabled="">&nbsp;Kinesis Analytics<br><b>D</b>. <input type="radio" disabled="">&nbsp;Zeppelin <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Zeppelin<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as Zeppelin provides data ingestion, data discovery, data analytics and data visualization &amp; collaboration. </p><p>Refer documentation - <a href="https://zeppelin.apache.org/" target="_blank">Zeppelin</a> </p><p><em>Zeppelin is a web-based notebook that enables data-driven, interactive data analytics and collaborative documents with SQL, Scala and more.</em> </p><p><em>Apache Zeppelin interpreter concept allows any language/data-processing-backend to be plugged into Zeppelin. Currently Apache Zeppelin supports many interpreters such as Apache Spark, Python, JDBC, Markdown and Shell.<br></em> </p><p><em>Some basic charts are already included in Apache Zeppelin. Visualizations are not limited to SparkSQL query, any output from any language backend can be recognized and visualized.</em><em></em><br> </p><p>Option A is wrong as Hive is data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. </p><p>Option B is wrong as D3.js a JavaScript library for manipulating documents based on data. D3 helps you bring data to life using HTML, SVG, and CSS.<br> </p><p>Option C is wrong as Kinesis Data Analytics is the easiest way to analyze streaming data, gain actionable insights, and respond to your business and customer needs in real time. <br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23123015">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 23 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251316"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> Your company needs to design a data warehouse for a client in the retail industry. The data warehouse will store historic purchases in Amazon Redshift. To comply with PCI:DSS requirements and meet data protection standards, the data must be encrypted at rest and have keys managed by a corporate on-premises HSM. How can you meet these requirements in a cost-effective manner?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Use AWS Import/Export to import a company HSM device into AWS alongside the Amazon Redshift cluster, and configure Redshift to use the imported HSM.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Create a VPN connection between a VPC you create in AWS and an on-premises network. Then launch the Redshift cluster in the VPC, and configure it to use your corporate HSM.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Use the AWS CloudHSM service to establish a trust relationship between the CloudHSM and the corporate HSM over a Direct Connect connection. Configure Amazon Redshift to use the CloudHSM device.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Configure the AWS Key Management Service to point to the corporate HSM device, and then launch the Amazon Redshift cluster with the KMS managing the encryption keys. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Create a VPN connection between a VPC you create in AWS and an on-premises network. Then launch the Redshift cluster in the VPC, and configure it to use your corporate HSM.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as Redshift Cluster can integrate with corporate HSM via VPN in a cost-effective way </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html" target="_blank">Redshift Encryption</a> </p><p><em>In Amazon Redshift, you can enable database encryption for your clusters to help protect data at rest. When you enable encryption for a cluster, the data blocks and system metadata are encrypted for the cluster and its snapshots.<br></em> </p><p><em>You can enable encryption when you launch your cluster, or you can modify an unencrypted cluster to use AWS Key Management Service (AWS KMS) encryption. To do so, you can use either an AWS-managed key or a customer-managed key (CMK). When you modify your cluster to enable KMS encryption, Amazon Redshift automatically migrates your data to a new encrypted cluster. Snapshots created from the encrypted cluster are also encrypted.<br></em> </p><p><em>Amazon Redshift uses a hierarchy of encryption keys to encrypt the database. You can use either AWS Key Management Service (AWS KMS) or a hardware security module (HSM) to manage the top-level encryption keys in this hierarchy. The process that Amazon Redshift uses for encryption differs depending on how you manage keys. Amazon Redshift automatically integrates with AWS KMS but not with an HSM. When you use an HSM, you must use client and server certificates to configure a trusted connection between Amazon Redshift and your HSM.</em><em></em><br> </p><p>Option A is wrong as importing HSM to AWS would not be secure as it against the requirement. </p><p>Option C is wrong as Direct Connect would be cheap and quick to start with.<br> </p><p>Option D is wrong as CloudHSM cannot connect to on-premises.<br> </p><p><em>Does CloudHSM work with on-premises HSMs? - </em><em>Yes. While CloudHSM does not interoperate directly with on-premises HSMs, you can securely transfer exportable keys between CloudHSM and most commercial HSMs using one of several supported RSA key wrap methods. </em> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23124748">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 24 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251317"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company wants to use Redshift cluster for petabyte-scale data warehousing. Data for processing would be stored on Amazon S3. As a security requirement, the company wants the data to be encrypted at rest. As a solution architect how would you implement the solution?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Store the data in S3 with Server Side Encryption and copy the data over to Redshift cluster</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Store the data in S3. Launch an encrypted Redshift cluster, copy the data to the Redshift cluster and store back in S3 in encrypted format<br><b>C</b>. <input type="radio" disabled="">&nbsp;Store the data in S3 with Server Side Encryption. Launch an encrypted Redshift cluster and copy the data to the cluster.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Store the data in S3 with Server Side Encryption. Launch a Redshift cluster, copy the data to cluster and enable encryption on the cluster. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Store the data in S3 with Server Side Encryption. Launch an encrypted Redshift cluster and copy the data to the cluster.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as the need is for data at rest encryption. S3 with SSE will help store the data in S3 in encrypted format. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html" target="_blank">Redshift Encryption</a> &amp; <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html" target="_blank">S3 Encryption</a> </p><p><em></em> </p><p><em>In Amazon Redshift, you can enable database encryption for your clusters to help protect data at rest. When you enable encryption for a cluster, the data blocks and system metadata are encrypted for the cluster and its snapshots.<br> </em> </p><p><em>Encryption is an optional, immutable setting of a cluster. If you want encryption, you enable it during the cluster launch process. To go from an unencrypted cluster to an encrypted cluster or the other way around, unload your data from the existing cluster and reload it in a new cluster with the chosen encryption setting. </em> </p><p>Option A is wrong as data is not encrypted in Redshift. </p><p>Option B is wrong as data is not encrypted in S3.<br> </p><p>Option D is wrong as you cannot enable encryption after Redshift cluster is launched.<br> </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23123036">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 25 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251318"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An organization needs a data store to handle the following data types and access patterns: Key-value access pattern Complex SQL queries and transactions Consistent reads Fixed schema Which data store should the organization choose? <br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Amazon S3</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Amazon Kinesis<br><b>C</b>. <input type="radio" disabled="">&nbsp;Amazon DynamoDB<br><b>D</b>. <input type="radio" disabled="">&nbsp;Amazon RDS <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Amazon RDS<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is D as Amazon RDS handles all these requirements, and although Amazon RDS is not typically thought of as optimized for key-value based access, a schema with a good primary key selection can provide this functionality. </p><p>Option A is wrong as Amazon S3 provides no fixed schema and does not have consistent read after PUT support. </p><p>Option B is wrong as Amazon Kinesis supports streaming data that is consistent as of a given sequence number but doesn't provide key/value access. </p><p>Option C is wrong as Amazon DynamoDB provides key/value access and consistent reads, it does not support SQL-based queries. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121318">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 26 of 50 </b><small>- Multiple Answer</small> <span style="float:right;" class="grad_5251319"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A video-sharing mobile application uploads files greater than 10 GB to an Amazon S3 bucket. However, when using the application in locations far away from the S3 bucket region, uploads take extended periods of time, and sometimes fail to complete. Which combination of methods would improve the performance of uploading to the application? (Select TWO.)<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="checkbox" checked="true" disabled="">&nbsp;Configure an S3 bucket in each region to receive the uploads, and use cross-region replication to copy the files to the distribution bucket.</span><br><b>B</b>. <input type="checkbox" disabled="">&nbsp;Modify the application to add random prefixes to the files before uploading.<br><b>C</b>. <input type="checkbox" disabled="">&nbsp;Set up Amazon Route 53 with latency-based routing to route the uploads to the nearest S3 bucket region.<br><b>D</b>. <input type="checkbox" disabled="">&nbsp;Enable S3 Transfer Acceleration on the S3 bucket, and configure the application to use the Transfer Acceleration endpoint for uploads.<br><b>E</b>. <input type="checkbox" disabled="">&nbsp;Configure the application to break the video files into chunks and use a multipart upload to transfer files to Amazon S3. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Enable S3 Transfer Acceleration on the S3 bucket, and configure the application to use the Transfer Acceleration endpoint for uploads.<br><b>E</b>. Configure the application to break the video files into chunks and use a multipart upload to transfer files to Amazon S3.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answers are <strong>D &amp; E</strong> </p><p>Option D as <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html" target="_blank">S3 Transfer Acceleration</a> helps speed up the upload performance. <em>Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.</em> </p><p>Option E as <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html">multipart upload</a> helps provide better recoverability. </p><p><em>Depending on the size of the data you are uploading, Amazon S3 offers the following options:</em> </p><ul> <li><em><strong>Upload objects in a single operation—</strong>With a single PUT operation, you can upload objects up to 5 GB in size.</em></li> <li><em><strong>Upload objects in parts—</strong>Using the multipart upload API, you can upload large objects, up to 5 TB.The multipart upload API is designed to improve the upload experience for larger objects. You can upload objects in parts. These object parts can be uploaded independently, in any order, and in parallel. You can use a multipart upload for objects from 5 MB to 5 TB in size.</em></li> </ul><p><em>We recommend that you use multipart uploading in the following ways:</em> </p><p><em></em> </p><ul> <li><em>If you're uploading large objects over a stable high-bandwidth network, use multipart uploading to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance.</em></li> <li><em>If you're uploading over a spotty network, use multipart uploading to increase resiliency to network errors by avoiding upload restarts. When using multipart uploading, you need to retry uploading only parts that are interrupted during the upload. You don't need to restart uploading your object from the beginning.</em></li> </ul><p>Option A is wrong as the mobile application needs to be configured for different endpoints and does not improve performance. Also, cross region replication would create duplication and increase cost. </p><p>Option B is wrong as random prefixes are no more needed for improving performance.<br> </p><p>Option C is wrong as Route 53 latency based routing works only with S3 static website.<br> </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121247">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 27 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251320"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company is collected real time senstive data using Amazon Kinesis. As a security requirement, the Amazon Kinesis stream needs to be encrypted. Which approach should be used to accomplish this task?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Perform a client-side encryption of the data before it enters the Amazon Kinesis stream on the producer.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use a partition key to segment the data by MD5 hash function, which makes it undecipherable while in transit.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Perform a client-side encryption of the data before it enters the Amazon Kinesis stream on the consumer.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use a shard to segment the data, which has built-in functionality to make it indecipherable while in transit. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Perform a client-side encryption of the data before it enters the Amazon Kinesis stream on the producer.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as the data can be encrypted using client side encryption. The encryption needs to be done on the producer before the data is pushed to Kinesis Streams. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/blogs/big-data/encrypt-and-decrypt-amazon-kinesis-records-using-aws-kms/" target="_blank">Kinesis Encrypt and Decrypt Data</a> </p><p><img src="./bds1_files/encrypt_decrypt_1_1_1.gif"><br>Options B &amp; D are wrong as they do not provide encryption.<br> </p><p>Option C is wrong as the encryption needs to happen at the producer side. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121390">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 28 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251321"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A customer has a machine learning workflow that consists of multiple quick cycles of reads-writes-reads on Amazon S3. The customer needs to run the workflow on EMR but is concerned that the reads in subsequent cycles will miss new data critical to the machine learning from the prior cycles. How should the customer accomplish this?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Use AWS Data Pipeline to orchestrate the data processing cycles.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Turn on EMRFS consistent view when configuring the EMR cluster.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Set hadoop.data.consistency=true in the core-site.xml file. <br><b>D</b>. <input type="radio" disabled="">&nbsp;Set hadoop.s3.consistency=true in the core-site.xml file. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Turn on EMRFS consistent view when configuring the EMR cluster.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as EMRFS Consistent View helps provide a view of the objects in S3 and also tracks the consistency. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-consistent-view.html" target="_blank">EMRFS Consistent View</a> </p><p><em>EMRFS consistent view is an optional feature available when using Amazon EMR release version 3.2.1 or later. Consistent view allows EMR clusters to check for list and read-after-write consistency for Amazon S3 objects written by or synced with EMRFS. Consistent view addresses an issue that can arise due to the <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel">Amazon S3 Data Consistency Model</a>. For example, if you add objects to Amazon S3 in one operation and then immediately list objects in a subsequent operation, the list and the set of objects processed may be incomplete. This is more commonly a problem for clusters that run quick, sequential steps using Amazon S3 as a data store, such as multi-step extract-transform-load (ETL) data processing pipelines.</em><em></em><br> </p><p><em>When you create a cluster with consistent view enabled, Amazon EMR uses an Amazon DynamoDB database to store object metadata and track consistency with Amazon S3. If consistent view determines that Amazon S3 is inconsistent during a file system operation, it retries that operation according to rules that you can define.<br></em> </p><p><em>With consistent view enabled, EMRFS returns the set of objects listed in an EMRFS metadata store and those returned directly by Amazon S3 for a given path. Because Amazon S3 is still the “source of truth” for the objects in a path, EMRFS ensures that everything in a specified Amazon S3 path is being processed regardless of whether it is tracked in the metadata. However, EMRFS consistent view only ensures that the objects in the folders that you track are checked for consistency.<br></em> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121397">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 29 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251322"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> Managers in a company need access to the human resources database that runs on Amazon Redshift, to run reports about their employees. Managers must only see information about their direct reports. Which technique should be used to address this requirement with Amazon Redshift?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 9 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Define an IAM group for each manager with each employee as an IAM user in that group, and use that to limit the access.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use Amazon Redshift snapshot to create one cluster per manager. Allow the manager to access only their designated clusters.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Define a key for each manager in AWS KMS and encrypt the data for their employees with their private keys.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Define a view that uses the employee’s manager name to filter the records based on current user names. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Define a view that uses the employee’s manager name to filter the records based on current user names.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as you can create a view in Redshift which filters the records based on the current user name and show only those results for the logged in user. </p><p>Option A is wrong as IAM group with users cannot limit the access to the data in Redshift. </p><p>Option B is wrong as it does not limit or filter the data and it is not a cost-effective solution. </p><p>Option C is wrong as encryption cannot be done at each employee record level. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121398">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 30 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251323"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company with a support organization needs support engineers to be able to search historic cases to provide fast responses on new issues raised. The company has forwarded all support messages into an Amazon Kinesis Stream. This meets a company objective of using only managed services to reduce operational overhead. The company needs an appropriate architecture that allows support engineers to search on historic cases and find similar issues and their associated responses. Which AWS Lambda action is most appropriate?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 9 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Ingest and index the content into an Amazon Elasticsearch domain.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Stem and tokenize the input and store the results into Amazon ElastiCache.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Write data as JSON into Amazon DynamoDB with primary and secondary indexes.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Aggregate feedback in Amazon S3 using a columnar format with partitioning. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Ingest and index the content into an Amazon Elasticsearch domain.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as Elasticsearch provides full text search capability and is a fully managed AWS service. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/es-aws-integrations.html" target="_blank">Elasticsearch Integration</a> </p><p><em>You can load <a href="http://aws.amazon.com/streaming-data/" target="_blank">streaming data</a> into your Amazon Elasticsearch Service domain from many different sources. Some sources, like Amazon Kinesis Data Firehose and Amazon CloudWatch Logs, have built-in support for Amazon ES. Others, like Amazon S3, Amazon Kinesis Data Streams, and Amazon DynamoDB, use AWS Lambda functions as event handlers. The Lambda functions respond to new data by processing it and streaming it to your domain.</em><br> </p><p>Option B is wrong as Elasticache do not provide search capability, but more of a key value lookup and caching ability. </p><p>Option C is wrong as its not easy and cost-efficient to search on DynamoDB </p><p>Option D is wrong as S3 does not provide any search capability.<br> </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121378">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 31 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251324"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An online retailer is using Amazon DynamoDB to store data related to customer transactions. The items in the table contains several string attributes describing the transaction as well as a JSON attribute containing the shopping cart and other details corresponding to the transaction. Average item size is – 250KB, most of which is associated with the JSON attribute. The average customer generates – 3GB of data per month. Customers access the table to display their transaction history and review transaction details as needed. Ninety percent of the queries against the table are executed when building the transaction history view, with the other 10% retrieving transaction details. The table is partitioned on CustomerID and sorted on transaction date. The client has very high read capacity provisioned for the table and experiences very even utilization, but complains about the cost of Amazon DynamoDB compared to other NoSQL solutions. Which strategy will reduce the cost associated with the client’s read queries while not degrading quality?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 6 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <b>A</b>. <input type="radio" disabled="">&nbsp;Modify all database calls to use eventually consistent reads and advise customers that transaction history may be one second out-of-date.<br><b>B</b>. <input type="radio" disabled="">&nbsp;Change the primary table to partition on TransactionID, create a GSI partitioned on customer and sorted on date, project small attributes into GSI, and then query GSI for summary data and the primary table for JSON details.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Vertically partition the table, store base attributes on the primary table, and create a foreign key reference to a secondary table containing the JSON data. Query the primary table for summary data and the secondary table for JSON details.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Create an LSI sorted on date, project the JSON attribute into the index, and then query the primary table for summary data and the LSI for JSON details. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Change the primary table to partition on TransactionID, create a GSI partitioned on customer and sorted on date, project small attributes into GSI, and then query GSI for summary data and the primary table for JSON details.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as the key requirement is to reduce cost without affecting quality. The issue here is the JSON data is being read always even though it is not needed 90% of time. As the data size for JSON is huge compared to other attributes, the provisioned throughput needed is high. The issue can be resolved by retrieving only the details for history and the transaction details when needed. Creating a GSI for the base transaction history and using the primary for transaction summary would work perfectly. </p><p>Option A is wrong as it would affect the quality. </p><p>Option C is wrong as it is not possible. </p><p>Option D is wrong as the LSI needs to be created with the sort key as transaction id instead of date. This will allow to retrieve the transaction details from LSI and summary from primary table. </p><p>Also, note, both options B &amp; D need changes to the base table as you cannot change the primary key of the table once created and nor can you create a LSI after the table is created. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121393">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 32 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251325"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> Your client needs to load a 600 GB file into a Redshift cluster from S3, using the Redshift COPY command. The file has several known (and potentially some unknown) issues that will probably cause the load process to fail. How should the client most efficiently detect load errors without needing to perform cleanup if the load process fails?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Split the 600 GB file into smaller 25 GB chunks and load each separately.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Compress the input file before running COPY.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Write a script to delete the data from the tables in case of errors.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use the COPY command with the NOLOAD parameter. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Use the COPY command with the NOLOAD parameter.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as <em>NOLOAD checks the integrity of all of the data without loading it into the database. The NOLOAD option displays any errors that would occur if you had attempted to load the data. All other options will require subsequent processing on the cluster which will consume resources.</em> </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-load.html#copy-noload" target="_blank">Data Load Copy Parameters</a> </p><p><em>If you want to validate your data without actually loading the table, use the NOLOAD option with the <a href="https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html">COPY</a> command.</em><br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23125172">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 33 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251326"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company that manufactures and sells smart air conditioning units also offers add-on services so that customers can see real-time dashboards in a mobile application or a web browser. Each unit sends its sensor information in JSON format every two seconds for processing and analysis. The company also needs to consume this data to predict possible equipment problems before they occur. A few thousand pre-purchased units will be delivered in the next couple of months. The company expects high market growth in the next year and needs to handle a massive amount of data and scale without interruption. Which ingestion solution should the company use?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <b>A</b>. <input type="radio" disabled="">&nbsp;Write sensor data records to Amazon Kinesis Streams. Process the data using KCL applications for the end-consumer dashboard and anomaly detection workflows.<br><b>B</b>. <input type="radio" disabled="">&nbsp;Batch sensor data to Amazon Simple Storage Service (S3) every 15 minutes. Flow the data downstream to the end-consumer dashboard and to the anomaly detection application.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Write sensor data records to Amazon Kinesis Firehose with Amazon Simple Storage Service (S3) as the destination. Consume the data with a KCL application for the end-consumer dashboard and anomaly detection.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Write sensor data records to Amazon Relational Database Service (RDS). Build both the end-consumer dashboard and anomaly detection application on top of Amazon RDS. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Write sensor data records to Amazon Kinesis Streams. Process the data using KCL applications for the end-consumer dashboard and anomaly detection workflows.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as Kinesis Data Streams can help handle the streaming data. Kinesis Streams provides you with the ability to build custom applications to process and analyze streaming data using KCL which can be used for anomaly detection and processing. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/streams/latest/dev/introduction.html" target="_blank">Kinesis Streams</a> </p><p><img src="./bds1_files/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png"><br><span class="redactor-invisible-space"><br></span> </p><p><em>Although you can use Kinesis Data Streams to solve a variety of streaming data problems, a common use is the real-time aggregation of data followed by loading the aggregate data into a data warehouse or map-reduce cluster.</em> </p><p><em>Data is put into Kinesis data streams, which ensures durability and elasticity. The delay between the time a record is put into the stream and the time it can be retrieved (put-to-get delay) is typically less than 1 second. In other words, a Kinesis Data Streams application can start consuming the data from the stream almost immediately after the data is added. The managed service aspect of Kinesis Data Streams relieves you of the operational burden of creating and running a data intake pipeline. You can create streaming map-reduce–type applications. The elasticity of Kinesis Data Streams enables you to scale the stream up or down, so that you never lose data records before they expire.</em> </p><p><em>Multiple Kinesis Data Streams applications can consume data from a stream, so that multiple actions, like archiving and processing, can take place concurrently and independently. For example, two applications can read data from the same stream. The first application calculates running aggregates and updates an Amazon DynamoDB table, and the second application compresses and archives data to a data store like Amazon Simple Storage Service (Amazon S3). The DynamoDB table with running aggregates is then read by a dashboard for up-to-the-minute reports.</em> </p><p><em>The Kinesis Client Library enables fault-tolerant consumption of data from streams and provides scaling support for Kinesis Data Streams applications.</em> </p><p>Option B is wrong as S3 with batching would not provide near real time analysis on the stream data.<br> </p><p>Option C is wrong as Kinesis Firehose would only transfer the data to S3. It does not provide ability for KCL applications to work on the streaming data. </p><p>Option D is wrong as RDS is not ideal for real time streaming data. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121394">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 34 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251327"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A web application is using Amazon Kinesis Streams for clickstream data that may not be consumed for up to 12 hours. As a security requirement, how can the data be secured at rest within the Kinesis Streams?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Enable SSL connections to Kinesis</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use Amazon Kinesis Consumer Library<br><b>C</b>. <input type="radio" disabled="">&nbsp;Encrypt the data once it is at rest with a Lambda function<br><b>D</b>. <input type="radio" disabled="">&nbsp;Enable server-side encryption in Kinesis Streams <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Enable server-side encryption in Kinesis Streams<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as Kinesis support Server Side Encryption with which the data can be encrypted at rest. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/streams/latest/dev/what-is-sse.html" target="_blank">Kinesis Server Side Encryption</a> </p><p><em>Server-side encryption is a feature in Amazon Kinesis Data Streams that automatically encrypts data before it's at rest by using an AWS KMS customer master key (CMK) you specify. Data is encrypted before it's written to the Kinesis stream storage layer, and decrypted after it’s retrieved from storage. As a result, your data is encrypted at rest within the Kinesis Data Streams service. This allows you to meet strict regulatory requirements and enhance the security of your data.</em> </p><p><em>With server-side encryption, your Kinesis stream producers and consumers don't need to manage master keys or cryptographic operations. Your data is automatically encrypted as it enters and leaves the Kinesis Data Streams service, so your data at rest is encrypted. AWS KMS provides all the master keys that are used by the server-side encryption feature. AWS KMS makes it easy to use a CMK for Kinesis that is managed by AWS, a user-specified AWS KMS CMK, or a master key imported into the AWS KMS service.</em> </p><p>Option A is wrong as SSL/TLS is for Encryption for data in transit<br> </p><p>Option B is wrong as Kinesis consumer library is for consumption of data. </p><p>Option C is wrong as the data is not encrypted when stored </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121749">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 35 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251328"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You're launching a test Elasticsearch cluster with the Amazon Elasticsearch Service, and you'd like to restrict access to only your office desktop computer that you occasionally share with an intern to allow her to get more experience interacting with Elasticsearch. What's the easiest way to do this?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Create a username and password combination to allow you to sign into the cluster.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Create an SSH key and add that to the accepted keys of the Elasticsearch cluster. Then store that SSH key on your desktop and use it to sign in.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Create an IAM user and role that allows access to the Elasticsearch cluster.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Create an IP-based resource policy on the Elasticsearch cluster that allows access to requests coming from the IP of the machine. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Create an IP-based resource policy on the Elasticsearch cluster that allows access to requests coming from the IP of the machine.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as IP-based resource policy can restrict access to the specific IP addresses only. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/es-ac.html" target="_blank">Elasticsearch Access Control</a> </p><p><em><strong><em>IP-based Policies</em><span class="redactor-invisible-space"></span></strong> - IP-based policies restrict access to a domain to one or more IP addresses or CIDR blocks. Technically, IP-based policies are not a distinct type of policy. Instead, they are just resource-based policies that specify an anonymous principal and include a special <code>Condition</code> element.</em> </p><p><em>The primary appeal of IP-based policies is that they allow unsigned requests to an Amazon ES domain, which lets you use clients like curl and Kibana or access the domain through a proxy server.</em> </p><p>Options A &amp; B are user/password and ssh keys do not apply to Elasticsearch cluster. </p><p>Options C is wrong as you can define <a href="https://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/es-ac.html#es-ac-types-identity" target="_blank">identity based policies</a>, however it would not limit access to the specific IP. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23124741">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 36 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251329"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> Your application development team is building a solution with two applications. The security team wants each application's logs to be captured in two different places because one of the applications produces logs with sensitive data. How can you meet the requirements with the least risk and effort?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Aggregate logs into one file, then use Amazon CloudWatch Logs and then design two CloudWatch metric filters to filter sensitive data from the logs.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use Amazon CloudWatch logs to capture all logs, write an AWS Lambda function that parses the log file, and move sensitive data to a different log.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Add logic to the application that saves sensitive data logs on the Amazon EC2 instances' local storage, and write a batch script that logs into the EC2 instances and moves sensitive logs to a secure location.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use Amazon CloudWatch logs with two log groups, one for each application, and use an AWS IAM policy to control access to the log groups as required. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Use Amazon CloudWatch logs with two log groups, one for each application, and use an AWS IAM policy to control access to the log groups as required.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as different CloudWatch log groups can be created, which can have separate access control policies. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Working-with-log-groups-and-streams.html" target="_blank">CloudWatch Log Groups</a> </p><p><em>A log group is a group of log streams that share the same retention, monitoring, and access control settings. You can define log groups and specify which streams to put into each group. There is no limit on the number of log streams that can belong to one log group.</em> </p><p>Option A is wrong as the logs file are still combined and can be accessed by anyone who has access. </p><p>Option B is wrong as its an overhead to create a Lambda function. </p><p>Option C is wrong as this would need application changes. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23124742">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 37 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251330"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> There are thousands of text files on Amazon S3. The total size of the files is 1 PB. The files contain retail order information for the past 2 years. A data engineer needs to run multiple interactive queries to manipulate the data. The Data Engineer has AWS access to spin up an Amazon EMR cluster. The data engineer needs to use an application on the cluster to process this data and return the results in interactive time frame. Which application on the cluster should the data engineer use?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Oozie</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Apache Pig with Tachyon<br><b>C</b>. <input type="radio" disabled="">&nbsp;Apache Hive<br><b>D</b>. <input type="radio" disabled="">&nbsp;Presto <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Presto<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as Presto can help work on Petabytes of data with the interactive ability. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/emr/features/presto/" target="_blank">EMR Presto</a> </p><p><em>Presto is an open-source distributed SQL query engine optimized for low-latency, ad-hoc analysis of data. It supports the ANSI SQL standard, including complex queries, aggregations, joins, and window functions. Presto can process data from multiple data sources including the Hadoop Distributed File System (HDFS) and Amazon S3. </em> </p><p><em>You can quickly and easily create managed Presto clusters from the AWS Management Console, AWS CLI, or the Amazon EMR API. Additionally, you can leverage additional Amazon EMR features, including fast Amazon S3 connectivity, integration with Amazon EC2 Spot instances, choice of a wide variety of Amazon EC2 instances, including the memory optimized instances, and resize commands to easily add or remove instances from your cluster. </em> </p><p><em>Presto uses a custom query execution engine with operators designed to support SQL semantics. Different from Hive/MapReduce, Presto executes queries in memory, pipelined across the network between stages, thus avoiding unnecessary I/O. The pipelined execution model runs multiple stages in parallel and streams data from one stage to the next as it becomes available. <br></em> </p><p><em>Run interactive queries that directly access data in Amazon S3, save costs using Amazon EC2 Spot instance capacity, use Auto Scaling to dynamically add and remove capacity, and launch long-running or ephemeral clusters to match your workload. You can also add other Hadoop ecosystem applications on your cluster.<br></em> </p><p>Option A is wrong as Oozie is more of a schedular tool. </p><p>Options B &amp; C are wrong as they will work with Map Reduce. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121381">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 38 of 50 </b><small>- Multiple Answer</small> <span style="float:right;" class="grad_5251331"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company hosts a web application on AWS which uses RDS instance to store critical data. As a part of a security audit, it was recommended hardening of RDS instance. What actions would help achieve the same? (Select TWO)<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 6 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="checkbox" checked="true" disabled="">&nbsp;Use Secure Socket Layer (SSL) connections with DB instances</span><br><b>B</b>. <input type="checkbox" disabled="">&nbsp;Use AWS CloudTrail to track all the SSH access to the RDS instance<br><b>C</b>. <input type="checkbox" disabled="">&nbsp;Use AWS Inspector to apply patches to the RDS instance<br><b>D</b>. <input type="checkbox" disabled="">&nbsp;Use RDS encryption to secure the RDS instances and snapshots at rest. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Use Secure Socket Layer (SSL) connections with DB instances<br><b>D</b>. Use RDS encryption to secure the RDS instances and snapshots at rest.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer are <strong>A, D </strong>as the RDS security can tightened using SSL connection and encryption. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.html" target="_blank">RDS Security</a> </p><ul><em> <li>Run your DB instance in an Amazon Virtual Private Cloud (VPC) for the greatest possible network access control.</li> <li>Use AWS Identity and Access Management (IAM) policies to assign permissions that determine who is allowed to manage RDS resources. For example, you can use IAM to determine who is allowed to create, describe, modify, and delete DB instances, tag resources, or modify security groups.</li> <li>Use security groups to control what IP addresses or Amazon EC2 instances can connect to your databases on a DB instance. When you first create a DB instance, its firewall prevents any database access except through rules specified by an associated security group.</li> <li>Use Secure Socket Layer (SSL) connections with DB instances running the MySQL, MariaDB, PostgreSQL, Oracle, or Microsoft SQL Server database engines.</li> <li>Use RDS encryption to secure your RDS instances and snapshots at rest. RDS encryption uses the industry standard AES-256 encryption algorithm to encrypt your data on the server that hosts your RDS instance.</li> <li>Use network encryption and transparent data encryption with Oracle DB instances</li> <li>Use the security features of your DB engine to control who can log in to the databases on a DB instance, just as you do if the database was on your local network.</li> </em> </ul><p>Option B is wrong you cannot SSH into an RDS instance and CloudTrail does not track SSH logins. </p><p>Option C is wrong as the RDS instance is AWS managed. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23124746">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 39 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251332"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A data engineer chooses Amazon DynamoDB as a data store for a regulated application. This application must be submitted to regulators for review. The data engineer needs to provide a control framework that lists the security controls from the process to follow to add new users down to the physical controls of the data center, including items like security guards and cameras. How should this control mapping be achieved using AWS?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Request AWS third-party audit reports and/or the AWS quality addendum and map the AWS responsibilities to the controls that must be provided.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Request data center Temporary Auditor access to an AWS data center to verify the control mapping.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Request relevant SLAs and security guidelines for Amazon DynamoDB and define these guidelines within the application’s architecture to map to the control framework.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Request Amazon DynamoDB system architecture designs to determine how to map the AWS responsibilities to the control that must be provided. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Request AWS third-party audit reports and/or the AWS quality addendum and map the AWS responsibilities to the controls that must be provided.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as these are AWS specific and not accessible directly. AWS provides access to third party audit reports to confirm the same. </p><p>Refer AWS documentation - <a href="https://d0.awsstatic.com/whitepapers/compliance/AWS_Risk_and_Compliance_Whitepaper.pdf" target="_blank">Risk Compliance Whitepaper</a> </p><p><em></em> </p><p><em>AWS and its customers share control over the IT environment, both parties have responsibility for managing the IT environment. AWS’ part in this shared responsibility includes providing its services on a highly secure and controlled platform and providing a wide array of security features customers can use. The customers’ responsibility includes configuring their IT environments in a secure and controlled manner for their purposes. While customers don’t communicate their use and configurations to AWS, AWS does communicate its security and control environment relevant to customers. AWS does this by doing the following:<br> </em> </p><ul><em> <li>Obtaining industry certifications and independent third-party attestations described in this document</li> <li>Publishing information about the AWS security and control practices in whitepapers and web site content</li> <li>Providing certificates, reports, and other documentation directly to AWS customers under NDA (as required)</li> </em> </ul><p>Option B is wrong as AWS does not allow access to the data center. </p><p>Option C is wrong as security guidelines is not specific to DynamoDB and it is pretty much customer controlled. </p><p>Option D is wrong as AWS does not share any DynamoDB system architecture design document. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121332">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 40 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251333"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You need to filter and transform incoming messages coming from a smart sensor you have connected with AWS. Once messages are received, you need to store them as time series data in DynamoDB. Which AWS service can you use?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 6 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;IoT Device Shadow Service</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Redshift<br><b>C</b>. <input type="radio" disabled="">&nbsp;Kinesis<br><b>D</b>. <input type="radio" disabled="">&nbsp;IoT Rules Engine <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. IoT Rules Engine<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as IoT Rules Engine can be used to capture data from Sensor and data received from the device can be inserted into DynamoDB. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/iot/latest/developerguide/iot-rules.html" target="_blank">AWS IoT Rules</a> </p><p><em>Rules give your devices the ability to interact with AWS services. Rules are analyzed and actions are performed based on the MQTT topic stream. You can use rules to support tasks like these:</em> </p><ul> <li><em>Augment or filter data received from a device.</em></li> <li><em><strong>Write data received from a device to an Amazon DynamoDB database.</strong></em></li> <li><em>Save a file to Amazon S3.</em></li> <li><em>Send a push notification to all users using Amazon SNS.</em></li> <li><em>Publish data to an Amazon SQS queue.</em></li> <li><em>Invoke a Lambda function to extract data.</em></li> <li><em>Process messages from a large number of devices using Amazon Kinesis.</em></li> <li><em>Send data to the Amazon Elasticsearch Service.</em></li> <li><em>Capture a CloudWatch metric.</em></li> <li><em>Change a CloudWatch alarm.</em></li> <li><em>Send the data from an MQTT message to Amazon Machine Learning to make predictions based on an Amazon ML model.</em></li> <li><em>Send a message to a Salesforce IoT Input Stream.</em></li> <li><em>Send message data to an AWS IoT Analytics channel.</em></li> <li><em>Start execution of a Step Functions state machine.</em></li> <li><em>Send message data to an AWS IoT Events input.</em></li> </ul><p>Option A is wrong as a device's <em>shadow</em> is a JSON document that is used to store and retrieve current state information for a device. The Device Shadow service maintains a shadow for each device you connect to AWS IoT. You can use the shadow to get and set the state of a device over MQTT or HTTP, regardless of whether the device is connected to the Internet. Each device's shadow is uniquely identified by the name of the corresponding thing. </p><p>Option B is wrong as Redshift is a data warehousing solution.<br> </p><p>Option C is wrong as while Kinesis could technically be used as an intermediary between different sources, it isn't a great way to get data into DynamoDB from an IoT device.<br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121745">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 41 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251334"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An administrator is processing events in near real-time using Kinesis streams and Lambda. Lambda intermittently fails to process batches from one of the shards due to a 15-minute time limit. What is a possible solution for this problem?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 3 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Add more Lambda functions to improve concurrent batch processing.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Reduce the batch size that Lambda is reading from the stream.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Ignore and skip events that are older than 15 minutes and put them to Dead Letter Queue (DLQ).<br><b>D</b>. <input type="radio" disabled="">&nbsp;Configure Lambda to read from fewer shards in parallel. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Reduce the batch size that Lambda is reading from the stream.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as Lambda reads in batches from Kinesis from a single shard, and hence it might timeout if the batch of records is huge. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html" target="_blank">Lambda with Kinesis</a> </p><p><em>You can use an AWS Lambda function to process records in an <a href="https://docs.aws.amazon.com/kinesis/latest/dev/amazon-kinesis-streams.html">Amazon Kinesis data stream</a>. With Kinesis, you can collect data from many sources and process them with multiple consumers. Lambda supports standard data stream iterators and HTTP/2 stream consumers.</em> </p><p><em>Lambda reads records from the data stream and invokes your function <a href="https://docs.aws.amazon.com/lambda/latest/dg/invocation-options.html">synchronously</a> with an event that contains stream records. Lambda reads records in batches and invokes your function to process records from the batch.</em> </p><p><em>Your Lambda function is a consumer application for your data stream. It processes one batch of records at a time from each shard.<br></em> </p><p><em>For standard iterators, Lambda polls each shard in your Kinesis stream for records at a base rate of once per second. When more records are available, Lambda keeps processing batches until it receives a batch that's smaller than the configured maximum batch size. The function shares read throughput with other consumers of the shard.<br></em> </p><p>Option A is wrong as adding Lambda function does not reduce the batch of records read from Kinesis. </p><p>Option C is wrong as ignoring the data would lead to data loss and does not solve the problem<br> </p><p>Option D is wrong as Lambda function reads from a single Kinesis shard.<br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121384">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 42 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251335"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company is using Kinesis data streams to store the log data, which is processed by an application every 12 hours. As the data needs to reside in Kinesis data streams for 12 hours, the Security team wants the data to be encrypted at rest. How can it be secured in a most efficient way?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 6 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Kinesis does not support encryption</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Encrypt using SSL/TLS for encrypting the data. <br><b>C</b>. <input type="radio" disabled="">&nbsp;Encrypt using S3 Server Side Encryption.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Encrypt using Kinesis Server Side Encryption. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Encrypt using Kinesis Server Side Encryption.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as Kinesis support Server Side Encryption with which the data can be encrypted at rest. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/streams/latest/dev/what-is-sse.html" target="_blank">Kinesis Server Side Encryption</a> </p><p><em>Server-side encryption is a feature in Amazon Kinesis Data Streams that automatically encrypts data before it's at rest by using an AWS KMS customer master key (CMK) you specify. Data is encrypted before it's written to the Kinesis stream storage layer, and decrypted after it’s retrieved from storage. As a result, your data is encrypted at rest within the Kinesis Data Streams service. This allows you to meet strict regulatory requirements and enhance the security of your data.<br> </em> </p><p><em>With server-side encryption, your Kinesis stream producers and consumers don't need to manage master keys or cryptographic operations. Your data is automatically encrypted as it enters and leaves the Kinesis Data Streams service, so your data at rest is encrypted. AWS KMS provides all the master keys that are used by the server-side encryption feature. AWS KMS makes it easy to use a CMK for Kinesis that is managed by AWS, a user-specified AWS KMS CMK, or a master key imported into the AWS KMS service. </em> </p><p>Option A is wrong as Kinesis supports encryption at rest </p><p>Option B is wrong as SSL/TLS is for Encryption for data in transit </p><p>Option C is wrong as S3 SSE does not work with Kinesis<br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121750">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 43 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251336"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company needs a churn prevention model to predict which customers will NOT renew their yearly subscription to the company’s service. The company plans to provide these customers with a promotional offer. A binary classification model that uses Amazon Machine Learning is required. On which basis should this binary classification model be built?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;User profiles (age, gender, income, occupation)</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Last user session<br><b>C</b>. <input type="radio" disabled="">&nbsp;Each user time series events in the past 3 months<br><b>D</b>. <input type="radio" disabled="">&nbsp;Quarterly results <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Each user time series events in the past 3 months<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as the time series data regarding the usage of the customer can give insights and help build and train the model </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/blogs/machine-learning/predicting-customer-churn-with-amazon-machine-learning/" target="_blank">AWS Machine Learning Churn prediction</a> </p><p>Options A &amp; D are wrong as they do not give any idea of customer behaviour. </p><p>Option B is wrong as the data is too limited. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121408">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 44 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251337"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company launched EMR cluster to support their big data analytics requirements. They have multiple data sources built out of S3, SQL databases, MongoDB, Redis, RDS, other file systems. They are looking for distributed processing framework and programming model that helps you do machine learning, stream processing, or graph analytics using Amazon EMR clusters Which EMR Hadoop ecosystem fulfils the requirements?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 6 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Apache Hive</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Apache HBase<br><b>C</b>. <input type="radio" disabled="">&nbsp;Apache HCatalog<br><b>D</b>. <input type="radio" disabled="">&nbsp;Apache Spark <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Apache Spark<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark.html" target="_blank">Apache Spark</a> is a distributed processing framework and programming model that helps you do machine learning, stream processing, or graph analytics using Amazon EMR clusters. Similar to Apache Hadoop, Spark is an open-source,distributed processing system commonly used for big data workloads. However, Spark has several notable differences from Hadoop MapReduce. Spark has an optimized directed acyclic graph (DAG) execution engine and actively caches data in-memory, which can boost performance, especially for certain algorithms and interactive queries. <br> </p><p>Option A is wrong as <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive.html" target="_blank">Hive</a> is an open-source, data warehouse, and analytic package that runs on top of a Hadoop cluster. Hive scripts use an SQL-like language called Hive QL (query language) that abstracts programming models and supports typical data warehouse interactions. Hive enables you to avoid the complexities of writing Tez jobs based on directed acyclic graphs (DAGs) or MapReduce programs in a lower level computer language, such as Java. Hive extends the SQL paradigm by including serialization formats. </p><p>Option B is wrong as <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hbase.html" target="_blank">HBase</a> is an open source, non-relational, distributed database developed as part of the Apache Software Foundation's Hadoop project. HBase runs on top of Hadoop Distributed File System (HDFS) to provide non- relational database capabilities for the Hadoop ecosystem. HBase works seamlessly with Hadoop, sharing its file system and serving as a direct input and output to the MapReduce framework and execution engine. HBase also integrates with Apache Hive, enabling SQL-like queries over HBase tables, joins with Hive-based tables, and support for Java Database Connectivity (JDBC). </p><p>Option C is wrong as <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hcatalog.html" target="_blank">HCatalog</a> is a tool that allows you to access Hive metastore tables within Pig, Spark SQL, and/or custom MapReduce applications. HCatalog has a REST interface and command line client that allows you to create tables or do other operations. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23125348">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 45 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251338"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> Your company produces customer commissioned one-of-a-kind skiing helmets combining high fashion with custom technical enhancements. Customers can show off their Individuality on the ski slopes and have access to head-up-displays. GPS rear-view cams and any other technical innovation they wish to embed in the helmet. The current manufacturing process is data rich and complex including assessments to ensure that the custom electronics and materials used to assemble the helmets are to the highest standards. Assessments are a mixture of human and automated assessments you need to add a new set of assessment to model the failure modes of the custom electronics using GPUs with CUDA across a cluster of servers with low latency networking. What architecture would allow you to automate the existing process using a hybrid approach and ensure that the architecture can support the evolution of processes over time?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Use AWS Data Pipeline to manage movement of data &amp; meta-data and assessments Use an auto-scaling group of G2 instances in a placement group.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use Amazon Simple Workflow (SWF) to manage assessments, movement of data &amp; meta-data. Use an autoscaling group of G2 instances in a placement group.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Use Amazon Simple Workflow (SWF) to manage assessments, movement of data &amp; meta-data. Use an autoscaling group of C3 instances with SR-IOV (Single Root I/O Virtualization).<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use AWS data Pipeline to manage movement of data &amp; meta-data and assessments use auto-scaling group of C3 with SR-IOV (Single Root I/O virtualization) <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Use Amazon Simple Workflow (SWF) to manage assessments, movement of data &amp; meta-data. Use an autoscaling group of G2 instances in a placement group.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Key point here hybrid work flow with both automated and manual tasks and ability to replay also needing GPUs with CUD instances with low latency networking </p><p>Correct answer is <strong>B</strong> as <a href="http://docs.aws.amazon.com/amazonswf/latest/awsrbflowguide/concepts-parts.html" target="_blank">SWF</a> provides an ability to have both human and automated assessments with <a href="https://aws.amazon.com/ec2/instance-types/" target="_blank">G2</a> instances in a <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html" target="_blank">placement group</a> providing GPU and low latency networking. </p><p>Option A &amp; D are wrong as it involves hybrid approach involving human assessments. </p><p>Option C &amp; D are wrong as C3 and SR-IOV won’t provide GPU as well as Enhanced networking needs to be enabled </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121212">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 46 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251339"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company operates an international business served from a single AWS region. The company wants to expand into a new country. The regulator for that country requires the Data Architect to maintain a log of financial transactions in the country within 24 hours of the product transaction. The production application is latency insensitive. The new country contains another AWS region. What is the most cost-effective way to meet this requirement? <br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Use CloudFormation to replicate the production application to the new region.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use Amazon CloudFront to serve application content locally in the country; Amazon CloudFront logs will satisfy the requirement.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Continue to serve customers from the existing region while using Amazon Kinesis to stream transaction data to the regulator.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use Amazon S3 cross-region replication to copy and persist production transaction logs to a bucket in the new country’s region. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Use Amazon S3 cross-region replication to copy and persist production transaction logs to a bucket in the new country’s region.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as only the logs need to be maintained in the new country, S3 cross region replication can be used to copy the data to the AWS region within the new Country. </p><p>Option A is wrong as there is not need for replication the complete application </p><p>Option B is wrong as CloudFront logs would only provide access logs and maybe not hold the financial transaction logs. </p><p>Option C is wrong as using Kinesis would need to build and host client application and have data storage charges as well. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121417">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 47 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251340"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You have recently joined a startup company building sensors to measure street noise and air quality in urban areas. The company has been running a pilot deployment of around 100 sensors for 3 months. Each sensor uploads 1KB of sensor data every minute to a backend hosted on AWS. During the pilot, you measured a peak or 10 IOPS on the database, and you stored an average of 3GB of sensor data per month in the database. The current deployment consists of a load-balanced auto scaled Ingestion layer using EC2 instances and a PostgreSQL RDS database with 500GB standard storage. The pilot is considered a success and your CEO has managed to get the attention or some potential investors. The business plan requires a deployment of at least 100K sensors, which needs to be supported by the backend. You also need to store sensor data for at least two years to be able to compare year over year Improvements. To secure funding, you have to make sure that the platform meets these requirements and leaves room for further scaling. Which setup will meet the requirements?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Add an SQS queue to the ingestion layer to buffer writes to the RDS instance</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Ingest data into a DynamoDB table and move old data to a Redshift cluster <br><b>C</b>. <input type="radio" disabled="">&nbsp;Replace the RDS instance with a 6 node Redshift cluster with 96TB of storage<br><b>D</b>. <input type="radio" disabled="">&nbsp;Keep the current architecture but upgrade RDS storage to 3TB and 10K provisioned IOPS <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Ingest data into a DynamoDB table and move old data to a Redshift cluster <br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Key point here is backend supporting the data with 2 years retention and architecture being scalable </p><p>Correct answer is <strong>B</strong> as DynamoDB can be used to support the ingestion throughput via autoscaled instances and later store data into Redshift for analysis </p><p>Option A &amp; D are wrong as RDS would not be scalable and performant with high input rate and storage for 2 years </p><p>Option C is wrong as Redshift is designed for data warehousing and would not be able to support the ingestion throughput </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121237">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 48 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251341"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company receives data sets coming from external providers on Amazon S3. Data sets from different providers are dependent on one another. Data sets will arrive at different times and in no particular order. A data architect needs to design a solution that enables the company to do the following: Rapidly perform cross data set analysis as soon as the data become available Manage dependencies between data sets that arrive at different times Which architecture strategy offers a scalable and cost-effective solution that meets these Requirements? <br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Maintain data dependency information in Amazon RDS for MySQL. Use an AWS Data Pipeline job to load an Amazon EMR Hive table based on task dependencies and event notification triggers in Amazon S3.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Maintain data dependency information in an Amazon DynamoDB table. Use Amazon SNS and event notifications to publish data to fleet of Amazon EC2 workers. Once the task dependencies have been resolved, process the data with Amazon EMR.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Maintain data dependency information in an Amazon ElastiCache Redis cluster. Use Amazon S3 event notifications to trigger an AWS Lambda function that maps the S3 object to Redis. Once the task dependencies have been resolved, process the data with Amazon EMR.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Maintain data dependency information in an Amazon DynamoDB table. Use Amazon S3 event notifications to trigger an AWS Lambda function that maps the S3 object to the task associated with it in DynamoDB. Once all task dependencies have been resolved, process the data with Amazon EMR. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Maintain data dependency information in an Amazon DynamoDB table. Use Amazon S3 event notifications to trigger an AWS Lambda function that maps the S3 object to the task associated with it in DynamoDB. Once all task dependencies have been resolved, process the data with Amazon EMR.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as the data dependency can be managed in DynamoDB. S3 event notifications can trigger Lambda functions to map the objects and check dependency. Once all satisfied, EMR job can be triggered. </p><p>Option A is wrong as EMR hive with RDS with need resource running always and not a cost-effective solution. </p><p>Option B is wrong as using EC2 fleet of servers would not be cost-effective as compared to lambda. </p><p>Option C is wrong as ElastiCache Redis is not an ideal storage for mapping and would not be cost-effective as compared to DynamoDB being a completely managed service. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121362">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 49 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251342"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A media advertising company handles a large number of real-time messages sourced from over 200 websites in real time. Processing latency must be kept low. Based on calculations, a 60-shard Amazon Kinesis stream is more than sufficient to handle the maximum data throughput, even with traffic spikes. The company also uses an Amazon Kinesis Client Library (KCL) application running on Amazon Elastic Compute Cloud (EC2) managed by an Auto Scaling group. Amazon CloudWatch indicates an average of 25% CPU and a modest level of network traffic across all running servers. The company reports a 150% to 200% increase in latency of processing messages from Amazon Kinesis during peak times. There are NO reports of delay from the sites publishing to Amazon Kinesis. What is the appropriate solution to address the latency?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Increase the number of shards in the Amazon Kinesis stream to 80 for greater concurrency.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Increase the size of the Amazon EC2 instances to increase network throughput.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Increase the minimum number of instances in the Auto Scaling group.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Increase Amazon DynamoDB throughput on the checkpoint table. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Increase the minimum number of instances in the Auto Scaling group.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as the shards are more than enough and the EC2 instances utilization is low, the only other reason can be the instances do not match up the shards and single instance is processing multiple shards. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html" target="_blank">Kinesis Record Processor Scaling</a> </p><p><em>Typically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it's fine if the number of shards exceeds the number of instances.</em> </p><p><em>To scale up processing in your application, you should test a combination of these approaches:</em> </p><ul> <li><em>Increasing the instance size (because all record processors run in parallel within a process)</em></li> <li><em>Increasing the number of instances up to the maximum number of open shards (because shards can be processed independently)</em></li> <li><em>Increasing the number of shards (which increases the level of parallelism)</em></li> </ul><p><em>Note that you can use Auto Scaling to automatically scale your instances based on appropriate metrics. </em> </p><p>Option A is wrong as shards are more than enough increasing would not improve performance. </p><p>Option B is wrong as the network traffic is modest increasing the size would not improve performance.<br> </p><p>Option D is wrong as there is no relation of DynamoDB.<br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121416">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 50 of 50 </b><small>- Multiple Answer</small> <span style="float:right;" class="grad_5251343"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An administrator needs to design a strategy for the schema in a Redshift cluster. The administrator needs to determine the optimal distribution style for the tables in the Redshift schema. In which two circumstances would choosing EVEN distribution be most appropriate? (Choose two.)<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="checkbox" checked="true" disabled="">&nbsp;When the tables are highly denormalized and do NOT participate in frequent joins.</span><br><b>B</b>. <input type="checkbox" disabled="">&nbsp;When data must be grouped based on a specific key on a defined slice.<br><b>C</b>. <input type="checkbox" disabled="">&nbsp;When data transfer between nodes must be eliminated.<br><b>D</b>. <input type="checkbox" disabled="">&nbsp;When a new table has been loaded and it is unclear how it will be joined to dimension. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. When the tables are highly denormalized and do NOT participate in frequent joins.<br><b>D</b>. When a new table has been loaded and it is unclear how it will be joined to dimension.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answers are <strong>A &amp; D</strong> as EVEN distribution distributes the data across slides in a round robin fashion and does not participate in joins. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html" target="_blank">Redshift Distribution Style</a> </p><p><em><strong>EVEN distribution</strong> - The leader node distributes the rows across the slices in a round-robin fashion, regardless of the values in any particular column. EVEN distribution is appropriate when a table does not participate in joins or when there is not a clear choice between KEY distribution and ALL distribution.</em><em></em><br> </p><p>Option B is wrong as if the data needs to be grouped by specific Key, KEY distribution should be used. </p><p><em>The rows are distributed according to the values in one column. The leader node places matching values on the same node slice. If you distribute a pair of tables on the joining keys, the leader node collocates the rows on the slices according to the values in the joining columns so that matching values from the common columns are physically stored together.</em><em></em><br> </p><p>Option C is wrong as if the data transfer needs to be eliminated, ALL distribution should be used as a copy of all the data is made on all the nodes. </p><p><em>A copy of the entire table is distributed to every node. Where EVEN distribution or KEY distribution place only a portion of a table's rows on each node, ALL distribution ensures that every row is collocated for every join that the table participates in.</em><em></em><br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121413">AWS BDS-C00 Question feedback</a></div></div></div><div class="col-sm-12 well"><h4 class="sm" style="color:#333;">12/50 Questions right</h4></div></div><style type="text/css">
span.label.label-danger,span.label.label-success {
    padding: .2em .6em .3em;
}
.saved{color:red; }
.question_info{
  margin-bottom: 40px;
  border: solid 1px #ccc;
  border-radius: 5px;
  overflow: hidden;
}
.question_no {
    background: #f4f4f4;
    padding: 0 15px;
    line-height: 40px;
    border-bottom: solid 1px #ccc;
}

.question_detail {
    padding: 10px;
}
.hide{
  display: none;
}
input[type="radio"]{
  -webkit-appearance: radio;
}
input[type="checkbox"]{
  -webkit-appearance: checkbox;
}
span.bgcolor {
    background: yellow;
    padding: 5px;
    margin-left: -5px;
}
</style><link href="./bds1_files/mcoursestyle.css" rel="stylesheet"></div> <script type="text/javascript" src="./bds1_files/bc-course.min_031117.js.下载"></script> <div class="overlayForm" style=""></div></div></div></div></div></div><div class="overlayForm"></div></div><iframe style="position:absolute;left:-999px;top:-999px;visibility:hidden" src="./bds1_files/saved_resource.html"></iframe><iframe style="display: none; visibility: hidden;" src="./bds1_files/saved_resource(1).html"></iframe><div style="position:absolute;top:-300px;left:-40px;width:1px;height:1px;"><iframe id="__l1__l" style="top:-300px;left:-400px;border:0px solid transparent !important;width:1px !important;height:1px !important;display:none;" seamless="" height="1px" border="0px" width="1px" src="./bds1_files/sto.html"></iframe></div><a href="https://www.adexchangecloud.com/jump/next.php?r=308135&amp;cb=1561950809795" target="_blank" id="___o02" style="cursor:default" onclick="window.postMessage(&quot;bbadc22b-e7d9-462f-84dd-e144ca850e86-302-pi-0&quot;, &quot;*&quot;)"><div style="z-index:10100;background:transparent;opacity:0;position:fixed;top:0;left:0;bottom:0;right:0;width:100%;height:100%"></div></a></body></html>