<!DOCTYPE html>
<!-- saved from url=(0105)https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-2/145260 -->
<html xmlns:fb="http://ogp.me/ns/fb#" lang="en-gb" dir="ltr" class="secondary-14px wf-proximanova-n7-active wf-proximanova-i7-active wf-proximanova-n4-active wf-raleway-n1-active wf-raleway-n7-active wf-raleway-n4-active wf-raleway-n5-active wf-raleway-n3-active wf-raleway-n8-active wf-raleway-n9-active wf-raleway-n2-active wf-raleway-n6-active wf-proximanova-i4-active wf-active"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><meta name="format-detection" content="telephone=no"><script type="text/javascript" src="./bds2_files/display.js.下载"></script><script type="text/javascript" src="./bds2_files/pro"></script><script type="text/javascript" src="./bds2_files/l.js.下载"></script><script type="text/javascript" src="./bds2_files/l.js(1).下载"></script><script type="text/javascript" src="./bds2_files/l.js(2).下载"></script><script type="text/javascript" src="./bds2_files/l.js(3).下载"></script><script type="text/javascript" src="./bds2_files/pageload"></script><script type="text/javascript" async="" src="./bds2_files/analytics.js.下载"></script><script type="text/javascript" async="" src="./bds2_files/atatus.js.下载"></script><script src="./bds2_files/fMy5LNtdDqis6adCpEbCXQHA47I.js.下载"></script><script src="./bds2_files/js"></script><link rel="canonical" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-2/145260"><link rel="stylesheet" type="text/css" href="./bds2_files/bc-course.min_092917.css"><link rel="stylesheet" type="text/css" href="./bds2_files/bc-style-092917.css"><!--[if lt IE 9]> <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script> <script src="//css3-mediaqueries-js.googlecode.com/svn/trunk/css3-mediaqueries.js"></script> <![endif]--><link rel="shortcut icon" href="https://www.braincert.com/images/favicon.ico"> <script type="text/javascript" src="./bds2_files/jquery-1.11.0.min.js.下载"></script> <script type="text/javascript">jQuery.noConflict();</script> <script type="application/javascript" src="./bds2_files/fVBYAHUg.js.下载"></script> <script type="text/javascript">jwplayer.key="Kfk7MAHVl4Y33jPduQlHwUdmLu+1l6cvPHVklw==";</script> <script src="./bds2_files/jdk4nqa.js.下载"></script> <style type="text/css">.tk-proxima-nova{font-family:"proxima-nova",sans-serif;}.tk-raleway{font-family:"raleway",sans-serif;}</style><style type="text/css">@font-face{font-family:tk-proxima-nova-n7;src:url(https://use.typekit.net/af/925423/00000000000000003b9b038f/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff2"),url(https://use.typekit.net/af/925423/00000000000000003b9b038f/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff"),url(https://use.typekit.net/af/925423/00000000000000003b9b038f/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("opentype");font-weight:700;font-style:normal;}@font-face{font-family:tk-proxima-nova-i7;src:url(https://use.typekit.net/af/cd78b3/00000000000000003b9b038e/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i7&v=3) format("woff2"),url(https://use.typekit.net/af/cd78b3/00000000000000003b9b038e/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i7&v=3) format("woff"),url(https://use.typekit.net/af/cd78b3/00000000000000003b9b038e/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i7&v=3) format("opentype");font-weight:700;font-style:italic;}@font-face{font-family:tk-proxima-nova-n4;src:url(https://use.typekit.net/af/219c30/00000000000000003b9b0389/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff2"),url(https://use.typekit.net/af/219c30/00000000000000003b9b0389/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff"),url(https://use.typekit.net/af/219c30/00000000000000003b9b0389/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("opentype");font-weight:400;font-style:normal;}@font-face{font-family:tk-proxima-nova-i4;src:url(https://use.typekit.net/af/0de7d4/00000000000000003b9b0388/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i4&v=3) format("woff2"),url(https://use.typekit.net/af/0de7d4/00000000000000003b9b0388/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i4&v=3) format("woff"),url(https://use.typekit.net/af/0de7d4/00000000000000003b9b0388/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i4&v=3) format("opentype");font-weight:400;font-style:italic;}@font-face{font-family:tk-raleway-n1;src:url(https://use.typekit.net/af/4c4265/00000000000000000001328e/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n1&v=3) format("woff2"),url(https://use.typekit.net/af/4c4265/00000000000000000001328e/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n1&v=3) format("woff"),url(https://use.typekit.net/af/4c4265/00000000000000000001328e/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n1&v=3) format("opentype");font-weight:100;font-style:normal;}@font-face{font-family:tk-raleway-n7;src:url(https://use.typekit.net/af/00d57c/000000000000000000013287/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff2"),url(https://use.typekit.net/af/00d57c/000000000000000000013287/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff"),url(https://use.typekit.net/af/00d57c/000000000000000000013287/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("opentype");font-weight:700;font-style:normal;}@font-face{font-family:tk-raleway-n4;src:url(https://use.typekit.net/af/3c6666/000000000000000000013288/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff2"),url(https://use.typekit.net/af/3c6666/000000000000000000013288/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff"),url(https://use.typekit.net/af/3c6666/000000000000000000013288/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("opentype");font-weight:400;font-style:normal;}@font-face{font-family:tk-raleway-n5;src:url(https://use.typekit.net/af/145edc/000000000000000000013289/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n5&v=3) format("woff2"),url(https://use.typekit.net/af/145edc/000000000000000000013289/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n5&v=3) format("woff"),url(https://use.typekit.net/af/145edc/000000000000000000013289/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n5&v=3) format("opentype");font-weight:500;font-style:normal;}@font-face{font-family:tk-raleway-n3;src:url(https://use.typekit.net/af/9a0c16/00000000000000000001328a/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n3&v=3) format("woff2"),url(https://use.typekit.net/af/9a0c16/00000000000000000001328a/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n3&v=3) format("woff"),url(https://use.typekit.net/af/9a0c16/00000000000000000001328a/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n3&v=3) format("opentype");font-weight:300;font-style:normal;}@font-face{font-family:tk-raleway-n8;src:url(https://use.typekit.net/af/62d84a/00000000000000000001328b/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n8&v=3) format("woff2"),url(https://use.typekit.net/af/62d84a/00000000000000000001328b/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n8&v=3) format("woff"),url(https://use.typekit.net/af/62d84a/00000000000000000001328b/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n8&v=3) format("opentype");font-weight:800;font-style:normal;}@font-face{font-family:tk-raleway-n9;src:url(https://use.typekit.net/af/f4139f/00000000000000000001328c/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n9&v=3) format("woff2"),url(https://use.typekit.net/af/f4139f/00000000000000000001328c/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n9&v=3) format("woff"),url(https://use.typekit.net/af/f4139f/00000000000000000001328c/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n9&v=3) format("opentype");font-weight:900;font-style:normal;}@font-face{font-family:tk-raleway-n2;src:url(https://use.typekit.net/af/6b6454/00000000000000000001328d/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n2&v=3) format("woff2"),url(https://use.typekit.net/af/6b6454/00000000000000000001328d/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n2&v=3) format("woff"),url(https://use.typekit.net/af/6b6454/00000000000000000001328d/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n2&v=3) format("opentype");font-weight:200;font-style:normal;}@font-face{font-family:tk-raleway-n6;src:url(https://use.typekit.net/af/baea6e/000000000000000000014968/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n6&v=3) format("woff2"),url(https://use.typekit.net/af/baea6e/000000000000000000014968/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n6&v=3) format("woff"),url(https://use.typekit.net/af/baea6e/000000000000000000014968/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n6&v=3) format("opentype");font-weight:600;font-style:normal;}</style><script>try{Typekit.load({ async: true });}catch(e){}</script> <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="keywords" content="virtual classroom, online test, online course, MOOC,  SCORM, whiteboard, adaptive testing, e-learning, online education, learn online, teach online, live class, lms, monetize, sell course, online meetings, collaboration, webinar, how to, social, teach, learn"><meta name="description" content="Deliver live engaging classes using Virtual Classroom. Create and sell courses and tests online."><title>Review Answers | BrainCert</title><link href="https://www.braincert.com/templates/yoo_nano/favicon.ico" rel="shortcut icon" type="image/vnd.microsoft.icon"> <script type="text/javascript">
function keepAlive() {	var myAjax = new Request({method: "get", url: "index.php"}).send();} window.addEvent("domready", function(){ keepAlive.periodical(3540000); });
  </script> <script type="text/javascript">
				/*<![CDATA[*/
					var jax_live_site = 'https://www.braincert.com/index.php';
					var jax_token_var='925395911814f17127e11ea28577607f';
				/*]]>*/
				</script><script type="text/javascript" src="./bds2_files/ajax_1.5.pack.js.下载"></script> <link rel="apple-touch-icon-precomposed" href="https://d9q55ve2f7k8m.cloudfront.net/images/apple_touch_icon.png"> <script>
        !function(window, document) {
            window._atatusConfig = {
                apikey: '8c2f3d535648489b9826fd95a6484c2b'
            };
            function _asyncAtatus(callback) {
                var script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.src = "https://dmc1acwvwny3.cloudfront.net/atatus.js";
                var node = document.getElementsByTagName("script")[0];
                script.addEventListener('load', function (e) {
                    callback(null, e);
                }, false);
                node.parentNode.insertBefore(script, node);
            }
            _asyncAtatus(function() {
                // Any atatus related calls.
                if (window.atatus) {
                    window.atatus.setUser('138600', 'liugongjianxin@163.com', 'gongjian liu');
                    console.log(window.atatus);
                }
            });
        }(window, document);
</script> <style type="text/css">@font-face{font-family:proxima-nova;src:url(https://use.typekit.net/af/925423/00000000000000003b9b038f/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff2"),url(https://use.typekit.net/af/925423/00000000000000003b9b038f/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff"),url(https://use.typekit.net/af/925423/00000000000000003b9b038f/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("opentype");font-weight:700;font-style:normal;}@font-face{font-family:proxima-nova;src:url(https://use.typekit.net/af/cd78b3/00000000000000003b9b038e/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i7&v=3) format("woff2"),url(https://use.typekit.net/af/cd78b3/00000000000000003b9b038e/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i7&v=3) format("woff"),url(https://use.typekit.net/af/cd78b3/00000000000000003b9b038e/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i7&v=3) format("opentype");font-weight:700;font-style:italic;}@font-face{font-family:proxima-nova;src:url(https://use.typekit.net/af/219c30/00000000000000003b9b0389/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff2"),url(https://use.typekit.net/af/219c30/00000000000000003b9b0389/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff"),url(https://use.typekit.net/af/219c30/00000000000000003b9b0389/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("opentype");font-weight:400;font-style:normal;}@font-face{font-family:proxima-nova;src:url(https://use.typekit.net/af/0de7d4/00000000000000003b9b0388/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i4&v=3) format("woff2"),url(https://use.typekit.net/af/0de7d4/00000000000000003b9b0388/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i4&v=3) format("woff"),url(https://use.typekit.net/af/0de7d4/00000000000000003b9b0388/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i4&v=3) format("opentype");font-weight:400;font-style:italic;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/4c4265/00000000000000000001328e/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n1&v=3) format("woff2"),url(https://use.typekit.net/af/4c4265/00000000000000000001328e/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n1&v=3) format("woff"),url(https://use.typekit.net/af/4c4265/00000000000000000001328e/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n1&v=3) format("opentype");font-weight:100;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/00d57c/000000000000000000013287/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff2"),url(https://use.typekit.net/af/00d57c/000000000000000000013287/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff"),url(https://use.typekit.net/af/00d57c/000000000000000000013287/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("opentype");font-weight:700;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/3c6666/000000000000000000013288/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff2"),url(https://use.typekit.net/af/3c6666/000000000000000000013288/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff"),url(https://use.typekit.net/af/3c6666/000000000000000000013288/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("opentype");font-weight:400;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/145edc/000000000000000000013289/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n5&v=3) format("woff2"),url(https://use.typekit.net/af/145edc/000000000000000000013289/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n5&v=3) format("woff"),url(https://use.typekit.net/af/145edc/000000000000000000013289/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n5&v=3) format("opentype");font-weight:500;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/9a0c16/00000000000000000001328a/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n3&v=3) format("woff2"),url(https://use.typekit.net/af/9a0c16/00000000000000000001328a/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n3&v=3) format("woff"),url(https://use.typekit.net/af/9a0c16/00000000000000000001328a/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n3&v=3) format("opentype");font-weight:300;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/62d84a/00000000000000000001328b/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n8&v=3) format("woff2"),url(https://use.typekit.net/af/62d84a/00000000000000000001328b/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n8&v=3) format("woff"),url(https://use.typekit.net/af/62d84a/00000000000000000001328b/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n8&v=3) format("opentype");font-weight:800;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/f4139f/00000000000000000001328c/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n9&v=3) format("woff2"),url(https://use.typekit.net/af/f4139f/00000000000000000001328c/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n9&v=3) format("woff"),url(https://use.typekit.net/af/f4139f/00000000000000000001328c/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n9&v=3) format("opentype");font-weight:900;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/6b6454/00000000000000000001328d/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n2&v=3) format("woff2"),url(https://use.typekit.net/af/6b6454/00000000000000000001328d/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n2&v=3) format("woff"),url(https://use.typekit.net/af/6b6454/00000000000000000001328d/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n2&v=3) format("opentype");font-weight:200;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/baea6e/000000000000000000014968/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n6&v=3) format("woff2"),url(https://use.typekit.net/af/baea6e/000000000000000000014968/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n6&v=3) format("woff"),url(https://use.typekit.net/af/baea6e/000000000000000000014968/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n6&v=3) format("opentype");font-weight:600;font-style:normal;}</style><script type="text/javascript" src="./bds2_files/ga.js.下载"></script><script async="" type="text/javascript" src="./bds2_files/pops"></script><script async="" type="text/javascript" src="./bds2_files/pops(1)"></script><script type="text/javascript" src="./bds2_files/jquery.min.js.下载"></script></head><body id="page-top"><div id="page-wrap"><div id="preloader"> </div> <header id="header" class="header chapter-header"><div class="container"><div class="logo"><a href="https://www.braincert.com/"><img src="./bds2_files/bc-logo-sm.png" alt="BrainCert" style="max-height:60px;"></a> </div><nav class="navigation"><div class="navbar-header"> <a class="navbar-brand" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-2/145260#"><span class="navbar-header-title"> AWS Certified Big Data - Specialty BDS-C00 Practice Exam 2 </span></a> </div><ul class="menu"> <li><a href="https://www.braincert.com/">Home</a></li> </ul><div class="search-box"> <a href="https://www.braincert.com/test/13669-AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-2" class="smoothScroll"><i class="fa fa-chevron-left"></i> Back</a> </div></nav> </div> </header><div class="main-container"> <script type="text/javascript">
  jQuery(document).ready(function (){
     
    jQuery( "html" ).addClass( "secondary-14px" );
    jQuery(document)[0].oncontextmenu = function() {return false;} 
    // code for preventing copy from keyboard
    var ambit = jQuery(document);
    // Disable Cut + Copy + Paste (input)
    ambit.on('copy paste cut', function (e) {
    e.preventDefault(); //disable cut,copy,paste
      return false;
    });
      });
</script> <div class="container"><div id="content-main" class="row-fluid"><div class="col-sm-12"><h2 style="margin-bottom:10px;margin-top:10px; float:left">Test Report</h2><div style="margin-top:10px;float:right"><a onclick="window.history.back();" class="btn btn-warning"><span><strong>Back</strong></span></a></div><div style="float:right;margin-top:10px;margin-right: 10px;"><a href="https://www.braincert.com/test/reviewtest/exportdata/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-2/145260" class="btn btn-primary"><span><strong><i class="fa fa-share-square-o"></i>&nbsp;Export to .CSV</strong></span></a></div><div style="border-bottom-width: 1px;border-bottom-style: dashed;border-bottom-color: #e0e0e0;margin-bottom: 10px; clear:both"></div><div style="clear:both;"></div><div class="col-md-6 pull-left row"><h3 style="margin-top: 0px;"><strong>Review questions</strong></h3></div><div class="col-md-6 pull-right row" style="font-size: 16px;text-align: right;"><strong>Student : </strong>gongjian liu
<br> <i class="fa fa-calendar"></i>&nbsp;Jun 17, 2019&nbsp;&nbsp;<i class="fa fa-clock-o"></i>&nbsp;05:14AM EDT<br> <br> </div><div id="test_results" style="padding-top: 80px;"><div id="quiz_specific"> <span id="select"></span> <div class="quiz_attempt_breakdown"><div class="percent_correct_bar col-sm-2"><div class="progress" style="margin-bottom: 2px;"><div class="progress-bar" role="progressbar" aria-valuenow="70" aria-valuemin="0" aria-valuemax="100" style="width:46%"> </div> </div> <span id="percent"><strong>46% </strong>correct</span> </div><div><div class="questions_correct col-sm-2"><span class="inline_pipe">|</span>&nbsp;&nbsp; <img src="./bds2_files/tick.webp" alt="you got this question right">&nbsp;<strong>23 correct</strong></div><div class="questions_incorrect col-sm-2"><span class="inline_pipe"> | </span>&nbsp;&nbsp; <img src="./bds2_files/cross.webp" alt="you got this question wrong">&nbsp;<strong>27 incorrect</strong></div><div class="questions_incorrect col-sm-3" style="margin:0;"><span class="inline_pipe"> | </span>&nbsp;&nbsp;<img src="./bds2_files/icon_un_answered.webp" alt="you got this question unanswer">&nbsp;<strong>0 Unanswered</strong></div><div class="total_questions col-sm-3"><span class="inline_pipe"> | </span>&nbsp;&nbsp;<strong>50 questions attempted out of 50</strong></div></div></div><br class="clear"><hr style="clear:both"><br> <br><div style="margin-bottom: 10px;"> <b style="font-size: 14.5px;">Filter by</b> : <a style="padding: 3px 5px 3px 5px;" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-2/145260" class="label-default label">All</a>&nbsp;|&nbsp; <a style="padding: 3px 5px 3px 5px;" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-2/145260?sort=1">correct</a>&nbsp;|&nbsp; <a style="padding: 3px 5px 3px 5px;" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-2/145260?sort=0">incorrect</a>&nbsp;|&nbsp; <a style="padding: 3px 5px 3px 5px;" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-2/145260?sort=-1">Unanswered</a>&nbsp;|&nbsp; <a style="padding: 3px 5px 3px 5px;" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-2/145260?sort=2">Question feedback</a> </div><br><div class="" style="font-size: 18px;line-height: 25px;"><div class="question_info"><div class="question_no"><b>Question : 1 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251351"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A solutions architect for a logistics organization ships packages from thousands of suppliers to end customers. The architect is building a platform where suppliers can view the status of one or more of their shipments. Each supplier can have multiple roles that will only allow access to specific fields in the resulting information. Which strategy allows the appropriate level of access control and requires the LEAST amount of management work?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 35 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Send the tracking data to Amazon Kinesis Streams. Use AWS Lambda to store the data in an Amazon DynamoDB Table. Generate temporary AWS credentials for the suppliers’ users with AWS STS, specifying fine-grained security policies to limit access only to their applicable data.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Send the tracking data to Amazon Kinesis Firehose. Use Amazon S3 notifications and AWS Lambda to prepare files in Amazon S3 with appropriate data for each supplier’s roles. Generate temporary AWS credentials for the suppliers’ users with AWS STS. Limit access to the appropriate files through security policies.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Send the tracking data to Amazon Kinesis Streams. Use Amazon EMR with Spark Streaming to store the data in HBase. Create one table per supplier. Use HBase Kerberos integration with the suppliers’ users. Use HBase ACL-based security to limit access for the roles to their specific table and columns.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Send the tracking data to Amazon Kinesis Firehose. Store the data in an Amazon Redshift cluster. Create views for the suppliers’ users and roles. Allow suppliers access to the Amazon Redshift cluster using a user limited to the applicable view. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Send the tracking data to Amazon Kinesis Streams. Use AWS Lambda to store the data in an Amazon DynamoDB Table. Generate temporary AWS credentials for the suppliers’ users with AWS STS, specifying fine-grained security policies to limit access only to their applicable data.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as DynamoDB can be used to store the data. Access to fields can be controlled using DynamoDB fine grained access control, which can be mapped to IAM role. This solution also requires the least amount of management effort. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html" target="_blank">DynamoDB Control Access</a> </p><p><em>In DynamoDB, you have the option to specify conditions when granting permissions using an IAM policy (see <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/authentication-and-access-control.html#access-control">Access Control</a>). For example, you can:</em> </p><ul> <li><em>Grant permissions to allow users read-only access to certain items and attributes in a table or a secondary index.</em></li> <li><em>Grant permissions to allow users write-only access to certain attributes in a table, based upon the identity of that user.</em></li> </ul><p>Option B is wrong as S3 would not provide fine grained access control of data within the file. </p><p>Option C is wrong as although its possible, the option does not satisfy the least amount of management work requirement.<br> </p><p>Option D is wrong as Redshift is more for a data warehouse solution and comes with management effort.<br> </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121379">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 2 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251352"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A utility company is building an application that stores data coming from more than 10,000 sensors. Each sensor has a unique ID and will send a datapoint (approximately 1KB) every 10 minutes throughout the day. Each datapoint contains the information coming from the sensor as well as a timestamp. This company would like to query information coming from a particular sensor for the past week very rapidly and want to delete all the data that is older than 4 weeks. Using Amazon DynamoDB for its scalability and rapidity, how do you implement this in the most cost effective way?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;One table, with a primary key that is the sensor ID and a sort key that is the timestamp</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;One table, with a primary key that is the concatenation of the sensor ID and timestamp<br><b>C</b>. <input type="radio" disabled="">&nbsp;One table for each week, with a primary key that is the concatenation of the sensor ID and timestamp<br><b>D</b>. <input type="radio" disabled="">&nbsp;One table for each week, with a primary key that is the sensor ID and a sort key that is the timestamp <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. One table for each week, with a primary key that is the sensor ID and a sort key that is the timestamp<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> Composite key with Sensor ID and timestamp would help for faster queries </p><p>Refer AWS documentation for <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GuidelinesForTables.html#GuidelinesForTables.TimeSeriesDataAccessPatterns" rel="nofollow">DynamoDB handling Timeseries data</a><br> </p><p>Option C &amp; D are valid as they are keeping tables for each week. However, with Option C, concatenation will cause queries would be slower<br>Table should be designed with a composite primary key consisting of Customer ID as the partition key and date/time as the sort key<br><br>Option A &amp; B are wrong as One table would not make sense as we need to query only on past week and want data only for 4 weeks. This would impact performance. Also, provisioned throughput consumption is based on the size of the deleted item and its more costly as compared to dropping a table. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121751">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 3 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251353"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You need to provide customers with rich visualizations that allow you to easily connect multiple disparate data sources in S3, Redshift, and several CSV files. Which tool should you use that requires the least setup?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Hue on EMR</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Redshift<br><b>C</b>. <input type="radio" disabled="">&nbsp;QuickSight<br><b>D</b>. <input type="radio" disabled="">&nbsp;Elasticsearch <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. QuickSight<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as QuickSight provides visualization capability with integration with RDS, Redshift. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/quicksight/" target="_blank">QuickSight</a> </p><p><em>Amazon QuickSight is a fast, cloud-powered business intelligence service that makes it easy to deliver insights to everyone in your organization.</em> </p><p><em>As a fully managed service, QuickSight lets you easily create and publish interactive dashboards that include ML Insights. Dashboards can then be accessed from any device, and embedded into your applications, portals, and websites.</em> </p><p><em>QuickSight allows you to directly connect to and import data from a wide variety of cloud and on-premises data sources. These include SaaS applications such as Salesforce, Square, ServiceNow, Twitter, Github, and JIRA; 3rd party databases such as Teradata, MySQL, Postgres, and SQL Server; native AWS services such as Redshift, Athena, S3, RDS, and Aurora; and private VPC subnets. You can also upload a variety of file types including Excel, CSV, JSON, and Presto.<br></em> </p><p>Option A is wrong as Hue is a Web interface for analyzing data with Hadoop. </p><p>Option B is wrong as Redshift is a fast, scalable data warehouse that makes it simple and cost-effective to analyze all your data across your data warehouse and data lake.<br> </p><p>Option D is wrong as Elasticsearch is a fully managed service that makes it easy for you to deploy, secure, and operate Elasticsearch at scale with zero down time. It does not provide visualization support and needs to be used with Kibana and nor does it integrate with RDS, Redshift. The data needs to be loaded into Elasticsearch.<br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23123005">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 4 of 50 </b><small>- Multiple Answer</small> <span style="float:right;" class="grad_5251354"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You need to create a recommendation engine for your e-commerce website that sells over 300 items. The items never change, and the new users need to be presented with the list of all 300 items in order of their interest. Which option do you use to accomplish this?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="checkbox" checked="true" disabled="">&nbsp;Mahout</span><br><b>B</b>. <input type="checkbox" disabled="">&nbsp;Spark/Spark MLlib<br><b>C</b>. <input type="checkbox" disabled="">&nbsp;Amazon Machine Learning<br><b>D</b>. <input type="checkbox" disabled="">&nbsp;RDS MySQL <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Mahout<br><b>B</b>. Spark/Spark MLlib<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answers are <strong>A &amp; B</strong> </p><p>Option A as Mahout provides recommender engine/collaborative filtering capability </p><p>Option B as Spark's MLlib machine learning library should help with this task. Amazon ML is limited to 100 "categorical' recommendations, so a custom system is required for this purpose. </p><p>Option C is wrong as Amazon ML is limited to 100 "categorical' recommendations, so a custom system is required for this purpose. </p><p>Option D is wrong as RDS MySQL is just a database engine and does not provide analytics capability. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%202%20-%20%23125174">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 5 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251355"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A web application emits multiple types of events to Amazon Kinesis Streams for operational reporting. Critical events must be captured immediately before processing can continue, but informational events do not need to delay processing. What is the most appropriate solution to record these different types of events?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Log all events using the Kinesis Producer Library.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Log critical events using the Kinesis Producer Library, and log informational events using the PutRecords API method.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Log critical events using the PutRecords API method, and log informational events using the Kinesis Producer Library.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Log all events using the PutRecords API method. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Log critical events using the PutRecords API method, and log informational events using the Kinesis Producer Library.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as the core of this question is how to send event messages to Kinesis synchronously vs. asynchronously. The critical events must be sent synchronously, and the informational events can be sent asynchronously. The Kinesis Producer Library (KPL) implements an asynchronous send function, so it can be used for the informational messages. PutRecords is a synchronous send function, so it must be used for the critical events. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html" target="_blank">Developing Producers using KPL</a> </p><p><em>Because the KPL may buffer records before sending them to Kinesis Data Streams, it does not force the caller application to block and wait for a confirmation that the record has arrived at the server before continuing execution. A call to put a record into the KPL always returns immediately and does not wait for the record to be sent or a response to be received from the server. Instead, a <code>Future</code> object is created that receives the result of sending the record to Kinesis Data Streams at a later time. This is the same behavior as asynchronous clients in the AWS SDK.</em><br> </p><p>Option A is wrong as Kinesis Producer Library sends all events asynchronously </p><p>Option B is wrong as critical events needs to be synchronous and information events needs to be asynchronous.<br> </p><p>Option D is wrong as PutRecords sends all events synchronously<br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121319">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 6 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251356"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You have to identify potential fraudulent credit card transactions using Amazon Machine Learning. You have been given historical labeled data that you can use to create your model. You will also need to the ability to tune the model you pick. Which model type should you use?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 3 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Clustering</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Regression<br><b>C</b>. <input type="radio" disabled="">&nbsp;Binary<br><b>D</b>. <input type="radio" disabled="">&nbsp;Cannot be done using Amazon Machine Learning <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Binary<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as Binary classification can be used to predict for whether the transaction is fraudulent or not. </p><table class="blue"> <tbody> <tr> <th>Type of ML Problem </th> <th>Description </th> <th>Example </th> </tr> <tr> <td>Classification </td> <td>Pick one of N labels </td> <td>Cat, dog, horse, or bear </td> </tr> <tr> <td>Regression </td> <td>Predict numerical values </td> <td>Click-through rate </td> </tr> <tr> <td>Clustering </td> <td>Group similar examples </td> <td>Most relevant documents (unsupervised) </td> </tr> <tr> <td>Association rule learning </td> <td>Infer likely association patterns in data </td> <td>If you buy hamburger buns, you're likely to buy hamburgers (unsupervised) </td> </tr> <tr> <td>Structured output </td> <td>Create complex output </td> <td>Natural language parse trees, image recognition bounding boxes </td> </tr> <tr> <td>Ranking </td> <td>Identify position on a scale or status </td> <td>Search result ranking </td> </tr> </tbody> </table><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%202%20-%20%23125178">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 7 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251357"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You've been asked by the VP of People to showcase the current breakdown of the headcount for each department within your organization. What chart do you select to do this to make it easy to compare each department?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Line chart</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Column chart<br><b>C</b>. <input type="radio" disabled="">&nbsp;Pie chart<br><b>D</b>. <input type="radio" disabled="">&nbsp;Scatter plot <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Pie chart<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as Pie charts are best to use when you are trying to compare parts of a whole, which is ideal for the use case. They do not show changes over time. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/quicksight/latest/user/working-with-visual-types.html" target="_blank">QuickSight Chart Types</a> </p><p>Option A is wrong as Line graphs are used to track changes over short and long periods of time. </p><p>Option B is wrong as Column chart is a data visualization where each category is represented by a rectangle, with the height of the rectangle being proportional to the values being plotted<br> </p><p>Option D is wrong as Scatter plot is a two-dimensional data visualization that uses dots to represent the values obtained for two different variables - one plotted along the x-axis and the other plotted along the y-axis<br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23123021">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 8 of 50 </b><small>- Multiple Answer</small> <span style="float:right;" class="grad_5251358"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An online gaming company uses DynamoDB to store user activity logs and is experiencing throttled writes on the company’s DynamoDB table. The company is NOT consuming close to the provisioned capacity. The table contains a large number of items and is partitioned on user and sorted by date. The table is 200GB and is currently provisioned at 10K WCU and 20K RCU. Which two additional pieces of information are required to determine the cause of the throttling? (Choose two.)<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 3 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="checkbox" checked="true" disabled="">&nbsp;The structure of any GSIs that have been defined on the table</span><br><b>B</b>. <input type="checkbox" disabled="">&nbsp;CloudWatch data showing consumed and provisioned write capacity when writes are being throttled<br><b>C</b>. <input type="checkbox" disabled="">&nbsp;Application-level metrics showing the average item size and peak update rates for each attribute<br><b>D</b>. <input type="checkbox" disabled="">&nbsp;The structure of any LSIs that have been defined on the table<br><b>E</b>. <input type="checkbox" disabled="">&nbsp;The maximum historical WCU and RCU for the table <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. CloudWatch data showing consumed and provisioned write capacity when writes are being throttled<br><b>D</b>. The structure of any LSIs that have been defined on the table<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answers are <strong>B &amp; D </strong>as the key reason for throttling is hot keys, as the application does not consume the entire provisioned capacity. </p><p>Option B as CloudWatch helps shows the stats for consumed vs provisioned throughput capacity. </p><p>Option D as an LSI consumes WCU for writes on the primary table. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/premiumsupport/knowledge-center/throttled-ddb/" target="_blank">Throttled DB</a> &amp; <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html#LSI.ThroughputConsiderations" target="_blank">DynamoDB LSI Considerations</a> &amp; <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/monitoring-cloudwatch.html" target="_blank">DynamoDB CloudWatch</a> </p><p><em>Partitions are usually throttled when they are accessed by your downstream applications much more frequently than other partitions (that is, a "hot" partition), or when workloads rely on short periods of time with high usage (a "burst" of read or write activity). </em><em>To avoid hot partitions and throttling, you must optimize your table and partition structure. </em> </p><p><em><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html#LSI.ThroughputConsiderations" target="_blank"></a>Distribute your read operations and <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-sharding.html" target="_blank">write operations</a> as evenly as possible across your table. A "hot" partition can degrade the overall performance of your table.</em> </p><p><em><strong>Write Capacity Units</strong> - When an item in a table is added, updated, or deleted, updating the local secondary indexes will consume provisioned write capacity units for the table. The total provisioned throughput cost for a write is the sum of write capacity units consumed by writing to the table and those consumed by updating the local secondary indexes.</em> </p><p><em></em> </p><table> <tbody> <tr> <td><code>How can I determine how much of my provisioned throughput is being used?</code> </td> <td><p>You can monitor <code>ConsumedReadCapacityUnits</code> or <code>ConsumedWriteCapacityUnits</code> over the specified time period, to track how much of your provisioned throughput is being used. </p></td> </tr> </tbody> </table><p>Option A is wrong as GSI does not impact primary table throughput capacity. </p><p>Option C is wrong as the provisioned capacity is not exceeded, the average would not be of much help. </p><p>Option E is wrong as the provisioned capacity is not exceeded the historical stats would not be of much help. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121403">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 9 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251359"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A Redshift data warehouse has different user teams that need to query the same table with very different query types. These user teams are experiencing poor performance. Which action improves performance for the user teams in this situation?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Create custom table views.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Add interleaved sort keys per team.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Maintain team-specific copies of the table.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Add support for workload management queue hopping. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Add interleaved sort keys per team.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as multiple teams query different columns with different queries it would be best to use Interleaved keys to improve performance. Interleaved keys are provided to help with the limitations of compound keys. They are designed to weigh each column in the key evenly, allowing improved performance regardless of which columns in the key you’re filtering. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/redshift/latest/dg/t_Sorting_data.html#t_Sorting_data-interleaved" target="_blank">Redshift Interleaved Sort Keys</a> </p><p><em>An interleaved sort gives equal weight to each column, or subset of columns, in the sort key. If multiple queries use different columns for filters, then you can often improve performance for those queries by using an interleaved sort style. When a query uses restrictive predicates on secondary sort columns, interleaved sorting significantly improves query performance as compared to compound sorting.</em><em></em><br> </p><p>Options A &amp; C are wrong as they create duplicate copies or refer to the same underlying table and would not improve performance. </p><p>Option D is wrong as the key here is queries on same table as Amazon <a href="https://docs.aws.amazon.com/redshift/latest/dg/c_workload_mngmt_classification.html" target="_blank">Redshift workload management (WLM)</a> enables users to flexibly manage priorities within workloads so that short, fast-running queries won't get stuck in queues behind long-running queries. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121335">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 10 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251360"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A data engineer needs to collect data from multiple Amazon Redshift clusters within a business and consolidate the data into a single central data warehouse. Data must be encrypted at all times while at rest or in flight. What is the most scalable way to build this data collection process?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Run an ETL process that connects to the source clusters using SSL to issue a SELECT query for new data, and then write to the target data warehouse using an INSERT command over another SSL secured connection.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use AWS KMS data key to run an UNLOAD ENCRYPTED command that stores the data in an unencrypted S3 bucket; run a COPY command to move the data into the target cluster.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Run an UNLOAD command that stores the data in an S3 bucket encrypted with an AWS KMS data key; run a COPY command to move the data into the target cluster.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Connect to the source cluster over an SSL client connection, and write data records to Amazon Kinesis Firehose to load into your target data warehouse. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Use AWS KMS data key to run an UNLOAD ENCRYPTED command that stores the data in an unencrypted S3 bucket; run a COPY command to move the data into the target cluster.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as the UNLOAD ENCRYPTED command automatically stores the data encrypted using-client side encryption and uses HTTPS to encrypt the data during the transfer to S3. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/redshift/latest/dg/c_unloading_data.html" target="_blank">Redshift Unloading Data</a> </p><p><em>UNLOAD automatically creates files using Amazon S3 server-side encryption with AWS-managed encryption keys (SSE-S3). You can also specify server-side encryption with an AWS Key Management Service key (SSE-KMS) or client-side encryption with a customer-managed key (CSE-CMK).</em><span class="redactor-invisible-space"><em></em><br></span> </p><p>Option C is wrong because the data would not be encrypted in flight, and you cannot encrypt an entire bucket with a KMS key. </p><p>Options A &amp; D are wrong as the most scalable solutions are the UNLOAD/COPY solutions because they will work in parallel. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121322">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 11 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251361"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> Your company releases new features with high frequency while demanding high application availability. As part of the application’s A/B testing, logs from each updated Amazon EC2 instance of the application need to be analyzed in near real-time, to ensure that the application is working flawlessly after each deployment. If the logs show any anomalous behavior, then the application version of the instance is changed to a more stable one. Which of the following methods should you use for shipping and analyzing the logs in a highly available manner?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Ship the logs to Amazon S3 for durability and use Amazon EMR to analyze the logs in a batch manner each hour.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Ship the logs to Amazon CloudWatch Logs and use Amazon EMR to analyze the logs in a batch manner each hour.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Ship the logs to an Amazon Kinesis stream and have the consumers analyze the logs in a live manner.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Ship the logs to a large Amazon EC2 instance and analyze the logs in a live manner.<br><b>E</b>. <input type="radio" disabled="">&nbsp;Store the logs locally on each instance and then have an Amazon Kinesis stream pull the logs for live analysis <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Ship the logs to an Amazon Kinesis stream and have the consumers analyze the logs in a live manner.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as the data can be ingested into the Kinesis streams using agents and the logs can then be analyzed real time. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/blogs/big-data/implement-serverless-log-analytics-using-amazon-kinesis-analytics/" target="_blank">Kinesis Serverless log Analytics</a> </p><p><em>Amazon Kinesis Streams enables you to build custom applications that process or analyze <a adhocenable="false" href="https://aws.amazon.com/streaming-data/" target="_blank">streaming data</a> for specialized needs. Amazon Kinesis Streams can continuously capture and store terabytes of data per hour from hundreds of thousands of sources such as website clickstreams, financial transactions, social media feeds, IT logs, and location-tracking events.</em><em></em><br> </p><p>Option A &amp; B are wrong as analyzing the logs every hour does not provide real time capability as required. </p><p>Option D is wrong as storing the logs on EC2 instance is not a scalable performant model </p><p>Option E is wrong as Amazon Kinesis stream work on the push mechanism, and the data from the EC2 instances need to be ingested into the Kinesis streams. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121282">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 12 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251362"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> Your company is in the process of developing a next generation pet collar that collects biometric information to assist families with promoting healthy lifestyles for their pets. Each collar will push 30kb of biometric data In JSON format every 2 seconds to a collection platform that will process and analyze the data providing health trending information back to the pet owners and veterinarians via a web portal Management has tasked you to architect the collection platform ensuring the following requirements are met. Provide the ability for real-time analytics of the inbound biometric data to ensure processing of the biometric data is highly durable, Elastic and parallel. The results of the analytic processing should be persisted for data mining. Which architecture outlined below will meet the initial requirements for the collection platform? <br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Utilize S3 to collect the inbound sensor data analyze the data from S3 with a daily scheduled Data Pipeline and save the results to a Redshift Cluster.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Utilize Amazon Kinesis to collect the inbound sensor data, analyze the data with Kinesis clients and save the results to a Redshift cluster using EMR.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Utilize SQS to collect the inbound sensor data analyze the data from SQS with Amazon Kinesis and save the results to a Microsoft SQL Server RDS instance.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Utilize EMR to collect the inbound sensor data, analyze the data from EMR with Amazon Kinesis and save the results to DynamoDB. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Utilize Amazon Kinesis to collect the inbound sensor data, analyze the data with Kinesis clients and save the results to a Redshift cluster using EMR.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Key point here to architect durable collection platform with real time analytics, data mining storage. </p><p>Correct answer is <strong>B</strong> to use Kinesis to capture the data in a elastic, durable and parallel manner. Analyze data with Kinesis clients and store data to Redshift for data mining using EMR. </p><p>Option A is wrong as S3 would not be ideal to capture data with that frequency and daily job will not provide real time analytics </p><p>Option C is wrong as SQS is not an ideal solution to capture this data and Kinesis clients are required to analyze the data. SQL server might not be a scalable option </p><p>Option D is wrong as EMR alone is not ideal to capture data and would need specific frameworks like Kafka to capture data for processing. Also real time analytics needs to done using Spark Streaming and not EMR alone. DynamoDB is not for data mining. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121253">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 13 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251363"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A social media customer has data from different data sources including RDS running MySQL, Redshift, and Hive on EMR. To support better analysis, the customer needs to be able to analyze data from different data sources and to combine the results. What is the most cost-effective solution to meet these requirements?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Load all data from a different database/warehouse to S3. Use Redshift COPY command to copy data to Redshift for analysis.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Install Presto on the EMR cluster where Hive sits. Configure MySQL and PostgreSQL connector to select from different data sources in a single query.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Spin up an Elasticsearch cluster. Load data from all three data sources and use Kibana to analyze.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Write a program running on a separate EC2 instance to run queries to three different systems. Aggregate the results after getting the responses from all three systems. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Install Presto on the EMR cluster where Hive sits. Configure MySQL and PostgreSQL connector to select from different data sources in a single query.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as Presto can help query over multiple datasources and also provides connectors to interact directly MySQL, Redshift and Hive. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/emr/features/presto/" target="_blank">EMR Presto</a> </p><p><em>Presto is an open-source distributed SQL query engine optimized for low-latency, ad-hoc analysis of data. It supports the ANSI SQL standard, including complex queries, aggregations, joins, and window functions. Presto can process data from multiple data sources including the Hadoop Distributed File System (HDFS) and Amazon S3.</em><br> </p><p>Option A is wrong as data is replicated and is not a cost effective solution. </p><p>Option C is wrong as Elasticsearch does not provide analytics capabilities.<br> </p><p>Option D is wrong as running on EC2 instances is not a scalable and cost-effective solution.<br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121385">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 14 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251364"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> Management has requested a comparison of total sales performance in the five North American regions in January. They're hoping to determine how to allocate a budget to regions based on performance in that single period. What sort of visualization do you use in Amazon QuickSight?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 3 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Bar chart</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Line chart<br><b>C</b>. <input type="radio" disabled="">&nbsp;Stacked area chart<br><b>D</b>. <input type="radio" disabled="">&nbsp;Histogram <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Bar chart<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as Bar Chart can be used to represent the data for comparison in sales for each region. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/quicksight/latest/user/working-with-visual-types.html" target="_blank">QuickSight Visual Types</a><br> </p><p>Option B is wrong as Use line charts to compare changes in measure values over period of time </p><p>Option C is wrong as Stacked area chart is the extension of a basic area chart to display the evolution of the value of several groups on the same graphic. <br> </p><p>Option D is wrong as <strong>Histograms</strong> are sometimes confused with bar <strong>charts</strong>. A <strong>histogram</strong> is used for continuous data, where the bins represent ranges of data, while a bar <strong>chart</strong> is a <strong>plot </strong>of categorical variables.<br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23123034">AWS BDS-C00 Question feedback</a></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 15 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251365"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A new client is requesting a tool that will provide fast query performance for enterprise reporting and business intelligence workloads, particularly those involving extremely complex SQL with multiple joins and sub-queries. They also want the ability to give analysts access to a central system through tradition SQL clients that allow them to explore and familiarize themselves with the data. What solution do you initially recommend they investigate?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 6 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;SQS</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Redshift<br><b>C</b>. <input type="radio" disabled="">&nbsp;Athena<br><b>D</b>. <input type="radio" disabled="">&nbsp;EMR <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Redshift<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as Redshift is a fully managed data warehousing solution providing standard SQL interface and ability to run complex queries. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/redshift/faqs/" target="_blank">Redshift</a> </p><p><em>Amazon Redshift is a fast, fully managed data warehouse that makes it simple and cost-effective to analyze all your data using standard SQL and your existing Business Intelligence (BI) tools. It allows you to run complex analytic queries against petabytes of structured data, using sophisticated query optimization, columnar storage on high-performance local disks, and massively parallel query execution.</em><em></em><br> </p><p>Option A is wrong as SQS does not provide querying capability </p><p>Option C is wrong as Athena does not provide complex querying capability </p><p>Option D is wrong as EMR provides a managed Hadoop framework that makes it easy, fast, and cost-effective to process vast amounts of data across dynamically scalable Amazon EC2 instances. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23124749">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 16 of 50 </b><small>- Multiple Answer</small> <span style="float:right;" class="grad_5251366"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company stores data in an S3 bucket. Some of the data contains sensitive information. They need to ensure that the bucket complies with PCI DSS (Payment Card Industry Data Security Standard) compliance standards. Which of the following should be implemented to fulfill this requirement? (Select TWO)<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="checkbox" checked="true" disabled="">&nbsp;Enable server side encryption SSE for a bucket.</span><br><b>B</b>. <input type="checkbox" disabled="">&nbsp;Enable versioning for the bucket<br><b>C</b>. <input type="checkbox" disabled="">&nbsp;Ensure that access to the bucket is only given to one IAM role<br><b>D</b>. <input type="checkbox" disabled="">&nbsp;Ensure that objects from the bucket are request only via HTTPS <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Enable server side encryption SSE for a bucket.<br><b>D</b>. Ensure that objects from the bucket are request only via HTTPS<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answers are <strong>A &amp; D </strong>as one of the requirement is data security with encryption at rest and in transit. PCI DSS helps ensure that companies maintain a secure environment for storing, processing, and transmitting credit card information. </p><p>Option B is wrong as versioning only helps maintain data versions and are helpful to recover from accidental overwrites or deletions. </p><p>Option C is wrong as this is not a requirement and a best practice. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121360">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 17 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251367"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> Your company sells consumer devices and needs to record the first activation of all sold devices. Devices are not activated until the information is written on a persistent database. Activation data is very important for your company and must be analyzed daily with a MapReduce job. The execution time of the data analysis process must be less than three hours per day. Devices are usually sold evenly during the year, but when a new device model is out, there is a predictable peak in activation's, that is, for a few days there are 10 times or even 100 times more activation's than in average day. Which of the following databases and analysis framework would you implement to better optimize costs and performance for this workload?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Amazon RDS and Amazon Elastic MapReduce with Spot instances.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Amazon DynamoDB and Amazon Elastic MapReduce with Spot instances.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Amazon RDS and Amazon Elastic MapReduce with Reserved instances.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Amazon DynamoDB and Amazon Elastic MapReduce with Reserved instances <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Amazon DynamoDB and Amazon Elastic MapReduce with Spot instances.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Key point here is to optimize cost and performance only for the increased workload only, not the existing one </p><p>Refer <a href="https://d0.awsstatic.com/whitepapers/aws-amazon-emr-best-practices.pdf" target="_blank">EMR best practices</a> - For unpredictable workloads, the suggested pricing model is Spot or On-Demand.<br> </p><p>Correct answer is <strong>B</strong> and preferred over A as DynamoDB would be preferred over RDS for the throughput supported and Spot instances to reduce cost and handle the temporary workload. </p><p>Option C &amp; D are wrong as Reserved instances would be preferred for a consistent and predictable workload and would prove costly in this scenario.<br> </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121213">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 18 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251368"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> Your company is storing millions of sensitive transactions across thousands of 100-GB files that must be encrypted in transit and at rest. Analysts concurrently depend on subsets of files, which can consume up to 5TB of space, to generate simulations that can be used to steer business decisions. You are required to design an AWS solution that can cost effectively accommodate the long-term storage and in-flight subsets of data.<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Use Amazon Simple Storage Service (S3) with server-side encryption, and run simulations on subsets in ephemeral drives on Amazon EC2.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use Amazon S3 with server-side encryption, and run simulations on subsets in-memory on Amazon EC2.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Use HDFS on Amazon EMR, and run simulations on subsets in ephemeral drives on Amazon EC2.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use HDFS on Amazon Elastic MapReduce (EMR), and run simulations on subsets in-memory on Amazon Elastic Compute Cloud (EC2).<br><b>E</b>. <input type="radio" disabled="">&nbsp;Store the full data set in encrypted Amazon Elastic Block Store (EBS) volumes, and regularly capture snapshots that can be cloned to EC2 workstations <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Use Amazon Simple Storage Service (S3) with server-side encryption, and run simulations on subsets in ephemeral drives on Amazon EC2.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as the S3 with SSE provides encryption at rest and HTTPS can be used to push data to S3 for encryption in transit. S3 provides an option for cost effective long term storage. Ephemeral drives would help run simulations and the data would lost once the EC2 instance is terminated. </p><p>Option B is wrong as S3 with SSE provides encryption at rest and HTTPS can be used to push data to S3 for encryption in transit. However, in memory simulations with 5 TB data would not be feasible. </p><p>Option C &amp; D are wrong as HDFS is not an cost effective solution as data nodes would be required to store the data and it does not provide encryption by default. </p><p>Option E is wrong as EBS for long term storage is an expensive option. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121244">AWS BDS-C00 Question feedback</a></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 19 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251369"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An administrator needs to design the event log storage architecture for events from mobile devices. The event data will be processed by an Amazon EMR cluster daily for aggregated reporting and analytics before being archived. How should the administrator recommend storing the log data?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Create an Amazon S3 bucket and write log data into folders by device. Execute the EMR job on the device folders.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Create an Amazon DynamoDB table partitioned on the device and sorted on date, write log data to table. Execute the EMR job on the Amazon DynamoDB table.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Create an Amazon S3 bucket and write data into folders by day. Execute the EMR job on the daily folder.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Create an Amazon DynamoDB table partitioned on EventID, write log data to table. Execute the EMR job on the table. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Create an Amazon S3 bucket and write data into folders by day. Execute the EMR job on the daily folder.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as the EMR jobs needs to process daily data, it would be best to partition the data by day. </p><p>Refer AWS documentation - <a href="https://d0.awsstatic.com/whitepapers/aws-amazon-emr-best-practices.pdf" target="_blank">EMR Best Practices</a> </p><p><em>Data partitioning is an essential optimization to your data processing workflow. Without any data partitioning in place, your data processing job needs to read or scan all available data sets and apply additional filters in order to skip unnecessary data. Such architecture might work for a low volume of data, but scanning the entire data set is a very time consuming and expensive approach for larger data sets. Data partitioning lets you create unique buckets of data and eliminate the need for a data processing job to read the entire data set.<br> </em> </p><p><em>Three considerations determine how you partition your data: </em> </p><ul><em> <li>Data type (time series)</li> <li>Data processing frequency (per hour, per day, etc.)</li> <li>Data access and query pattern (query on time vs. query on geo location)</li> </em> </ul><p><em>For instance, if you are processing a time-series data set where you need to process your data once every hour and your data-access pattern is based on time, partitioning your data based on date makes the most sense. An example of such data processing would be processing your daily logs. If you have incoming logs from variety of data sources (web servers, devices etc.), then creating partitions of data based on the hour of the day gives you a date-based partitioning scheme. </em> </p><p><em>The structure of such partitioning scheme will look similar to the following: </em> </p><p><em>/data/logs/YYYY-MM-DD-HH/logfiles for this given hour, where YYYY-MM-DD-HH changes based on the current log ingest time. </em> </p><p>Option A is wrong as the data needs to be processed by day, it would be best to partition the data by date instead of device id.<br> </p><p>Options B &amp; D are wrong as DynamoDB is not an ideal solution for storage and archival of logs data and does not provide easy integration with EMR.<br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121336">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 20 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251370"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> Your company uses DynamoDB to support their mobile application and S3 to host the images and other documents shared between users. DynamoDB has a table with 60 partitions and is being heavily accessed by users. The queries run by users do not fully use the per-partition’s throughput. However there are times when in less than 3 minutes, a heavy load of queries flow in and this happen occasionally. Sometimes there are many background tasks that are running in background. How can DynamoDB be configured to handle the workload?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 3 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <b>A</b>. <input type="radio" disabled="">&nbsp;Using Burst Capacity effectively<br><b>B</b>. <input type="radio" disabled="">&nbsp;Using Adaptive Capacity<br><b>C</b>. <input type="radio" disabled="">&nbsp;Design Partition Keys to distribute workload evenly<br><b>D</b>. <input type="radio" disabled="">&nbsp;Using Write Sharding to distribute Workloads Evenly <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Using Burst Capacity effectively<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A </strong>as DynamoDB burst capacity can retain part of unused provisioned capacity, upto 5 minutes, allowing application to burst. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html" target="_blank">DynamoDB Best Practices</a> </p><p><em>DynamoDB provides some flexibility in your per-partition throughput provisioning by providing <strong>burst capacity</strong>, as follows. Whenever you are not fully using a partition's throughput, DynamoDB reserves a portion of that unused capacity for later bursts of throughput to handle usage spikes.</em> </p><p><em>DynamoDB currently retains up to five minutes (300 seconds) of unused read and write capacity. During an occasional burst of read or write activity, these extra capacity units can be consumed quickly—even faster than the per-second provisioned throughput capacity that you've defined for your table.</em> </p><p><em>DynamoDB can also consume burst capacity for background maintenance and other tasks without prior notice.</em> </p><p>Option B is wrong as Adaptive Capacity enables your application to continue reading and writing to hot partitions without being throttled, provided that traffic does not exceed your table’s total provisioned capacity or the partition maximum capacity. </p><p>Option C is wrong as the partition keys are designed fine, as the application does not consume its total capacity and is not throttled. </p><p>Option D is wrong as Write sharding only allows for writes. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%202%20-%20%23125350">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 21 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251371"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An online photo album app has a key design feature to support multiple screens (e.g, desktop, mobile phone, and tablet) with high-quality displays. Multiple versions of the image must be saved in different resolutions and layouts. The image-processing Java program takes an average of five seconds per upload, depending on the image size and format. Each image upload captures the following image metadata: user, album, photo label, upload timestamp.The app should support the following requirements: Hundreds of user image uploads per second Maximum image upload size of 10 MB Maximum image metadata size of 1 KB Image displayed in optimized resolution in all supported screens no later than one minute after image upload Which strategy should be used to meet these requirements? <br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Write image and metadata to RDS with BLOB data type. Use AWS Data Pipeline to run the image processing and save the image output to Amazon S3 and metadata to the app repository DB.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Write images and metadata to Amazon Kinesis. Use a Kinesis Client Library (KCL) application to run the image processing and save the image output to Amazon S3 and metadata to the app repository DB.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Write image and metadata to Amazon Kinesis. Use Amazon Elastic MapReduce (EMR) with Spark Streaming to run image processing and save the images output to Amazon S3 and metadata to app repository DB.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Upload image with metadata to Amazon S3, use Lambda function to run the image processing and save the images output to Amazon S3 and metadata to the app repository DB. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Upload image with metadata to Amazon S3, use Lambda function to run the image processing and save the images output to Amazon S3 and metadata to the app repository DB.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as the images with metadata can be uploaded to S3. S3 can support both the size and request rate. A Lambda function can be triggered to convert and same the images output back to S3 and metadata to app DB. </p><p>Option A is wrong as RDS is not an ideal storage for images as well as handle hundreds of uploads per second.<br> </p><p>Options B &amp; C are wrong as Kinesis support max message size of 1MB per record and would not be able to support 10MB of image files. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121391">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 22 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251372"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company is using Amazon Machine Learning as part of a medical software application. The application will predict the most likely blood type for a patient based on a variety of other clinical tests that are available when blood type knowledge is unavailable. What is the appropriate model choice and target attribute combination for this problem?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Multi-class classification model with a categorical target attribute.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Regression model with a numeric target attribute.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Binary Classification with a categorical target attribute.<br><b>D</b>. <input type="radio" disabled="">&nbsp;K-Nearest Neighbors model with a multi-class target attribute. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Multi-class classification model with a categorical target attribute.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as the blood group types are limited, a multi-class classification model can help classification the result into the blood groups </p><p>Option B is wrong as regression is for predictive modelling </p><p>Option C is wrong as Binary classification can only classify a yes or no. </p><p>Option D is wrong as K-Nearest Neighbours is more for grouping unknown data. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121388">AWS BDS-C00 Question feedback</a></p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 23 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251373"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company is developing a video application that will emit a log stream. Each record in the stream may contain up to 400 KB of data. To improve the video-streaming experience, it is necessary to collect a subset of metrics from the stream to be analyzed for trends over time using complex SQL queries. A Solutions Architect will create a solution that allows the application to scale without customer interaction. Which solution should be implemented to meet these requirements?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Send the log data to an Amazon Kinesis Data Firehose delivery stream. Use an AWS Lambda function to transform the data. Deliver the data to Amazon Redshift. Query the data in Amazon Redshift.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Send the log data to an Amazon SQS standard queue. Make the queue an event source for an AWS Lambda function that transforms the data and stores it in Amazon Redshift. Query the data in Amazon Redshift.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Send the log data to an Amazon CloudWatch Logs log group. Make the log group an event source for an AWS Lambda function that transforms the data and stores it in an Amazon S3 bucket. Query the data with Amazon Athena.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Send the log data to an Amazon Kinesis data stream. Subscribe an AWS Lambda function to the stream that transforms the data and sends it to a second data stream. Use Amazon Kinesis Data Analytics to query the data in the second stream. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Send the log data to an Amazon Kinesis data stream. Subscribe an AWS Lambda function to the stream that transforms the data and sends it to a second data stream. Use Amazon Kinesis Data Analytics to query the data in the second stream.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as the data can be captured using Kinesis Data Stream and Kinesis Data Analytics can be used to query on the streaming data using time or window queries to generate trend analysis. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/solutions/streaming-analytics-pipeline/" target="_blank">Streaming Analytics Pipeline</a> </p><p><em>Many Amazon Web Services (AWS) customers use <a href="https://aws.amazon.com/streaming-data/" target="_blank">streaming data</a> to gain real-time insight into customer activity and immediate business trends. Streaming data, which is generated continuously from thousands of data sources, includes a wide variety of data such as log files from your mobile or web applications, e-commerce purchases, in-game player activity, information from social networks, financial trading floors, or geospatial services, and telemetry from connected devices. This data can help companies make well-informed decisions and proactively respond to changing business conditions.</em> </p><p><em><a href="https://aws.amazon.com/kinesis/" target="_blank">Amazon Kinesis</a>, a platform for streaming data on AWS, offers powerful services that make it easier to build data processing applications, load massive volumes of streaming data, and analyze it in real time.</em> </p><p><img src="./bds2_files/real-time-web-analytics-with-kinesis-architecture.2f7f348cf627d65c5eb29d0548524e9b8be028b2.png"><br><br> </p><p>Options A, B &amp; C are wrong as they do not provide analytics on streaming data. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121263">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 24 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251374"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A data engineer in a manufacturing company is designing a data processing platform that receives a large volume of unstructured data. The data engineer must populate a well-structured star schema in Amazon Redshift. What is the most efficient architecture strategy for this purpose?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Transform the unstructured data using Amazon EMR and generate CSV data. COPY the CSV data into the analysis schema within Redshift.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Load the unstructured data into Redshift, and use string parsing functions to extract structured data for inserting into the analysis schema.<br><b>C</b>. <input type="radio" disabled="">&nbsp;When the data is saved to Amazon S3, use S3 Event Notifications and AWS Lambda to transform the file contents. Insert the data into the analysis schema on Redshift.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Normalize the data using an AWS Marketplace ETL tool, persist the results to Amazon S3, and use AWS Lambda to INSERT the data into Redshift. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Transform the unstructured data using Amazon EMR and generate CSV data. COPY the CSV data into the analysis schema within Redshift.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as the data volume is large, it can be processed using EMR to generate structured CSV data and then load the data into Redshift. </p><p>Refer AWS documentation - <a href="https://d1.awsstatic.com/whitepapers/enterprise-data-warehousing-on-aws.pdf" target="_blank">Data Warehousing on AWS</a> </p><p><em><em></em>Data in Amazon Redshift must be structured by a defined schema. Amazon Redshift doesn’t support an arbitrary schema structure for each row. If your data is unstructured, you can perform extract, transform, and load (ETL) on Amazon EMR to get the data ready for loading into Amazon Redshift. For JSON data, you can store key value pairs and use the native JSON functions in your queries.</em><br> </p><p>Option B is wrong as unstructured data cannot be loaded into Redshift.<br> </p><p>Option C is wrong as Lambda would not be able to handle large amounts of data due to its limitation. </p><p>Option D is wrong as market ETL tool is not needed and EMR can be used. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121330">AWS BDS-C00 Question feedback</a></p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 25 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251375"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You are deploying an application to track GPS coordinates of delivery trucks in the United States. Coordinates are transmitted from each delivery truck once every three seconds. You need to design an architecture that will enable real-time processing of these coordinates from multiple consumers. Which service should you use to implement data ingestion?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 3 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Amazon Kinesis</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;AWS Data Pipeline<br><b>C</b>. <input type="radio" disabled="">&nbsp;Amazon AppStream<br><b>D</b>. <input type="radio" disabled="">&nbsp;Amazon Simple Queue Service <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Amazon Kinesis<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Key point here is address real time data ingestion. </p><p>Correct answer is <strong>A</strong> </p><p><em>Amazon Kinesis is a platform for streaming data on AWS, making it easy to load and analyze streaming data, and also providing the ability for you to build custom streaming data applications for specialized needs.</em> </p><ul> <li><em>Use Amazon Kinesis Streams to collect and process large streams of data records in real time.</em></li> <li><em>Use Amazon Kinesis Firehose to deliver real-time streaming data to destinations such as Amazon S3 and Amazon Redshift.<br></em></li> <li><em>Use Amazon Kinesis Analytics to process and analyze streaming data with standard SQL.</em></li> </ul><p>Option B is wrong as <a href="https://aws.amazon.com/datapipeline/" target="_blank">Data Pipeline</a> is more of an orchestration service and just helps move data between different data stores. </p><p>Option C is wrong as <em>Amazon <a href="http://docs.aws.amazon.com/appstream/latest/developerguide/appstream-intro.html" target="_blank">AppStream</a> is an application streaming service that lets you stream your existing resource-intensive applications from the cloud without code modifications.</em><em></em> </p><p>Option D is wrong as SQS would not be able to handle large scale real time ingestion. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121747">AWS BDS-C00 Question feedback</a></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 26 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251376"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You have a customer-facing application running on multiple M3 instances in two AZs. These instances are in an auto-scaling group configured to scale up when load increases. After taking a look at your CloudWatch metrics, you realize that during specific times every single day, the auto-scaling group has a lot more instances than it normally does. Despite this, one of your customers is complaining that the application is very slow to respond during those time periods every day. The application is reading and writing to a DynamoDB table which has 400 Write Capacity Units and 400 Read Capacity Units. The primary key is the company ID, and the table is storing roughly 20 TB of data. Which solution would solve the issue in a scalable and cost-effective manner?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Use data pipelines to migrate your DynamoDB table to a new DynamoDB table with a different primary key that evenly distributes the dataset across the table.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Add a caching layer in front of the web application with ElastiCache Memcached, or Redis.<br><b>C</b>. <input type="radio" disabled="">&nbsp;DynamoDB is not a good solution for this use case. Instead, create a data pipeline to move data from DynamoDB to Amazon RDS, which is more suitable for this.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Double the number of Read and Write Capacity Units. The DynamoDB table is being throttled when customers from the same company all use the table at the same time. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Use data pipelines to migrate your DynamoDB table to a new DynamoDB table with a different primary key that evenly distributes the dataset across the table.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as a single company is facing the issue and it would be a hot key issue cause of the primary key being Company ID. Data Pipeline can be used to migrate the data. </p><p>Option B is wrong as Elasticache may be reduce the load depending upon the queries. </p><p>Option C is wrong as RDS would not be able to handle the huge amount of data.<br> </p><p>Option D is wrong as this is not a cost-effective solution.<br> </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%202%20-%20%23125177">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 27 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251377"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> Your enterprise application requires key-value storage as the database. The data is expected to be about 10 GB the first month and grow to 2 PB over the next two years. There are no other query requirements at this time. What solution would you recommend?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 3 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Hive on HDFS</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;RDS MySQL<br><b>C</b>. <input type="radio" disabled="">&nbsp;HBase on HDFS<br><b>D</b>. <input type="radio" disabled="">&nbsp;Hadoop with Spark <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. HBase on HDFS<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as HBase on HDFS provide the ability the store the large amount of data in a non-relational key-value format. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/emr/features/hbase/" target="_blank">EMR HBase</a> </p><p><em>HBase is an open source, non-relational, distributed database developed as part of the Apache Software Foundation's Hadoop project. HBase runs on top of Hadoop Distributed File System (HDFS) to provide non-relational database capabilities for the Hadoop ecosystem.<br></em> </p><p><em>HBase works seamlessly with Hadoop, sharing its file system and serving as a direct input and output to the MapReduce framework and execution engine. HBase also integrates with Apache Hive, enabling SQL-like queries over HBase tables, joins with Hive-based tables, and support for Java Database Connectivity (JDBC). </em><em></em><br> </p><p>Option B is wrong as RDS would not support huge amount of data and is a relational database. </p><p>Options A &amp; D is wrong as they do not provide key-value storage format. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%202%20-%20%23125176">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 28 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251378"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A system needs to collect on-premises application spool files into a persistent storage layer in AWS. Each spool file is 2 KB. The application generates 1 M files per hour. Each source file is automatically deleted from the local server after an hour. What is the most cost-efficient option to meet these requirements?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Write file contents to an Amazon DynamoDB table.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Copy files to Amazon S3 Standard Storage.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Write file contents to Amazon ElastiCache.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Copy files to Amazon S3 infrequent Access Storage. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Write file contents to an Amazon DynamoDB table.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as the provisioned throughput required for DynamoDB would be most cost efficient as compared to the PUT requests for S3. </p><p><a href="https://calculator.s3.amazonaws.com/index.html#s=DynamoDB&amp;r=IAD&amp;key=calc-6689722E-38FC-4301-AC15-73DCC3ABCF2F" target="_blank">DynamoDB Calculation</a> vs <a href="https://calculator.s3.amazonaws.com/index.html#s=DynamoDB&amp;r=IAD&amp;key=calc-6689722E-38FC-4301-AC15-73DCC3ABCF2F" target="_blank">S3 Calculation</a> (Considering 31 days) </p><p>Option C is wrong as ElastiCache is not ideal storage, but its more ideal for caching </p><p>Options B &amp; D are wrong as PUT operations on S3 for small files would be expensive as compared to DynamoDB inserts. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121343">AWS BDS-C00 Question feedback</a></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 29 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251379"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A data engineer needs to architect a data warehouse for an online retail company to store historic purchases. The data engineer needs to use Amazon Redshift. To comply with PCI:DSS and meet corporate data protection standards, the data engineer must ensure that data is encrypted at rest and that the keys are managed by a corporate on-premises HSM. Which approach meets these requirements in the most cost-effective manner?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Create a VPC, and then establish a VPN connection between the VPC and the on-premises network. Launch the Amazon Redshift cluster in the VPC, and configure it to use your corporate HSM.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use the AWS CloudHSM service to establish a trust relationship between the CloudHSM and the corporate HSM over a Direct Connect connection. Configure Amazon Redshift to use the CloudHSM device.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Configure the AWS Key Management Service to point to the corporate HSM device, and then launch the Amazon Redshift cluster with the KMS managing the encryption keys.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use AWS Import/Export to import the corporate HSM device into the AWS Region where the Amazon Redshift cluster will launch, and configure Redshift to use the imported HSM. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Create a VPC, and then establish a VPN connection between the VPC and the on-premises network. Launch the Amazon Redshift cluster in the VPC, and configure it to use your corporate HSM.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as Amazon Redshift can use an on-premises HSM for key management over the VPN, which ensures that the encryption keys are locally managed. </p><p>Option B is wrong as although it is possible as CloudHSM can cluster to an on-premises HSM. But then key management could be performed on either the on-premises HSM or CloudHSM, and that doesn’t meet the design goal. </p><p>Option C is wrong as does not describe a valid feature of KMS and violates the requirement for the corporate HSM to manage the keys requirement, even if it were possible. </p><p>Option D is wrong as it is not possible because you cannot put hardware into an AWS Region. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121324">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 30 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251380"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> Your application generates a 1 KB JSON payload that needs to be queued and delivered to EC2 instances for applications. At the end of the day, the application needs to replay the data for the past 24 hours. In the near future, you also need the ability for other multiple EC2 applications to consume the same stream concurrently. What is the best solution for this?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 3 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Kinesis Data Streams</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Kinesis Firehose<br><b>C</b>. <input type="radio" disabled="">&nbsp;SNS<br><b>D</b>. <input type="radio" disabled="">&nbsp;SQS <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Kinesis Data Streams<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as Kinesis Data Streams allows the ability for replaying the data as well access to the same data to multiple Kinesis client applications. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/kinesis/data-streams/faqs/" target="_blank">Kinesis Data Streams FAQ</a> </p><p><em>Amazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs. You can continuously add various types of data such as clickstreams, application logs, and social media to an Amazon Kinesis data stream from hundreds of thousands of sources. Within seconds, the data will be available for your Amazon Kinesis Applications to read and process from the stream.</em><em></em><br> </p><p><em>Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).</em> </p><p><em>Amazon Simple Queue Service (Amazon SQS) offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows.</em> </p><p>Option B is wrong as Kinesis Firehose only allows data transfer to S3, Redshift. It does not provide the replay capability or buffering of messages </p><p>Option C is wrong as SNS can provide the message to multiple subscribers, however it cannot replay the data. </p><p>Option D is wrong SQS does not provide the ability to replay or access to multiple consumer. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121743">AWS BDS-C00 Question feedback</a></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 31 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251381"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An organization is designing an application architecture. The application will have over 100 TB of data and will support transactions that arrive at rates from hundreds per second to tens of thousands per second, depending on the day of the week and time of the day. All transaction data must be durably and reliably stored. Certain read operations must be performed with strong consistency. Which solution meets these requirements?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 3 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Use Amazon DynamoDB as the data store and use strongly consistent reads when necessary.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use an Amazon Relational Database Service (RDS) instance sized to meet the maximum anticipated transaction rate and with the High Availability option enabled.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Deploy a NoSQL data store on top of an Amazon Elastic MapReduce (EMR) cluster, and select the HDFS High Durability option.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use Amazon Redshift with synchronous replication to Amazon Simple Storage Service (S3) and row-level locking for strong consistency. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Use Amazon DynamoDB as the data store and use strongly consistent reads when necessary.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as DynamoDB can store and handle the transactions. DynamoDB also supports strongly consistent reads. DynamoDB is also a managed AWS service. </p><p><em>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multiregion, multimaster database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and support peaks of more than 20 million requests per second.</em><br> </p><p>Option B is wrong as RDS does not meet the storage and request handling rate. </p><p>Option C is wrong as NoSQL datastore would need resources and handling. Also, EMR HDFS option is not a cost-effective option. </p><p>Option D is wrong ad Redshift is a data warehouse solution and not ideal to handle high frequency data collection. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121346">AWS BDS-C00 Question feedback</a></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 32 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251382"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A data engineer wants to use an Amazon Elastic Map Reduce for an application. The data engineer needs to make sure it complies with regulatory requirements. The auditor must be able to confirm at any point which servers are running and which network access controls are deployed. Which action should the data engineer take to meet this requirement?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <b>A</b>. <input type="radio" disabled="">&nbsp;Provide the auditor IAM accounts with the SecurityAudit policy attached to their group.<br><b>B</b>. <input type="radio" disabled="">&nbsp;Provide the auditor with SSH keys for access to the Amazon EMR cluster.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Provide the auditor with CloudFormation templates.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Provide the auditor with access to AWS DirectConnect to use their existing tools. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Provide the auditor IAM accounts with the SecurityAudit policy attached to their group.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as the SecurityAudit managed policy can provide the Auditors with the read only access to AWS Services. </p><p><img src="./bds2_files/121382.png"><br> </p><p>Option B is wrong as providing SSH keys is not a good practice. </p><p>Option C is wrong as it does not mention the Cluster was setup using CloudFormation. Also, CloudFormation templates may not give the actual picture of whats deployed.<br> </p><p>Option D is wrong as Direct Connect does not provide access to tools and it is still control using IAM.<br> </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121382">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 33 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251383"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You are using IOT sensors to monitor the movement of a group of hikers on a three day trek and send the information into an Kinesis Stream. They each have a sensor in their shoe and you know for certain that there is no problem with mobile coverage so all the data is getting back to the stream. You have used default settings for the stream. At the end of the third day the data is sent to an S3 bucket. When you go to interpret the data in S3 there is only data for the last day and nothing for the first 2 days. Which of the following is the most probable cause of this?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 3 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Temporary loss of mobile coverage; although mobile coverage was good in the area, even temporary loss of data will stop the streaming</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;You cannot send Kinesis data to the same bucket on consecutive days if you do not have versioning enabled on the bucket. If you don't have versioning enabled you would need to define 3 different buckets or else the data is overwritten each day<br><b>C</b>. <input type="radio" disabled="">&nbsp;Data records are only accessible for a default of 24 hours from the time they are added to a stream.<br><b>D</b>. <input type="radio" disabled="">&nbsp;A sensor probably stopped working on the second day. If one sensor fails, no data is sent to the stream until that sensor is fixed <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Data records are only accessible for a default of 24 hours from the time they are added to a stream.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as by default, Kinesis stores the records for 24 hours only. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/kinesis/streams/faqs/" target="_blank">Kinesis FAQs</a> </p><p><em>By default, <a href="https://aws.amazon.com/kinesis/streams/faqs/#datarecord" target="_blank">Records</a> of a stream are accessible for up to 24 hours from the time they are added to the stream. You can raise this limit to up to 7 days by enabling extended data retention.</em><em></em> </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121748">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 34 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251384"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A research scientist is planning for the one-time launch of an Elastic MapReduce cluster and is encouraged by her manager to minimize the costs. The cluster is designed to ingest 200TB of genomics data with a total of 100 Amazon EC2 instances and is expected to run for around four hours. The resulting data set must be stored temporarily until archived into an Amazon RDS Oracle instance. Which option will help save the most money while meeting requirements?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Store ingest and output files in Amazon S3. Deploy on-demand for the master and core nodes and spot for the task nodes.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Optimize by deploying a combination of on-demand, RI and spot-pricing models for the master, core and task nodes. Store ingest and output files in Amazon S3 with a lifecycle policy that archives them to Amazon Glacier.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Store the ingest files in Amazon S3 RRS and store the output files in S3. Deploy Reserved Instances for the master and core nodes and on-demand for the task nodes.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Deploy on-demand master, core and task nodes and store ingest and output files in Amazon S3 RRS <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Store ingest and output files in Amazon S3. Deploy on-demand for the master and core nodes and spot for the task nodes.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Key point here is to save most money while being able to process the huge data. </p><p>Correct answer is <strong>A</strong> as it follows best practice of using On demand for master and core and spot for task nodes also help reduce cost using spot instances. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances.html" target="_blank">EMR Instances</a> </p><p>Option B is wrong as RI will make it expensive as there is no consistent requirement </p><p>Option C is wrong as RI will make it expensive as there is no consistent requirement. </p><p>Option D is wrong as input should be in S3 standard, as re-ingesting the input data might end up being more costly then holding the data for limited time in standard S3 </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121238">AWS BDS-C00 Question feedback</a></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 35 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251385"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company needs to deploy a data lake solution for their data scientists in which all company data is accessible and stored in a central S3 bucket. The company segregates the data by business unit, using specific prefixes. Scientists can only access the data from their own business unit. The company needs a single sign-on identity and management solution based on Microsoft Active Directory (AD) to manage access to the data in Amazon S3. Which method meets these requirements?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Use AWS IAM Federation functions and specify the associated role based on the users' groups in AD.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Create bucket policies that only allow access to the authorized prefixes based on the users' group name in Active Directory.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Deploy the AD Synchronization service to create AWS IAM users and groups based on AD information.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use Amazon S3 API integration with AD to impersonate the users on access in a transparent manner. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Use AWS IAM Federation functions and specify the associated role based on the users' groups in AD.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is A as Identity Federation allows organizations to associate temporary credentials to users authenticated through an external identity provider such as Microsoft Active Directory (AD). These temporary credentials are linked to AWS IAM roles that grant access to the S3 bucket. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/" target="_blank">S3 Cross Account Access</a> (Role to S3 Bucket with Prefix access can be configured similarly) </p><p>Option B is wrong as it does not work because bucket policies are linked to IAM principles and cannot recognize AD attributes. </p><p>Option C is wrong as it does not work because AD Synchronization will not sync directly with AWS IAM, and custom synchronization would not result in Amazon S3 being able to see group information. </p><p>Option D is wrong as it isn't possible because there is no feature to integrate Amazon S3 directly with external identity providers. </p><p>Correct answer is B as <em>NOLOAD checks the integrity of all of the data without loading it into the database. The NOLOAD option displays any errors that would occur if you had attempted to load the data. All other options will require subsequent processing on the cluster which will consume resources.</em> </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-load.html#copy-noload" target="_blank">Data Load Copy Parameters</a> </p><p><em>If you want to validate your data without actually loading the table, use the NOLOAD option with the <a href="https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html">COPY</a> command.</em><br> </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121315">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 36 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251386"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company has several teams of analysts. Each team of analysts has their own cluster. The teams need to run SQL queries using Hive, Spark-SQL, and Presto with Amazon EMR. The company needs to enable a centralized metadata layer to expose the Amazon S3 objects as tables to the analysts. Which approach meets the requirement for a centralized metadata layer?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;EMRFS consistent view with a common Amazon DynamoDB table</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Bootstrap action to change the Hive Metastore to an Amazon RDS database<br><b>C</b>. <input type="radio" disabled="">&nbsp;s3distcp with the output Manifest option to generate RDS DDL<br><b>D</b>. <input type="radio" disabled="">&nbsp;Naming scheme support with automatic partition discovery from Amazon S3 <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. EMRFS consistent view with a common Amazon DynamoDB table<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as EMRFS consistent view using a DynamoDB table can be implemented using a separate common DynamoDB table. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emrfs-metadata.html" target="_blank">EMRFS Metadata</a> </p><p><em>EMRFS consistent view tracks consistency using a DynamoDB table to track objects in Amazon S3 that have been synced with or created by EMRFS. The metadata is used to track all operations (read, write, update, and copy), and no actual content is stored in it. This metadata is used to validate whether the objects or metadata received from Amazon S3 matches what is expected. This confirmation gives EMRFS the ability to check list consistency and read-after-write consistency for new objects EMRFS writes to Amazon S3 or objects synced with EMRFS. Multiple clusters can share the same metadata.</em><br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121333">AWS BDS-C00 Question feedback</a></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 37 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251387"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A Company has two batch processing applications that consume financial data about the day's stock transactions. Each transaction needs to be stored durably and guarantee that a record of each application is delivered so the audit and billing batch processing applications can process the data. However, the two applications run separately and several hours apart and need access to the same transaction information. After reviewing the transaction information for the day, the information no longer needs to be stored. What is the best way to architect this application? Choose the correct answer from the options below<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Use SQS for storing the transaction messages. When the billing batch process consumes each message, have the application create an identical message and place it in a different SQS for the audit application to use several hours later.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use SQS for storing the transaction messages; when the billing batch process performs first and consumes the message, write the code in a way that does not remove the message after consumed, so it is available for the audit application several hours later. The audit application can consume the SQS message and remove it from the queue when completed.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Store the transaction information in a DynamoDB table. The billing application can read the rows while the audit application will read the rows them remove the data.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use Kinesis to store the transaction information. The billing application will consume data from the stream, the audit application can consume the same data several hours later. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Use Kinesis to store the transaction information. The billing application will consume data from the stream, the audit application can consume the same data several hours later.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as the key point here is batch application and message being stored durably and delivery guarantee. Kinesis can store the data durably and allow access to multiple consumers without any dependencies. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/kinesis/data-streams/faqs/" target="_blank">Kinesis Data Streams</a> </p><p><em>Q: How does Amazon Kinesis Data Streams differ from Amazon SQS?</em> </p><p><em>Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).</em> </p><p><em>Amazon Simple Queue Service (Amazon SQS) offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows.</em> </p><p><em>Q: When should I use Amazon Kinesis Data Streams, and when should I use Amazon SQS?</em> </p><p><em>We recommend Amazon Kinesis Data Streams for use cases with requirements that are similar to the following:</em> </p><ul> <li><em>Routing related records to the same record processor (as in streaming MapReduce). For example, counting and aggregation are simpler when all records for a given key are routed to the same record processor.</em></li> <li><em>Ordering of records. For example, you want to transfer log data from the application host to the processing/archival host while maintaining the order of log statements.</em></li> <li><em>Ability for multiple applications to consume the same stream concurrently. For example, you have one application that updates a real-time dashboard and another that archives data to Amazon Redshift. You want both applications to consume data from the same stream concurrently and independently.</em></li> <li><em>Ability to consume records in the same order a few hours later. For example, you have a billing application and an audit application that runs a few hours behind the billing application. Because Amazon Kinesis Data Streams stores data for up to 7 days, you can run the audit application up to 7 days behind the billing application.</em></li> </ul><p><em>We recommend Amazon SQS for use cases with requirements that are similar to the following:</em> </p><p><em></em> </p><ul> <li><em>Messaging semantics (such as message-level ack/fail) and visibility timeout. For example, you have a queue of work items and want to track the successful completion of each item independently. Amazon SQS tracks the ack/fail, so the application does not have to maintain a persistent checkpoint/cursor. Amazon SQS will delete acked messages and redeliver failed messages after a configured visibility timeout.</em></li> <li><em>Individual message delay. For example, you have a job queue and need to schedule individual jobs with a delay. With Amazon SQS, you can configure individual messages to have a delay of up to 15 minutes.</em></li> <li><em>Dynamically increasing concurrency/throughput at read time. For example, you have a work queue and want to add more readers until the backlog is cleared. With Amazon Kinesis Data Streams, you can scale up to a sufficient number of shards (note, however, that you'll need to provision enough shards ahead of time).</em></li> <li><em>Leveraging Amazon SQS’s ability to scale transparently. For example, you buffer requests and the load changes as a result of occasional load spikes or the natural growth of your business. Because each buffered request can be processed independently, Amazon SQS can scale transparently to handle the load without any provisioning instructions from you.</em></li> </ul><p>Option A is wrong as SQS chaining would create dependency among the consumers. If one consumer fails, the message would not be available for the other consumer impacting the availability. </p><p>Option B is wrong as although possible is error prone and needs to maintain the position read by the application. </p><p>Option C is wrong as with DynamoDB delivery guarantee needs to handled by application as well is not a cost effective solution. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121219">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 38 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251388"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> Your website is serving on-demand training videos to your workforce. Videos are uploaded monthly in high resolution MP4 format. Your workforce is distributed globally often on the move and using company-provided tablets that require the HTTP Live Streaming (HLS) protocol to watch a video. Your company has no video transcoding expertise and it required you might need to pay for a consultant. How do you implement the most cost-efficient architecture without compromising high availability and quality of video delivery?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Elastic Transcoder to transcode original high-resolution MP4 videos to HLS. S3 to host videos with lifecycle Management to archive original flies to Glacier after a few days. CloudFront to serve HLS transcoded videos from S3</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;A video transcoding pipeline running on EC2 using SQS to distribute tasks and Auto Scaling to adjust the number or nodes depending on the length of the queue S3 to host videos with Lifecycle Management to archive all files to Glacier after a few days CloudFront to serve HLS transcoding videos from Glacier<br><b>C</b>. <input type="radio" disabled="">&nbsp;Elastic Transcoder to transcode original high-resolution MP4 videos to HLS EBS volumes to host videos and EBS snapshots to incrementally backup original rues after a few days. CloudFront to serve HLS transcoded videos from EC2.<br><b>D</b>. <input type="radio" disabled="">&nbsp;A video transcoding pipeline running on EC2 using SQS to distribute tasks and Auto Scaling to adjust the number of nodes depending on the length of the queue. EBS volumes to host videos and EBS snapshots to incrementally backup original files after a few days. CloudFront to serve HLS transcoded videos from EC2 <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Elastic Transcoder to transcode original high-resolution MP4 videos to HLS. S3 to host videos with lifecycle Management to archive original flies to Glacier after a few days. CloudFront to serve HLS transcoded videos from S3<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Key here the cost efficient solution with company needing video transcoding expertise and needing to hire a consultant with global distribution. </p><p>Correct answer is <strong>A</strong> as <a href="https://aws.amazon.com/elastictranscoder/" target="_blank">Elastic Transcoder</a> provides and out of box option to transcode videos into any format without any expertise. S3 to host videos and <a href="https://aws.amazon.com/cloudfront/streaming/" target="_blank">CloudFront to serve HLS transcoded videos</a> for global distribution while being cost efficient </p><p>Option B &amp; D are wrong as a video transcoding pipeline with instances would increase the cost needing expertise as well as infrastructure </p><p>Option C &amp; D are wrong as EBS volumes to host data with snapshots would increase the cost. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121270">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 39 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251389"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You need to create an Amazon Machine Learning model to predict how many inches of rain will fall in an area based on the historical rainfall data. What type of modeling will you use?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 3 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Categorical</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Binary<br><b>C</b>. <input type="radio" disabled="">&nbsp;Regression<br><b>D</b>. <input type="radio" disabled="">&nbsp;Unsupervised <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Regression<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong><span class="redactor-invisible-space"> as Supervised learning using Regression can help build a model to predict rain based on the historical data. </span> </p><p>Refer documentation - <a href="https://developers.google.com/machine-learning/problem-framing/cases">Machine Learning</a> </p><table class="blue"> <tbody> <tr> <th>Type of ML Problem </th> <th>Description </th> <th>Example </th> </tr> <tr> <td>Classification </td> <td>Pick one of N labels </td> <td>Cat, dog, horse, or bear </td> </tr> <tr> <td>Regression </td> <td>Predict numerical values </td> <td>Click-through rate </td> </tr> <tr> <td>Clustering </td> <td>Group similar examples </td> <td>Most relevant documents (unsupervised) </td> </tr> <tr> <td>Association rule learning </td> <td>Infer likely association patterns in data </td> <td>If you buy hamburger buns, you're likely to buy hamburgers (unsupervised) </td> </tr> <tr> <td>Structured output </td> <td>Create complex output </td> <td>Natural language parse trees, image recognition bounding boxes </td> </tr> <tr> <td>Ranking </td> <td>Identify position on a scale or status </td> <td>Search result ranking </td> </tr> </tbody> </table><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%202%20-%20%23125175">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 40 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251390"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company has launched EMR cluster to support their big data analytics requirements. AFS has multiple data sources built out of S3, SQL databases, MongoDB, Redis, RDS, other file systems. They are looking for a web application to create and share documents that contain live code, equations, visualizations, and narrative text. Which EMR Hadoop ecosystem fulfils the requirements?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 3 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <b>A</b>. <input type="radio" disabled="">&nbsp;Apache Hive<br><b>B</b>. <input type="radio" disabled="">&nbsp;Apache Hue<br><b>C</b>. <input type="radio" disabled="">&nbsp;Jupyter Notebook<br><b>D</b>. <input type="radio" disabled="">&nbsp;Apache Presto <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Jupyter Notebook<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-jupyter.html" target="_blank">Jupyter Notebook</a> is an open-source web application that you can use to create and share documents that contain live code, equations, visualizations, and narrative text </p><p>Option A is wrong as <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive.html" target="_blank">Hive</a> is an open-source, data warehouse, and analytic package that runs on top of a Hadoop cluster. </p><p>Option B is wrong as <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hue.html" target="_blank">Apache Hue</a> is an open-source, web-based, graphical user interface for use with Amazon EMR and Apache Hadoop. It does not provide live code and sharing of documents. </p><p>Option D is wrong as <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-presto.html" target="_blank">Presto</a> is a fast SQL query engine designed for interactive analytic queries over large datasets from multiple sources </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%202%20-%20%23125334">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 41 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251391"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company is building a new application in AWS. The architect needs to design a system to collect application log events. The design should be a repeatable pattern that minimizes data loss if an application instance fails, and keeps a durable copy of a log data for at least 30 days. What is the simplest architecture that will allow the architect to analyze the logs?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <b>A</b>. <input type="radio" disabled="">&nbsp;Write them directly to a Kinesis Firehose. Configure Kinesis Firehose to load the events into an Amazon Redshift cluster for analysis.<br><b>B</b>. <input type="radio" disabled="">&nbsp;Write them to a file on Amazon Simple Storage Service (S3). Write an AWS Lambda function that runs in response to the S3 event to load the events into Amazon Elasticsearch Service for analysis.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Write them to the local disk and configure the Amazon CloudWatch Logs agent to load the data into CloudWatch Logs and subsequently into Amazon Elasticsearch Service.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Write them to CloudWatch Logs and use an AWS Lambda function to load them into HDFS on an Amazon Elastic MapReduce (EMR) cluster for analysis. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Write them directly to a Kinesis Firehose. Configure Kinesis Firehose to load the events into an Amazon Redshift cluster for analysis.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as the simplest would be to use Firehose to stream data to collect the logs and load the data to Redshift for analysis. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/kinesis/data-firehose/" target="_blank">Kinesis Data Firehose</a> </p><p><em>Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.</em><br> </p><p>Option B is wrong the logs file cannot be written to S3 directly and would need an agent like CloudWatch for Kinesis. Also you would need to develop the Lambda function to track events and load data to Elasticsearch </p><p>Option C is wrong as local disk would not provide the durability and might lead to data loss if the instance goes down. </p><p><em></em> </p><p>Option D is wrong as the Lambda functions needs to be developed. Also, loading the data to HDFS using lambda might not seamless and would need AWS Data Pipeline. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121339">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 42 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251392"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A large oil and gas company needs to provide near real-time alerts when peak thresholds are exceeded in its pipeline system. The company has developed a system to capture pipeline metrics such as flow rate, pressure, and temperature using millions of sensors. The sensors deliver to AWS IoT. What is a cost-effective way to provide near real-time alerts on the pipeline metrics?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 3 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Create an AWS IoT rule to generate an Amazon SNS notification.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Store the data points in an Amazon DynamoDB table and poll if for peak metrics data from an Amazon EC2 application.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Create an Amazon Machine Learning model and invoke it with AWS Lambda.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use Amazon Kinesis Streams and a KCL-based application deployed on AWS Elastic Beanstalk. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Create an AWS IoT rule to generate an Amazon SNS notification.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as IoT rules can help evaluate and send notifications when the peak thresholds are exceeded. <em>The AWS IoT rules engine listens for incoming MQTT messages that match a rule. When a matching message is received, the rule takes some action with the data in the MQTT message (for example, writing data to an Amazon S3 bucket, invoking a Lambda function, or sending a message to an Amazon SNS topic).</em> </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/iot/latest/developerguide/iot-rules.html" target="_blank">IoT Rules</a> </p><p><em>Rules give your devices the ability to interact with AWS services. Rules are analyzed and actions are performed based on the MQTT topic stream. You can use rules to support tasks like these:</em> </p><ul> <li><em>Augment or filter data received from a device.</em></li> <li><em>Write data received from a device to an Amazon DynamoDB database.</em></li> <li><em>Save a file to Amazon S3.</em></li> <li><em>Send a push notification to all users using Amazon SNS.</em></li> <li><em>Publish data to an Amazon SQS queue.</em></li> <li><em>Invoke a Lambda function to extract data.</em></li> <li><em>Process messages from a large number of devices using Amazon Kinesis.</em></li> <li><em>Send data to the Amazon Elasticsearch Service.</em></li> <li><em>Capture a CloudWatch metric.</em></li> <li><em>Change a CloudWatch alarm.</em></li> <li><em>Send the data from an MQTT message to Amazon Machine Learning to make predictions based on an Amazon ML model.</em></li> <li><em>Send a message to a Salesforce IoT Input Stream.</em></li> <li><em>Send message data to an AWS IoT Analytics channel.</em></li> <li><em>Start execution of a Step Functions state machine.</em></li> <li><em>Send message data to an AWS IoT Events input.</em></li> </ul><p>Options B, C &amp; D are wrong as they are not cost-effective and need additional development or involve other services. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121370">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 43 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251393"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You need real-time reporting on logs generated from your applications. In addition, you need anomaly detection. The processing latency needs to be one second or less. Which option would you choose if your team has no experience with Machine learning libraries and doesn't want to have to maintain any software installations yourself?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 3 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Kinesis Streams with Kinesis Analytics</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Kafka<br><b>C</b>. <input type="radio" disabled="">&nbsp;Kinesis Firehose to S3 and Athena<br><b>D</b>. <input type="radio" disabled="">&nbsp;Spark Streaming with SparkSQL and MLlib <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Kinesis Streams with Kinesis Analytics<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as Kinesis Data Streams with Kinesis Data Analytics can provide real time analytics only data while using managed services </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/kinesis/data-analytics/" target="_blank">Kinesis Data Analytics</a> </p><p><img src="./bds2_files/writingsql_image1.gif"><br><em>Amazon Kinesis Data Analytics is the easiest way to analyze streaming data, gain actionable insights, and respond to your business and customer needs in real time. Amazon Kinesis Data Analytics reduces the complexity of building, managing, and integrating streaming applications with other AWS services. SQL users can easily query streaming data or build entire streaming applications using templates and an interactive SQL editor. Java developers can quickly build sophisticated streaming applications using open source Java libraries and AWS integrations to transform and analyze data in real-time.<br></em><em> </em> </p><p><em>Amazon Kinesis Data Analytics takes care of everything required to run your real-time applications continuously and scales automatically to match the volume and throughput of your incoming data.</em> </p><p>Option B is wrong as Kafka needs to be managed and does not provide analytical capability </p><p>Option C is wrong as Athena does not work on the streaming analytics. It is more for batch analytics </p><p>Option D is wrong as Spark Streaming and MLlib would need EMR cluster management and development. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23124750">AWS BDS-C00 Question feedback</a></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 44 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251394"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A city has been collecting data on its public bicycle share program for the past three years. The 5PB dataset currently resides on Amazon S3. The data contains the following datapoints: Bicycle origination points Bicycle destination points Mileage between the points Number of bicycle slots available at the station (which is variable based on the station location) Number of slots available and taken at a given time The program has received additional funds to increase the number of bicycle stations available. All data is regularly archived to Amazon Glacier. The new bicycle stations must be located to provide the most riders access to bicycles. How should this task be performed? <br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 7 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Move the data from Amazon S3 into Amazon EBS-backed volumes and use an EC2 based Hadoop cluster with spot instances to run a Spark job that performs a stochastic gradient descent optimization.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use the Amazon Redshift COPY command to move the data from Amazon S3 into Redshift and perform a SQL query that outputs the most popular bicycle stations.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Persist the data on Amazon S3 and use a transient EMR cluster with spot instances to run a Spark streaming job that will move the data into Amazon Kinesis.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Keep the data on Amazon S3 and use an Amazon EMR-based Hadoop cluster with spot instances to run a Spark job that performs a stochastic gradient descent optimization over EMRFS. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Keep the data on Amazon S3 and use an Amazon EMR-based Hadoop cluster with spot instances to run a Spark job that performs a stochastic gradient descent optimization over EMRFS.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as the data is already hosted in S3, EMR with EMRFS can be used to perform analysis. </p><p>Option A is wrong EBS backed volumes cannot handled the data capacity </p><p>Option B is wrong as copying the data in Redshift would duplicate it and current size limitation of Redshift is 2PB. </p><p>Option C is wrong as the answer is incomplete as it does not provide any analysis option. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121404">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 45 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251395"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A large grocery distributor receives daily depletion reports from the field in the form of gzip archives of CSV files uploaded to Amazon S3. The files range from 500MB to 5GB. These files are processed daily by an EMR job. Recently it has been observed that the file sizes vary, and the EMR jobs take too long. The distributor needs to tune and optimize the data processing workflow with this limited information to improve the performance of the EMR job. Which recommendation should an administrator provide?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Reduce the HDFS block size to increase the number of task processors.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use bzip2 or Snappy rather than gzip for the archives.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Decompress the gzip archives and store the data as CSV files.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use Avro rather than gzip for the archives. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Use bzip2 or Snappy rather than gzip for the archives.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as gzip is not ideal compression for files larger than 1GB and compression technique should be checked with supports splitting like bzip2 or one with higher compression handling like Snappy. </p><p>Refer AWS documentation - <a href="https://d0.awsstatic.com/whitepapers/aws-amazon-emr-best-practices.pdf" target="_blank">EMR Best Practices</a> </p><p><em>Depending on how large your aggregated data files are, the compression algorithm becomes an important choice. For instance, if you are aggregating your data (using the ingest tool of your choice) and the aggregated data files are <span class="redactor-invisible-space"> between 500 MB to 1 GB, GZIP compression is an acceptable data compression type. However, if your data aggregation creates files larger than 1 GB, its best to pick a compression algorithm that supports splitting. </span></em><br> </p><p><strong><em>What Compression Algorithm Should I Use? </em></strong> </p><p><em>Naturally, not all compression algorithms are alike. Consider these potential advantages and disadvantages: </em> </p><p><em> As the table below suggests, some compression algorithms are faster. You need to understand your workload in order to decide if faster compressions are any use for you. For example, if your job is CPU bounded, faster compression algorithms may not give you enough performance improvement. If you decide compression speed is important, Snappy compression seems to perform faster. </em> </p><p><em> Some compressions algorithms are slower but offer better space savings, which may be important to you. However, if storage cost is not an important factor, you may want a faster algorithm instead. </em> </p><p><em><span class="redactor-invisible-space"></span></em> </p><p><em> Importantly, some algorithms allow file output to be split. As discussed earlier, the ability to split your data file affects how you store your data files. If the compression algorithm does not support splitting, you may have to maintain smaller file sizes. However, if your compressed files can be chunked, you may want to store large files for Amazon EMR processing. </em> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121414">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 46 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251396"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An administrator is deploying Spark on Amazon EMR for two distinct use cases: machine learning algorithms and ad-hoc querying. All data will be stored in Amazon S3. Two separate clusters for each use case will be deployed. The data volumes on Amazon S3 are less than 10 GB. How should the administrator align instance types with the cluster’s purpose?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Machine Learning on C instance types and ad-hoc queries on R instance types</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Machine Learning on R instance types and ad-hoc queries on G2 instance types<br><b>C</b>. <input type="radio" disabled="">&nbsp;Machine Learning on T instance types and ad-hoc queries on M instance types<br><b>D</b>. <input type="radio" disabled="">&nbsp;Machine Learning on D instance types and ad-hoc queries on I instance types <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Machine Learning on C instance types and ad-hoc queries on R instance types<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as machine learning are usually compute intensive and adhoc queries are suited for memory optimized. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/" target="_blank">EMR Best Practices</a> </p><p><em>For memory-intensive applications, prefer R type instances over the other instance types. For compute-intensive applications, prefer C type instances. For applications balanced between memory and compute, prefer M type general-purpose instances.</em><em></em><br> </p><p><em>Compute Optimized - High performance web servers, scientific modelling, batch processing, distributed analytics, high-performance computing (HPC), machine/deep learning inference, ad serving, highly scalable multiplayer gaming, and video encoding.<br></em> </p><p><em>Memory Optimized - Instances are well suited for memory intensive applications such as high performance databases, distributed web scale in-memory caches, mid-size in-memory databases, real time big data analytics, and other enterprise applications.</em> </p><p>Option B is wrong as G2 is GPU powered and GPU-powered G2 instance family is home to molecular modeling, rendering, machine learning, game streaming, and transcoding jobs that require massive amounts of parallel processing power. </p><p>Option C is wrong as T are ideally suited for general purpose applications.<br> </p><p>Option D is wrong as I &amp; D are storage optimized instances. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121406">AWS BDS-C00 Question feedback</a></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 47 of 50 </b><small>- Multiple Answer</small> <span style="float:right;" class="grad_5251397"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A customer's nightly EMR job processes a single 2-TB data file stored on Amazon Simple Storage Service (S3). The Amazon Elastic Map Reduce (EMR) job runs on two On-Demand core nodes and three On-Demand task nodes. Which of the following may help reduce the EMbR job completion time? Choose 2 answers<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <b>A</b>. <input type="checkbox" disabled="">&nbsp;Use three Spot Instances rather than three On-Demand instances for the task nodes.<br><b>B</b>. <input type="checkbox" disabled="">&nbsp;Change the input split size in the MapReduce job configuration.<br><b>C</b>. <input type="checkbox" disabled="">&nbsp;Use a bootstrap action to present the S3 bucket as a local filesystem.<br><b>D</b>. <input type="checkbox" disabled="">&nbsp;Launch the core nodes and task nodes within an Amazon Virtual Cloud.<br><b>E</b>. <input type="checkbox" disabled="">&nbsp;Adjust the number of simultaneous mapper tasks.<br><b>F</b>. <input type="checkbox" disabled="">&nbsp;Enable termination protection for the job flow. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Change the input split size in the MapReduce job configuration.<br><b>E</b>. Adjust the number of simultaneous mapper tasks.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B &amp; E</strong> as the key point here is to reduce job completion time. </p><p>Option B as the split size of the match in memory block size of task and HDFS files will help to complete the job faster. </p><p>Option E as adjusting and tuning the number of simultaneous mapper task would help reduce time </p><p>Refer to <a href="https://d0.awsstatic.com/whitepapers/aws-amazon-emr-best-practices.pdf" target="_blank">EMR Best Practices</a> </p><p>Option A is wrong as Spot instances would help reduce cost but might increase the job completion time </p><p>Option C is wrong as it would not help as the data is already there in the data nodes. </p><p>Option D is wrong as the instances would be in VPC already and would not improve job times </p><p>Option F is wrong as termination protection would not help as the instances are not being terminated adhoc </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121252">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 48 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251398"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A travel website needs to present a graphical quantitative summary of its daily bookings to website visitors for marketing purposes. The website has millions of visitors per day, but wants to control costs by implementing the least-expensive solution for this visualization. What is the most cost-effective solution?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Generate a static graph with a transient EMR cluster daily, and store it an Amazon S3.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Generate a graph using MicroStrategy backed by a transient EMR cluster.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Implement a Jupyter front-end provided by a continuously running EMR cluster leveraging spot instances for task nodes.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Implement a Zeppelin application that runs on a long-running EMR cluster. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Generate a static graph with a transient EMR cluster daily, and store it an Amazon S3.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as the most cost effective solution is to use a transient cluster to create the stats and use S3 to host the same. </p><p>Option B is wrong as using <a href="https://aws.amazon.com/marketplace/pp/B00JK1ZFLE" target="_blank">MicroStrategy</a> is an overhead and its a marketplace product, which would not be a cost effective solution. </p><p>Options C &amp; D are wrong as using a long running or continuous cluster is not cost effective </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121395">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 49 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251399"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You need to perform ad-hoc business analytics queries on well-structured data. Data comes in constantly at a high velocity. Your business intelligence team can understand SQL. What AWS service(s) should you look to first?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Kinesis Firehose + RDS</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Kinesis Firehose + Redshift<br><b>C</b>. <input type="radio" disabled="">&nbsp;EMR using Hive<br><b>D</b>. <input type="radio" disabled="">&nbsp;EMR running Apache Spark <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Kinesis Firehose + Redshift<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Key point is perform ad-hoc analytics with data at high velocity </p><p>Correct answer is B as <a href="https://aws.amazon.com/kinesis/firehose/details/" target="_blank">Kinesis Firehose</a> provides a managed service for aggregating streaming data and inserting it into RedShift. RedShift also supports ad-hoc queries over well-structured data using a SQL-compliant wire protocol, so the business team should be able to adopt this system easily.</p><p>Option A is wrong as RDS would not be suitable for ad-hoc analytics </p><p>Option C &amp; D are wrong as EMR does not itself handle data at high velocity. Need Kafka or similar frameworks </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121216">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 50 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251400"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A game company needs to properly scale its game application, which is backed by DynamoDB. Amazon Redshift has the past two years of historical data. Game traffic varies throughout the year based on various factors such as season, movie release, and holiday season. An administrator needs to calculate how much read and write throughput should be provisioned for DynamoDB table for each week in advance. How should the administrator accomplish this task?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Feed the data into Amazon Machine Learning and build a regression model.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Feed the data into Spark Mlib and build a random forest model.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Feed the data into Apache Mahout and build a multi-classification model.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Feed the data into Amazon Machine Learning and build a binary classification model. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Feed the data into Amazon Machine Learning and build a regression model.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as Regression model can help predict or forecast data based on the earlier dataset. </p><p><em>Regression predictive modeling is the task of approximating a mapping function (f) from input variables (X) to a continuous output variable (y).</em><br> </p><p>Option B is wrong as Random forest is not needed and Spark MLib needs to have EMR to run, while Amazon Machine Learning is quick. </p><p>Options C &amp; D are wrong as they are only or classification and do not forecast values. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121365">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="col-sm-12 well"><h4 class="sm" style="color:#333;">23/50 Questions right</h4></div></div><style type="text/css">
span.label.label-danger,span.label.label-success {
    padding: .2em .6em .3em;
}
.saved{color:red; }
.question_info{
  margin-bottom: 40px;
  border: solid 1px #ccc;
  border-radius: 5px;
  overflow: hidden;
}
.question_no {
    background: #f4f4f4;
    padding: 0 15px;
    line-height: 40px;
    border-bottom: solid 1px #ccc;
}

.question_detail {
    padding: 10px;
}
.hide{
  display: none;
}
input[type="radio"]{
  -webkit-appearance: radio;
}
input[type="checkbox"]{
  -webkit-appearance: checkbox;
}
span.bgcolor {
    background: yellow;
    padding: 5px;
    margin-left: -5px;
}
</style><link href="./bds2_files/mcoursestyle.css" rel="stylesheet"></div> <script type="text/javascript" src="./bds2_files/bc-course.min_031117.js.下载"></script> <div class="overlayForm" style=""></div></div></div></div></div></div><div class="overlayForm"></div></div><iframe style="position:absolute;left:-999px;top:-999px;visibility:hidden" src="./bds2_files/saved_resource.html"></iframe><iframe style="display: none; visibility: hidden;" src="./bds2_files/saved_resource(1).html"></iframe><iframe style="display: none; visibility: hidden;" src="./bds2_files/saved_resource(2).html"></iframe></body></html>