<!DOCTYPE html>
<!-- saved from url=(0105)https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-3/145262 -->
<html xmlns:fb="http://ogp.me/ns/fb#" lang="en-gb" dir="ltr" class="secondary-14px wf-proximanova-n7-active wf-proximanova-i7-active wf-proximanova-n4-active wf-raleway-n1-active wf-raleway-n7-active wf-raleway-n4-active wf-raleway-n5-active wf-raleway-n3-active wf-raleway-n8-active wf-raleway-n9-active wf-raleway-n2-active wf-raleway-n6-active wf-proximanova-i4-active wf-active"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><meta name="format-detection" content="telephone=no"><script type="text/javascript" src="./bds3_files/display.js.下载"></script><script type="text/javascript" src="./bds3_files/pro"></script><script type="text/javascript" src="./bds3_files/l.js.下载"></script><script type="text/javascript" src="./bds3_files/l.js(1).下载"></script><script type="text/javascript" src="./bds3_files/l.js(2).下载"></script><script type="text/javascript" src="./bds3_files/l.js(3).下载"></script><script type="text/javascript" async="" src="./bds3_files/atatus.js.下载"></script><script type="text/javascript" async="" src="./bds3_files/analytics.js.下载"></script><script src="./bds3_files/fMy5LNtdDqis6adCpEbCXQHA47I.js.下载"></script><script src="./bds3_files/js"></script><link rel="canonical" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-3/145262"><link rel="stylesheet" type="text/css" href="./bds3_files/bc-course.min_092917.css"><link rel="stylesheet" type="text/css" href="./bds3_files/bc-style-092917.css"><!--[if lt IE 9]> <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script> <script src="//css3-mediaqueries-js.googlecode.com/svn/trunk/css3-mediaqueries.js"></script> <![endif]--><link rel="shortcut icon" href="https://www.braincert.com/images/favicon.ico"> <script type="text/javascript" src="./bds3_files/jquery-1.11.0.min.js.下载"></script> <script type="text/javascript">jQuery.noConflict();</script> <script type="application/javascript" src="./bds3_files/fVBYAHUg.js.下载"></script><script type="text/javascript" src="./bds3_files/ga.js.下载"></script> <script type="text/javascript">jwplayer.key="Kfk7MAHVl4Y33jPduQlHwUdmLu+1l6cvPHVklw==";</script> <script src="./bds3_files/jdk4nqa.js.下载"></script> <style type="text/css">.tk-proxima-nova{font-family:"proxima-nova",sans-serif;}.tk-raleway{font-family:"raleway",sans-serif;}</style><style type="text/css">@font-face{font-family:tk-proxima-nova-n7;src:url(https://use.typekit.net/af/925423/00000000000000003b9b038f/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff2"),url(https://use.typekit.net/af/925423/00000000000000003b9b038f/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff"),url(https://use.typekit.net/af/925423/00000000000000003b9b038f/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("opentype");font-weight:700;font-style:normal;}@font-face{font-family:tk-proxima-nova-i7;src:url(https://use.typekit.net/af/cd78b3/00000000000000003b9b038e/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i7&v=3) format("woff2"),url(https://use.typekit.net/af/cd78b3/00000000000000003b9b038e/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i7&v=3) format("woff"),url(https://use.typekit.net/af/cd78b3/00000000000000003b9b038e/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i7&v=3) format("opentype");font-weight:700;font-style:italic;}@font-face{font-family:tk-proxima-nova-n4;src:url(https://use.typekit.net/af/219c30/00000000000000003b9b0389/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff2"),url(https://use.typekit.net/af/219c30/00000000000000003b9b0389/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff"),url(https://use.typekit.net/af/219c30/00000000000000003b9b0389/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("opentype");font-weight:400;font-style:normal;}@font-face{font-family:tk-proxima-nova-i4;src:url(https://use.typekit.net/af/0de7d4/00000000000000003b9b0388/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i4&v=3) format("woff2"),url(https://use.typekit.net/af/0de7d4/00000000000000003b9b0388/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i4&v=3) format("woff"),url(https://use.typekit.net/af/0de7d4/00000000000000003b9b0388/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i4&v=3) format("opentype");font-weight:400;font-style:italic;}@font-face{font-family:tk-raleway-n1;src:url(https://use.typekit.net/af/4c4265/00000000000000000001328e/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n1&v=3) format("woff2"),url(https://use.typekit.net/af/4c4265/00000000000000000001328e/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n1&v=3) format("woff"),url(https://use.typekit.net/af/4c4265/00000000000000000001328e/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n1&v=3) format("opentype");font-weight:100;font-style:normal;}@font-face{font-family:tk-raleway-n7;src:url(https://use.typekit.net/af/00d57c/000000000000000000013287/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff2"),url(https://use.typekit.net/af/00d57c/000000000000000000013287/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff"),url(https://use.typekit.net/af/00d57c/000000000000000000013287/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("opentype");font-weight:700;font-style:normal;}@font-face{font-family:tk-raleway-n4;src:url(https://use.typekit.net/af/3c6666/000000000000000000013288/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff2"),url(https://use.typekit.net/af/3c6666/000000000000000000013288/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff"),url(https://use.typekit.net/af/3c6666/000000000000000000013288/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("opentype");font-weight:400;font-style:normal;}@font-face{font-family:tk-raleway-n5;src:url(https://use.typekit.net/af/145edc/000000000000000000013289/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n5&v=3) format("woff2"),url(https://use.typekit.net/af/145edc/000000000000000000013289/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n5&v=3) format("woff"),url(https://use.typekit.net/af/145edc/000000000000000000013289/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n5&v=3) format("opentype");font-weight:500;font-style:normal;}@font-face{font-family:tk-raleway-n3;src:url(https://use.typekit.net/af/9a0c16/00000000000000000001328a/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n3&v=3) format("woff2"),url(https://use.typekit.net/af/9a0c16/00000000000000000001328a/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n3&v=3) format("woff"),url(https://use.typekit.net/af/9a0c16/00000000000000000001328a/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n3&v=3) format("opentype");font-weight:300;font-style:normal;}@font-face{font-family:tk-raleway-n8;src:url(https://use.typekit.net/af/62d84a/00000000000000000001328b/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n8&v=3) format("woff2"),url(https://use.typekit.net/af/62d84a/00000000000000000001328b/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n8&v=3) format("woff"),url(https://use.typekit.net/af/62d84a/00000000000000000001328b/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n8&v=3) format("opentype");font-weight:800;font-style:normal;}@font-face{font-family:tk-raleway-n9;src:url(https://use.typekit.net/af/f4139f/00000000000000000001328c/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n9&v=3) format("woff2"),url(https://use.typekit.net/af/f4139f/00000000000000000001328c/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n9&v=3) format("woff"),url(https://use.typekit.net/af/f4139f/00000000000000000001328c/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n9&v=3) format("opentype");font-weight:900;font-style:normal;}@font-face{font-family:tk-raleway-n2;src:url(https://use.typekit.net/af/6b6454/00000000000000000001328d/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n2&v=3) format("woff2"),url(https://use.typekit.net/af/6b6454/00000000000000000001328d/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n2&v=3) format("woff"),url(https://use.typekit.net/af/6b6454/00000000000000000001328d/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n2&v=3) format("opentype");font-weight:200;font-style:normal;}@font-face{font-family:tk-raleway-n6;src:url(https://use.typekit.net/af/baea6e/000000000000000000014968/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n6&v=3) format("woff2"),url(https://use.typekit.net/af/baea6e/000000000000000000014968/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n6&v=3) format("woff"),url(https://use.typekit.net/af/baea6e/000000000000000000014968/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n6&v=3) format("opentype");font-weight:600;font-style:normal;}</style><script>try{Typekit.load({ async: true });}catch(e){}</script> <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="keywords" content="virtual classroom, online test, online course, MOOC,  SCORM, whiteboard, adaptive testing, e-learning, online education, learn online, teach online, live class, lms, monetize, sell course, online meetings, collaboration, webinar, how to, social, teach, learn"><meta name="description" content="Deliver live engaging classes using Virtual Classroom. Create and sell courses and tests online."><title>Review Answers | BrainCert</title><link href="https://www.braincert.com/templates/yoo_nano/favicon.ico" rel="shortcut icon" type="image/vnd.microsoft.icon"> <script type="text/javascript">
function keepAlive() {	var myAjax = new Request({method: "get", url: "index.php"}).send();} window.addEvent("domready", function(){ keepAlive.periodical(3540000); });
  </script> <script type="text/javascript">
				/*<![CDATA[*/
					var jax_live_site = 'https://www.braincert.com/index.php';
					var jax_token_var='925395911814f17127e11ea28577607f';
				/*]]>*/
				</script><script type="text/javascript" src="./bds3_files/ajax_1.5.pack.js.下载"></script> <link rel="apple-touch-icon-precomposed" href="https://d9q55ve2f7k8m.cloudfront.net/images/apple_touch_icon.png"> <script>
        !function(window, document) {
            window._atatusConfig = {
                apikey: '8c2f3d535648489b9826fd95a6484c2b'
            };
            function _asyncAtatus(callback) {
                var script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.src = "https://dmc1acwvwny3.cloudfront.net/atatus.js";
                var node = document.getElementsByTagName("script")[0];
                script.addEventListener('load', function (e) {
                    callback(null, e);
                }, false);
                node.parentNode.insertBefore(script, node);
            }
            _asyncAtatus(function() {
                // Any atatus related calls.
                if (window.atatus) {
                    window.atatus.setUser('138600', 'liugongjianxin@163.com', 'gongjian liu');
                    console.log(window.atatus);
                }
            });
        }(window, document);
</script><style type="text/css">@font-face{font-family:proxima-nova;src:url(https://use.typekit.net/af/925423/00000000000000003b9b038f/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff2"),url(https://use.typekit.net/af/925423/00000000000000003b9b038f/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff"),url(https://use.typekit.net/af/925423/00000000000000003b9b038f/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("opentype");font-weight:700;font-style:normal;}@font-face{font-family:proxima-nova;src:url(https://use.typekit.net/af/cd78b3/00000000000000003b9b038e/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i7&v=3) format("woff2"),url(https://use.typekit.net/af/cd78b3/00000000000000003b9b038e/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i7&v=3) format("woff"),url(https://use.typekit.net/af/cd78b3/00000000000000003b9b038e/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i7&v=3) format("opentype");font-weight:700;font-style:italic;}@font-face{font-family:proxima-nova;src:url(https://use.typekit.net/af/219c30/00000000000000003b9b0389/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff2"),url(https://use.typekit.net/af/219c30/00000000000000003b9b0389/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff"),url(https://use.typekit.net/af/219c30/00000000000000003b9b0389/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("opentype");font-weight:400;font-style:normal;}@font-face{font-family:proxima-nova;src:url(https://use.typekit.net/af/0de7d4/00000000000000003b9b0388/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i4&v=3) format("woff2"),url(https://use.typekit.net/af/0de7d4/00000000000000003b9b0388/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i4&v=3) format("woff"),url(https://use.typekit.net/af/0de7d4/00000000000000003b9b0388/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=i4&v=3) format("opentype");font-weight:400;font-style:italic;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/4c4265/00000000000000000001328e/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n1&v=3) format("woff2"),url(https://use.typekit.net/af/4c4265/00000000000000000001328e/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n1&v=3) format("woff"),url(https://use.typekit.net/af/4c4265/00000000000000000001328e/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n1&v=3) format("opentype");font-weight:100;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/00d57c/000000000000000000013287/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff2"),url(https://use.typekit.net/af/00d57c/000000000000000000013287/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("woff"),url(https://use.typekit.net/af/00d57c/000000000000000000013287/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n7&v=3) format("opentype");font-weight:700;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/3c6666/000000000000000000013288/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff2"),url(https://use.typekit.net/af/3c6666/000000000000000000013288/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("woff"),url(https://use.typekit.net/af/3c6666/000000000000000000013288/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n4&v=3) format("opentype");font-weight:400;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/145edc/000000000000000000013289/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n5&v=3) format("woff2"),url(https://use.typekit.net/af/145edc/000000000000000000013289/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n5&v=3) format("woff"),url(https://use.typekit.net/af/145edc/000000000000000000013289/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n5&v=3) format("opentype");font-weight:500;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/9a0c16/00000000000000000001328a/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n3&v=3) format("woff2"),url(https://use.typekit.net/af/9a0c16/00000000000000000001328a/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n3&v=3) format("woff"),url(https://use.typekit.net/af/9a0c16/00000000000000000001328a/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n3&v=3) format("opentype");font-weight:300;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/62d84a/00000000000000000001328b/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n8&v=3) format("woff2"),url(https://use.typekit.net/af/62d84a/00000000000000000001328b/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n8&v=3) format("woff"),url(https://use.typekit.net/af/62d84a/00000000000000000001328b/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n8&v=3) format("opentype");font-weight:800;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/f4139f/00000000000000000001328c/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n9&v=3) format("woff2"),url(https://use.typekit.net/af/f4139f/00000000000000000001328c/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n9&v=3) format("woff"),url(https://use.typekit.net/af/f4139f/00000000000000000001328c/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n9&v=3) format("opentype");font-weight:900;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/6b6454/00000000000000000001328d/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n2&v=3) format("woff2"),url(https://use.typekit.net/af/6b6454/00000000000000000001328d/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n2&v=3) format("woff"),url(https://use.typekit.net/af/6b6454/00000000000000000001328d/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n2&v=3) format("opentype");font-weight:200;font-style:normal;}@font-face{font-family:raleway;src:url(https://use.typekit.net/af/baea6e/000000000000000000014968/27/l?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n6&v=3) format("woff2"),url(https://use.typekit.net/af/baea6e/000000000000000000014968/27/d?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n6&v=3) format("woff"),url(https://use.typekit.net/af/baea6e/000000000000000000014968/27/a?primer=7cdcb44be4a7db8877ffa5c0007b8dd865b3bbc383831fe2ea177f62257a9191&fvd=n6&v=3) format("opentype");font-weight:600;font-style:normal;}</style> <script async="" type="text/javascript" src="./bds3_files/pops"></script><script type="text/javascript" src="./bds3_files/jquery.min.js.下载"></script></head><body id="page-top"><div id="page-wrap"><div id="preloader"> </div> <header id="header" class="header chapter-header"><div class="container"><div class="logo"><a href="https://www.braincert.com/"><img src="./bds3_files/bc-logo-sm.png" alt="BrainCert" style="max-height:60px;"></a> </div><nav class="navigation"><div class="navbar-header"> <a class="navbar-brand" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-3/145262#"><span class="navbar-header-title"> AWS Certified Big Data - Specialty BDS-C00 Practice Exam 3 </span></a> </div><ul class="menu"> <li><a href="https://www.braincert.com/">Home</a></li> </ul><div class="search-box"> <a href="https://www.braincert.com/test/13670-AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-3" class="smoothScroll"><i class="fa fa-chevron-left"></i> Back</a> </div></nav> </div> </header><div class="main-container"> <script type="text/javascript">
  jQuery(document).ready(function (){
     
    jQuery( "html" ).addClass( "secondary-14px" );
    jQuery(document)[0].oncontextmenu = function() {return false;} 
    // code for preventing copy from keyboard
    var ambit = jQuery(document);
    // Disable Cut + Copy + Paste (input)
    ambit.on('copy paste cut', function (e) {
    e.preventDefault(); //disable cut,copy,paste
      return false;
    });
      });
</script> <div class="container"><div id="content-main" class="row-fluid"><div class="col-sm-12"><h2 style="margin-bottom:10px;margin-top:10px; float:left">Test Report</h2><div style="margin-top:10px;float:right"><a onclick="window.history.back();" class="btn btn-warning"><span><strong>Back</strong></span></a></div><div style="float:right;margin-top:10px;margin-right: 10px;"><a href="https://www.braincert.com/test/reviewtest/exportdata/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-3/145262" class="btn btn-primary"><span><strong><i class="fa fa-share-square-o"></i>&nbsp;Export to .CSV</strong></span></a></div><div style="border-bottom-width: 1px;border-bottom-style: dashed;border-bottom-color: #e0e0e0;margin-bottom: 10px; clear:both"></div><div style="clear:both;"></div><div class="col-md-6 pull-left row"><h3 style="margin-top: 0px;"><strong>Review questions</strong></h3></div><div class="col-md-6 pull-right row" style="font-size: 16px;text-align: right;"><strong>Student : </strong>gongjian liu
<br> <i class="fa fa-calendar"></i>&nbsp;Jun 17, 2019&nbsp;&nbsp;<i class="fa fa-clock-o"></i>&nbsp;05:27AM EDT<br> <br> </div><div id="test_results" style="padding-top: 80px;"><div id="quiz_specific"> <span id="select"></span> <div class="quiz_attempt_breakdown"><div class="percent_correct_bar col-sm-2"><div class="progress" style="margin-bottom: 2px;"><div class="progress-bar" role="progressbar" aria-valuenow="70" aria-valuemin="0" aria-valuemax="100" style="width:18%"> </div> </div> <span id="percent"><strong>18% </strong>correct</span> </div><div><div class="questions_correct col-sm-2"><span class="inline_pipe">|</span>&nbsp;&nbsp; <img src="./bds3_files/tick.webp" alt="you got this question right">&nbsp;<strong>9 correct</strong></div><div class="questions_incorrect col-sm-2"><span class="inline_pipe"> | </span>&nbsp;&nbsp; <img src="./bds3_files/cross.webp" alt="you got this question wrong">&nbsp;<strong>41 incorrect</strong></div><div class="questions_incorrect col-sm-3" style="margin:0;"><span class="inline_pipe"> | </span>&nbsp;&nbsp;<img src="./bds3_files/icon_un_answered.webp" alt="you got this question unanswer">&nbsp;<strong>0 Unanswered</strong></div><div class="total_questions col-sm-3"><span class="inline_pipe"> | </span>&nbsp;&nbsp;<strong>50 questions attempted out of 50</strong></div></div></div><br class="clear"><hr style="clear:both"><br> <br><div style="margin-bottom: 10px;"> <b style="font-size: 14.5px;">Filter by</b> : <a style="padding: 3px 5px 3px 5px;" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-3/145262" class="label-default label">All</a>&nbsp;|&nbsp; <a style="padding: 3px 5px 3px 5px;" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-3/145262?sort=1">correct</a>&nbsp;|&nbsp; <a style="padding: 3px 5px 3px 5px;" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-3/145262?sort=0">incorrect</a>&nbsp;|&nbsp; <a style="padding: 3px 5px 3px 5px;" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-3/145262?sort=-1">Unanswered</a>&nbsp;|&nbsp; <a style="padding: 3px 5px 3px 5px;" href="https://www.braincert.com/test/reviewtest/AWS-Certified-Big-Data-Specialty-BDS-C00-Practice-Exam-3/145262?sort=2">Question feedback</a> </div><br><div class="" style="font-size: 18px;line-height: 25px;"><div class="question_info"><div class="question_no"><b>Question : 1 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251466"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An administrator receives about 100 files per hour into Amazon S3 and will be loading the files into Amazon Redshift. Customers who analyze the data within Redshift gain significant value when they receive data as quickly as possible. The customers have agreed to a maximum loading interval of 5 minutes. Which loading approach should the administrator use to meet this objective?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 15 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Load each file as it arrives because getting data into the cluster as quickly as possibly is the priority.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Load the cluster as soon as the administrator has the same number of files as nodes in the cluster.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Load the cluster when the administrator has the number of files as multiple of files relative to Cluster Slice Count, or 5 minutes, whichever comes first.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Load the cluster when the number of files is less than the Cluster Slice Count. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Load the cluster when the administrator has the number of files as multiple of files relative to Cluster Slice Count, or 5 minutes, whichever comes first.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as the maximum lag allowed is 5 minutes, the target should to load the files once they reach multiple of the slice count of 5 minutes whichever comes first. Loading the files in multiple of slice count provides best performance. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/blogs/big-data/top-8-best-practices-for-high-performance-etl-processing-using-amazon-redshift/" target="_blank">Redshift Best Practices</a> </p><p><em>Amazon Redshift is an MPP (massively parallel processing) database, where all the compute nodes divide and parallelize the work of ingesting data. Each node is further subdivided into slices, with each slice having one or more dedicated cores, equally dividing the processing capacity. The number of <a href="http://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html" target="_blank" rel="noopener noreferrer">slices per node</a> depends on the node type of the cluster. For example, each DS2.XLARGE compute node has two slices, whereas each DS2.8XLARGE compute node has 16 slices.<br></em> </p><p><em>When you load data into Amazon Redshift, you should aim to have each slice do an equal amount of work. When you load the data from a single large file or from files split into uneven sizes, some slices do more work than others. As a result, the process runs only as fast as the slowest, or most heavily loaded, slice.<br></em> </p><p><em>When splitting your data files, ensure that they are of approximately equal size – between 1 MB and 1 GB after compression. The number of files should be a multiple of the number of slices in your cluster. Also, I strongly recommend that you individually compress the load files using gzip, lzop, or bzip2 to efficiently load large datasets.</em> </p><p><em>When loading multiple files into a single table, use a single COPY command for the table, rather than multiple COPY commands. Amazon Redshift automatically parallelizes the data ingestion. Using a single COPY command to bulk load data into a table ensures optimal use of cluster resources, and quickest possible throughput.</em> </p><p>Option A is wrong as files should be loaded in bulk. </p><p>Options B &amp; D are wrong as the best practice to load the files is to match the slice count.<br> </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121400">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 2 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251467"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company hosts a portfolio of e-commerce websites across the Oregon, N. Virginia, Ireland, and Sydney AWS regions. Each site keeps log files that capture user behavior. The company has built an application that generates batches of product recommendations with collaborative filtering in Oregon. Oregon was selected because the flagship site is hosted there and provides the largest collection of data to train machine learning models against. The other regions do NOT have enough historic data to train accurate machine learning models. Which set of data processing steps improves recommendations for each region?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <b>A</b>. <input type="radio" disabled="">&nbsp;Use the e-commerce application in Oregon to write replica log files in each other region.<br><b>B</b>. <input type="radio" disabled="">&nbsp;Use Amazon S3 bucket replication to consolidate log entries and build a single model in Oregon.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Use Kinesis as a buffer for web logs and replicate logs to the Kinesis stream of a neighboring region.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use the CloudWatch Logs agent to consolidate logs into a single CloudWatch Logs group. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Use the CloudWatch Logs agent to consolidate logs into a single CloudWatch Logs group.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as the resources can point to a CloudWatch logs destination to consolidate the logs using Subscription filters. </p><p>Refer documentation - <a href="https://blend.com/blog/community/engineering/centralizing-logs-aws/" target="_blank">Article</a> and <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html" target="_blank">CrossAccountSubscribtions</a> </p><p><img src="./bds3_files/121410.png"><br> </p><p><em>You can collaborate with an owner of a different AWS account and receive their log events on your AWS resources, such as a Amazon Kinesis stream (this is known as cross-account data sharing). For example, this log event data can be read from a centralized Amazon Kinesis stream to perform custom processing and analysis. Custom processing is especially useful when you collaborate and analyze data across many accounts. For example, a company's information security group might want to analyze data for real-time intrusion detection or anomalous behaviors so it could conduct an audit of accounts in all divisions in the company by collecting their federated production logs for central processing. A real-time stream of event data across those accounts can be assembled and delivered to the information security groups who can use Kinesis to attach the data to their existing security analytic systems.</em><br> </p><p>Option A is wrong as it would put the onus on application </p><p>Option B is wrong as it should work with S3 cross region replication but would duplicate the data. </p><p>Option C is wrong as Kinesis cannot be used to replicate logs across region and it would need custom client applications or Lambda. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121410">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 3 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251468"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A media advertising company handles a large number of real-time messages sourced from over 200 websites. The company’s data engineer needs to collect and process records in real time for analysis using Spark Streaming on Amazon Elastic MapReduce (EMR). The data engineer needs to fulfill a corporate mandate to keep ALL raw messages as they are received as a top priority. Which Amazon Kinesis configuration meets these requirements?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <b>A</b>. <input type="radio" disabled="">&nbsp;Publish messages to Amazon Kinesis Firehose backed by Amazon Simple Storage Service (S3). Pull messages off Firehose with Spark Streaming in parallel to persistence to Amazon S3.<br><b>B</b>. <input type="radio" disabled="">&nbsp;Publish messages to Amazon Kinesis Streams. Pull messages off Streams with Spark Streaming in parallel to AWS Lambda pushing messages from Streams to Firehose backed by Amazon Simple Storage Service (S3).<br><b>C</b>. <input type="radio" disabled="">&nbsp;Publish messages to Amazon Kinesis Firehose backed by Amazon Simple Storage Service (S3). Use AWS Lambda to pull messages from Firehose to Streams for processing with Spark Streaming.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Publish messages to Amazon Kinesis Streams, pull messages off with Spark Streaming, and write row data to Amazon Simple Storage Service (S3) before and after processing. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Publish messages to Amazon Kinesis Streams. Pull messages off Streams with Spark Streaming in parallel to AWS Lambda pushing messages from Streams to Firehose backed by Amazon Simple Storage Service (S3).<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as the data can be capture by Kinesis Streams. Kinesis Streams can feed data to Spark Streaming for analysis and Lambda to move the raw data to S3 for durable storage. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/blogs/big-data/persist-streaming-data-to-amazon-s3-using-amazon-kinesis-firehose-and-aws-lambda/" target="_blank">Streaming Pipeline</a> </p><p><img src="./bds3_files/121380.png"><br> </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121380">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 4 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251469"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company logs data from its application in large files and runs regular analytics of these logs to support internal reporting for three months after the logs are generated. After three months, the logs are infrequently accessed for up to a year. The company also has a regulatory control requirement to store application logs for seven years. Which course of action should the company take to achieve these requirements in the most cost-efficient way?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Store the files in S3 Glacier with a Deny Delete vault lock policy for archives less than seven years old and a vault access policy that restricts read access to the analytics IAM group and write access to the log writer service role.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Store the files in S3 Standard with a lifecycle policy to transition the storage class to Standard - IA after three months. After a year, transition the files to Glacier and add a Deny Delete vault lock policy for archives less than seven years old.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Store the files in S3 Standard with lifecycle policies to transition the storage class to Standard – IA after three months and delete them after a year. Simultaneously store the files in Amazon Glacier with a Deny Delete vault lock policy for archives less than seven years old.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Store the files in S3 Standard with a lifecycle policy to remove them after a year. Simultaneously store the files in Amazon S3 Glacier with a Deny Delete vault lock policy for archives less than seven years old. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Store the files in S3 Standard with lifecycle policies to transition the storage class to Standard – IA after three months and delete them after a year. Simultaneously store the files in Amazon Glacier with a Deny Delete vault lock policy for archives less than seven years old.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as there are two aspects to this question: setting up a lifecycle policy to ensure that objects are stored in the most cost-effective storage, and ensuring that the regulatory control is met. The lifecycle policy will store the objects on S3 Standard during the three months of active use, and then move the objects to S3 Standard – IA when access will be infrequent. The Deny Delete vault lock policy will ensure that the regulatory policy is met, but that policy must be applied over the entire lifecycle of the object, not just after it is moved to Glacier after the first year. Option C has the Deny Delete vault lock applied over the entire lifecycle of the object and is the right answer. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock-policy.html" target="_blank">Glacier Vault Lock Policy</a> </p><p><em>An Amazon S3 Glacier (Glacier) vault can have one resource-based vault access policy and one Vault Lock policy attached to it. A Vault Lock policy is a vault access policy that you can lock. Using a Vault Lock policy can help you enforce regulatory and compliance requirements. </em><em></em><br> </p><p>Option A is wrong as as the objects needs to be stored for instant access for 3 months and then moved to an infrequent access storage and then for archival </p><p>Option B is wrong as it might not meet the regulatory control as the deny is applied for the entire period.<br> </p><p>Option D is wrong as the files are infrequently accessed after 3 months and can be moved to S3-IA. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121323">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 5 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251470"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You need to perform ad-hoc SQL queries on massive amounts of well-structured data. Additional data comes in constantly at a high velocity, and you don't want to have to manage the infrastructure processing it if possible. Which solution should you use?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Kinesis Firehose and RDS</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;EMR running Apache Spark<br><b>C</b>. <input type="radio" disabled="">&nbsp;Kinesis Firehose and Redshift<br><b>D</b>. <input type="radio" disabled="">&nbsp;EMR using Hive <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Kinesis Firehose and Redshift<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as Kinesis Firehose can capture the data and store the in data in Redshift. Redshift can provide processing of huge structured data. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/kinesis/data-firehose/" target="_blank">Kinesis Firehose</a> </p><p><em>Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and <a href="https://aws.amazon.com/kinesis/data-firehose/splunk/">Splunk</a>, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.</em><em></em><br> </p><p>Option A is wrong as RDS is not an ideal choice for big data analytics. </p><p>Options B &amp; D are wrong as EMR processing would need to be managed. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%203%20-%20%23125351">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 6 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251471"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An administrator tries to use the Amazon Machine Learning service to classify social media posts that mention the administrator’s company into posts that require a response and posts that do not. The training dataset of 10,000 posts contains the details of each post including the timestamp, author, and full text of the post. The administrator is missing the target labels that are required for training. Which Amazon Machine Learning model is the most appropriate for the task?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <b>A</b>. <input type="radio" disabled="">&nbsp;Binary classification model, where the target class is the require-response post<br><b>B</b>. <input type="radio" disabled="">&nbsp;Binary classification model, where the two classes are the require-response post and does-not-require-response<br><b>C</b>. <input type="radio" disabled="">&nbsp;Multi-class prediction model, with two classes: require-response post and does-not-require-response<br><b>D</b>. <input type="radio" disabled="">&nbsp;Regression model where the predicted value is the probability that the post requires a response <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Binary classification model, where the target class is the require-response post<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as the labels are missing, a Binary classification model with a required response post can be applied. </p><p>Option B is wrong as B would require both the labels to classify the data </p><p>Options C &amp; D are wrong as the model are not ideal for classification. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121344">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 7 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251472"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company is performing a full migration of its systems from an on-premises data center to AWS. The company needs to move all the data stored on-premises to Amazon S3 within the next 4 weeks. Currently, the on-premises storage holds 900 TB of data and is connected to the Internet over a 100 Mbps link. Up to 20% of the link's throughput is regularly used in real time by existing systems. What is the MOST cost-effective way to perform the data migration in the given time frame?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Order multiple AWS Snowball devices to ship the data.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use a multipart upload to transfer the data over the existing link.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Set up an AWS Direct Connect link to upload the data.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Configure a VPN tunnel for the AWS environment to upload the data. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Order multiple AWS Snowball devices to ship the data.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as with 900TB of data and 80% of 100Mbps line, it would take years to transfer the data. Snowball provides a quick and cost effective option to transfer huge data from on-premises to AWS S3. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/snowball/" target="_blank">Snowball</a> </p><p><em>Snowball is a petabyte-scale data transport solution that uses devices designed to be secure to transfer large amounts of data into and out of the AWS Cloud. Using Snowball addresses common challenges with large-scale data transfers including high network costs, long transfer times, and security concerns. Customers today use Snowball to migrate analytics data, genomics data, video libraries, image repositories, backups, and to archive part of data center shutdowns, tape replacement or application migration projects. Transferring data with Snowball is simple, fast, more secure, and can be as little as one-fifth the cost of transferring data via high-speed Internet.</em><em></em><br> </p><p><img src="./bds3_files/Migration-Speeds.png"><br><br> </p><p>Options B &amp; D are wrong as the transfer is still done through internet. </p><p>Option C is wrong as Direct Connect needs time to setup and is not that cost effective for one time data transfer. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121246">AWS BDS-C00 Question feedback</a></p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 8 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251473"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A customer has an Amazon S3 bucket. Objects are uploaded simultaneously by a cluster of servers from multiple streams of data. The customer maintains a catalog of objects uploaded in Amazon S3 using an Amazon DynamoDB table. This catalog has the following fields: StreamName, TimeStamp, and ServerName, from which ObjectName can be obtained. The customer needs to define the catalog to support querying for a given stream or server within a defined time range. Which DynamoDB table scheme is most efficient to support these queries?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Define a Primary Key with ServerName as Partition Key and TimeStamp as Sort Key. Do NOT define a Local Secondary Index or Global Secondary Index.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Define a Primary Key with StreamName as Partition Key and TimeStamp followed by ServerName as Sort Key. Define a Global Secondary Index with ServerName as partition key and TimeStamp followed by StreamName.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Define a Primary Key with ServerName as Partition Key. Define a Local Secondary Index with StreamName as Partition Key. Define a Global Secondary Index with TimeStamp as Partition Key. <br><b>D</b>. <input type="radio" disabled="">&nbsp;Define a Primary Key with ServerName as Partition Key. Define a Local Secondary Index with TimeStamp as Partition Key. Define a Global Secondary Index with StreamName as Partition Key and TimeStamp as Sort Key. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Define a Primary Key with StreamName as Partition Key and TimeStamp followed by ServerName as Sort Key. Define a Global Secondary Index with ServerName as partition key and TimeStamp followed by StreamName.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as you can use composite primary keys using a combination of (StreamName as the partition key and TimeStamp as the sort key) and (ServerName as the partition key and TimeStamp as the sort key) which would provide the ability to query on both StreamName and ServerName over a time. </p><p>Option A is wrong as there is no way of querying for StreamName </p><p>Options C &amp; D are wrong Local Secondary Index needs to have the same partition key and only sort key can differ. Also, it does not allow to query for ServerName OR StreamName over time. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121383">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 9 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251474"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> Company A operates in Country X. Company A maintains a large dataset of historical purchase orders that contains personal data of their customers in the form of full names and telephone numbers. The dataset consists of 5 text files, 1TB each. Currently the dataset resides on-premises due to legal requirements of storing personal data in-country. The research and development department needs to run a clustering algorithm on the dataset and wants to use Elastic Map Reduce service in the closest AWS region. Due to geographic distance, the minimum latency between the on-premises system and the closet AWS region is 200 ms. Which option allows Company A to do clustering in the AWS Cloud and meet the legal requirement of maintaining personal data in-country?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <b>A</b>. <input type="radio" disabled="">&nbsp;Anonymize the personal data portions of the dataset and transfer the data files into Amazon S3 in the AWS region. Have the EMR cluster read the dataset using EMRFS.<br><b>B</b>. <input type="radio" disabled="">&nbsp;Establish a Direct Connect link between the on-premises system and the AWS region to reduce latency. Have the EMR cluster read the data directly from the on-premises storage system over Direct Connect.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Encrypt the data files according to encryption standards of Country X and store them on AWS region in Amazon S3. Have the EMR cluster read the dataset using EMRFS.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use AWS Import/Export Snowball device to securely transfer the data to the AWS region and copy the files onto an EBS volume. Have the EMR cluster read the dataset using EMRFS. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Anonymize the personal data portions of the dataset and transfer the data files into Amazon S3 in the AWS region. Have the EMR cluster read the dataset using EMRFS.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as the latency is high it would be ideal to transfer the data to AWS and process using EMR and EMRFS. Also, anonymizing the data would help meet the legal requirement. </p><p>Option B is wrong as EMR cannot access the data from on-premises directly. It still needs to be loaded to HDFS which would be in the AWS region. </p><p>Option C is wrong as it still does not meet the legal requirements as the data is moved out of the country, even though it is encrypted it can be accessed.<br> </p><p>Option D is wrong as EBS volumes are not an ideal storage for huge dataset and EMR cluster cannot read the data from EBS volumes.<br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%203%20-%20%23121412">AWS BDS-C00 Question feedback</a></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 10 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251475"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A system engineer for a company proposes digitalization and backup of large archives for customers. The systems engineer needs to provide users with a secure storage that makes sure that data will never be tampered with once it has been uploaded. How should this be accomplished?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Create an Amazon Glacier Vault. Specify a "Deny" Vault Lock policy on this Vault to block "glacier:DeleteArchive".</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Create an Amazon S3 bucket. Specify a "Deny" bucket policy on this bucket to block "s3:DeleteObject".<br><b>C</b>. <input type="radio" disabled="">&nbsp;Create an Amazon Glacier Vault. Specify a "Deny" vault access policy on this Vault to block "glacier:DeleteArchive".<br><b>D</b>. <input type="radio" disabled="">&nbsp;Create secondary AWS Account containing an Amazon S3 bucket. Grant "s3:PutObject" to the primary account. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Create an Amazon Glacier Vault. Specify a "Deny" Vault Lock policy on this Vault to block "glacier:DeleteArchive".<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as Glacier provides Vault Lock Policy which can be used to prevent an action on the Vault. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock-policy.html" target="_blank">Glacier Vault Lock Policy</a> </p><p><em>An Amazon S3 Glacier (Glacier) vault can have one resource-based vault access policy and one Vault Lock policy attached to it. A Vault Lock policy is a vault access policy that you can lock. Using a Vault Lock policy can help you enforce regulatory and compliance requirements.</em> </p><p><em>As an example of a Vault Lock policy, suppose that you are required to retain archives for one year before you can delete them. To implement this requirement, you can create a Vault Lock policy that denies users permissions to delete an archive until the archive has existed for one year. You can test this policy before locking it down. After you lock the policy, the policy becomes immutable.</em> </p><pre>{
     "Version":"2012-10-17",
     "Statement":[
      {
         "Sid": "deny-based-on-archive-age",
         "Principal": "*",
         "Effect": "Deny",
         "Action": "glacier:DeleteArchive",
         "Resource": [
            "arn:aws:glacier:us-west-2:123456789012:vaults/examplevault"
         ]
      }
   ]
}
</pre><p>Options B &amp; D are wrong as S3 is not ideal solution for archival </p><p>Option C is wrong as you need to use Vault Lock Policy and not Vault Access Policy. An Amazon S3 Glacier vault access policy is a resource-based policy that you can use to manage permissions to your vault. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121396">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 11 of 50 </b><small>- Multiple Answer</small> <span style="float:right;" class="grad_5251476"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A manufacturing company stores all telemetry data inside an Amazon DynamoDB table. The company is satisfied with the performance but concerned that the storage cost might increase over time. Which combination of steps should be taken to move old data into archival storage? (Select THREE.)<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="checkbox" checked="true" disabled="">&nbsp;Enable DynamoDB Streams on the table and TTL.</span><br><b>B</b>. <input type="checkbox" disabled="">&nbsp;Enable DynamoDB Streams and use the KCL with the DynamoDB Streams Kinesis Adapter to capture changes on DynamoDB tables.<br><b>C</b>. <input type="checkbox" disabled="">&nbsp;Create rolling tables on DynamoDB to store data in a particular order and create custom application logic to handle the creation and deletion of tables.<br><b>D</b>. <input type="checkbox" disabled="">&nbsp;Create a custom AWS Lambda function to regularly poll the DynamoDB Stream and deliver the batch records to an Amazon Kinesis Data Firehose.<br><b>E</b>. <input type="checkbox" disabled="">&nbsp;Create Amazon CloudWatch Events when a new item is added to the DynamoDB table. Invoke an AWS Lambda function to capture the changes and write to Amazon Kinesis Data Streams.<br><b>F</b>. <input type="checkbox" disabled="">&nbsp;Create an Amazon Kinesis Data Firehose delivery stream to load the data into Amazon S3 and set lifecycle policies to archive it to Amazon Glacier. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Enable DynamoDB Streams on the table and TTL.<br><b>D</b>. Create a custom AWS Lambda function to regularly poll the DynamoDB Stream and deliver the batch records to an Amazon Kinesis Data Firehose.<br><b>F</b>. Create an Amazon Kinesis Data Firehose delivery stream to load the data into Amazon S3 and set lifecycle policies to archive it to Amazon Glacier.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answers are <strong>A, D &amp; F</strong>. </p><p>Option A as TTL would enable DynamoDB data expiration without any additional cost. </p><p>Option D as Lambda can be configured to poll DynamoDB Streams for new items (i.e TTL expired) and push to Kinesis Firehose. </p><p>Option F as Kinesis Data Firehose can load the data into S3 and lifecycle can move to Glacier for archival. </p><p>Refer AWS Blog Post - <a href="https://aws.amazon.com/blogs/database/automatically-archive-items-to-s3-using-dynamodb-time-to-live-with-aws-lambda-and-amazon-kinesis-firehose/" target="_blank">DynamoDB Archival using Lambda and Kinesis</a> </p><p><img src="./bds3_files/OverviewImage.png"><br><br> </p><p><em>Above solution to remove older items from a DynamoDB table and archive them to S3 without having to manage a fleet of servers (see the following simplified workflow diagram). You use TTL to automatically delete old items and DynamoDB Streams to capture the TTL-expired items. You then connect DynamoDB Streams to Lambda, which lets you run code without provisioning or managing any servers. When new items are added to the DynamoDB stream (that is, as TTL deletes older items), the Lambda function triggers, writing the data to an <a href="https://aws.amazon.com/kinesis/firehose/">Amazon Kinesis Firehose</a> delivery stream. Firehose provides a simple, fully managed solution to load the data into S3, which is the archive.<br></em> </p><ol> <li><em>Activate TTL and DynamoDB Streams on your DynamoDB table.</em></li> <li><em>Create a Firehose delivery stream to load the data into S3.</em></li> <li><em>Create a Lambda function to poll the DynamoDB stream and deliver batch records from streams to Firehose.</em></li> <li><em>Validate that the application works.</em></li> </ol><p>Option B is wrong as this will include all the changes and it would be real time. </p><p>Option C is wrong as rolling tables require pre-creating tables to store data for particular months/weeks/days. This requires custom application logic to handle creating and deleting tables, and switching of reads and writes to new tables which would not be cost effective.<br> </p><p>Option E is wrong as CloudWatch Events do not track DynamoDB tables at data level.<br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121285">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 12 of 50 </b><small>- Multiple Answer</small> <span style="float:right;" class="grad_5251477"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An administrator decides to use the Amazon Machine Learning service to classify social media posts that mention your company into two categories: posts that require a response and posts that do not. The training dataset of 10,000 posts contains the details of each post, including the timestamp, author, and full text of the post. You are missing the target labels that are required for training. Which two options will create valid target label data?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="checkbox" checked="true" disabled="">&nbsp;Ask the social media handling team to review each post and provide the label.</span><br><b>B</b>. <input type="checkbox" disabled="">&nbsp;Use the sentiment analysis NLP library to determine whether a post requires a response.<br><b>C</b>. <input type="checkbox" disabled="">&nbsp;Use the Amazon Mechanical Turk web service to publish Human Intelligence Tasks that ask Turk workers to label the posts.<br><b>D</b>. <input type="checkbox" disabled="">&nbsp;Using the a priori probability distribution of the two classes, use Monte-Carlo simulation to generate the labels. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Ask the social media handling team to review each post and provide the label.<br><b>C</b>. Use the Amazon Mechanical Turk web service to publish Human Intelligence Tasks that ask Turk workers to label the posts.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answers are <strong>A &amp; C</strong> as you need accurate data to train the service and get accurate results from future data. </p><p>Options <strong>B &amp; D</strong> are wrong as it would end up training an ML model using the output from a different machine learning model and therefore would significantly increase the possible error rate. It is extremely important to have a very low error rate (if any!) in your training set, and therefore human-validated or assured labels are essential. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121320">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 13 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251478"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You have just joined a new company and have been put in charge of EC2 instances and any other services that use EC2 instances. You notice that the company has been slow to take advantage of AWS per-second Billing, specifically in the area of EMR and Spot Instances. What immediate steps can you take on EMR with spot instances to improve cost saving and performance?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Use on-demand instances instead.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Run fewer instances for a shorter amount of time.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Run fewer instances for a longer amount of time.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Run more instances for a shorter amount of time. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Run more instances for a shorter amount of time.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as EMR now supports instances with per second billing, it would be more cost efficient and performant to use more instances for shorter amount of time. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/blogs/aws/new-per-second-billing-for-ec2-instances-and-ebs-volumes/" target="_blank">Per-Second Billing</a> </p><p><em>Amazon EMR – Our customers add capacity to their EMR clusters in order to get their results more quickly. With per-second billing for the EC2 instances in the clusters, adding nodes is more cost-effective than ever. To learn more, read <a href="https://aws.amazon.com/about-aws/whats-new/2017/10/amazon-emr-now-supports-per-second-billing/">Amazon EMR Now Supports Per-Second Billing</a>.</em><em></em><br> </p><p>Option A is wrong as On-demand instances would not be cost effective. </p><p>Options B &amp; C are wrong as they are not performant. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121269">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 14 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251479"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company generates a large number of files each month and needs to use AWS import/export to move these files into Amazon S3 storage. To satisfy the auditors, the company needs to keep a record of which files were imported into Amazon S3. What is a low-cost way to create a unique log for each import job?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <b>A</b>. <input type="radio" disabled="">&nbsp;Use the same log file prefix in the import/export manifest files to create a versioned log file in Amazon S3 for all imports. <br><b>B</b>. <input type="radio" disabled="">&nbsp;Use the log file prefix in the import/export manifest files to create a unique log file in Amazon S3 for each import.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Use the log file checksum in the import/export manifest files to create a unique log file in Amazon S3 for each import.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use a script to iterate over files in Amazon S3 to generate a log after each import/export job. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Use the log file prefix in the import/export manifest files to create a unique log file in Amazon S3 for each import.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as creating a unique log file for each import would help </p><p><em>The AWS Import/Export process generates a log file. The log file name always ends with the phrase import-log- followed by your JobId. There is a remote chance that you already have an object with this name.To avoid a key collision, you can add an optional prefix to the log file by adding the logPrefix option in the manifest. AWS Import/Export takes the string value specified for this option and inserts it between the bucket name and log report name </em> </p><p><em>The log file is a UTF-8 encoded CSV file that contains, among other things, information about each file loaded to or from your storage device. With Amazon S3 import jobs, AWS Import/Export saves the log to the same Amazon S3 bucket as your data. </em> </p><p><em>For an import job, the log name ends with the phrase import-log- followed by your JOBID. For example, if the import JOBID is 53TX4, the log name ends in import-log-53TX4. By default, if you do not set logPrefix in the manifest file, a job loaded to mybucket with the JOBID of 53TX4 loads the logs to http://mybucket.s3.amazonaws.com/import-log-53TX4. If you set logPrefix to logs/, the log location is http://s3.amazonaws.com/mybucket/logs/import-log-53TX4. </em> </p><p><em>Note If you have a log object with the same key name as an existing Amazon S3 object, the new log overwrites the existing object.You can use the logPrefix option to prevent object collisions. </em> </p><p>Option A is wrong as same log file might cause an overwrite and the S3 bucket is not versioned by default. </p><p>Option C is wrong as checksum cannot be used for creating unique log files. </p><p>Option D is wrong as is not not cost-effective solution. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121407">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 15 of 50 </b><small>- Multiple Answer</small> <span style="float:right;" class="grad_5251480"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> Your team is building up a smart home iOS APP. The end users will use your company’s camera-equipped home devices such as baby monitors, webcams, and home surveillance systems. Then the videos would be uploaded to AWS. The users can then play the on-demand or live videos using the format of HTTP Live Streaming (HLS) through the Mobile application. Which combinations of steps should you use to design the solution? (Select TWO)<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="checkbox" checked="true" disabled="">&nbsp;Create a Kinesis Data Firehose to ingest, durably store and encrypt the live videos from the users' home devices.</span><br><b>B</b>. <input type="checkbox" disabled="">&nbsp;Create a Kinesis video stream to capture, store, and index the videos from the camera-equipped home devices.<br><b>C</b>. <input type="checkbox" disabled="">&nbsp;Transform the stream data to HLS compatible data by using Kinesis Data Analytics or customer code in EC2/Lambda. Then in the mobile application, use HLS protocol to display the video stream by using the converted HLS streaming data.<br><b>D</b>. <input type="checkbox" disabled="">&nbsp;In the mobile application, use HLS to display the video stream by using the HLS streaming session URL. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Create a Kinesis video stream to capture, store, and index the videos from the camera-equipped home devices.<br><b>D</b>. In the mobile application, use HLS to display the video stream by using the HLS streaming session URL.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answers are <strong>B &amp; D </strong>as Kinesis Video Streams can be used to stream, store Videos and these videos can then be streamed back using APIs. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/kinesis/video-streams/faqs/" target="_blank">Kinesis Video Streams</a> </p><p><em>Amazon Kinesis Video Streams makes it easy to securely stream video from connected devices to AWS for analytics, machine learning (ML), and other processing. Kinesis Video Streams automatically provisions and elastically scales all the infrastructure needed to ingest streaming video data from millions of devices. It also durably stores, encrypts, and indexes video data in your streams, and allows you to access your data through easy-to-use APIs. Kinesis Video Streams enables you to quickly build computer vision and ML applications through integration with Amazon Rekognition Video and libraries for ML frameworks such as Apache MxNet, TensorFlow, and OpenCV.<br></em> </p><p><em>Kinesis Video Streams is ideal for building computer vision-enabled ML applications that are becoming prevalent in a wide range of use cases such as the following:<br></em> </p><p><i><em>Smart Home - </em></i><em>With Kinesis Video Streams, you can easily stream video and audio from camera-equipped home devices such as baby monitors, webcams, and home surveillance systems to AWS. You can then use the streams to build a variety of smart home applications ranging from simple video playback to intelligent lighting, climate control systems, and security solutions.</em> </p><p>Option A is wrong as Kinesis Data Firehose is used for streaming data delivery rather than video streaming. </p><p>Option C is wrong as there is no need for transforming the data to HLS, if using Kinesis Video Streams. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121229">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 16 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251481"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> Your customer is willing to consolidate their log streams (access logs, application logs, security logs etc.) in one single system. Once consolidated, the customer wants to analyze these logs in real time based on heuristics. From time to time, the customer needs to validate heuristics, which requires going back to data samples extracted from the last 12 hours. What is the best approach to meet your customer’s requirements?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Send all the log events to Amazon SQS. Setup an Auto Scaling group of EC2 servers to consume the logs and apply the heuristics.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Send all the log events to Amazon Kinesis develop a client process to apply heuristics on the logs<br><b>C</b>. <input type="radio" disabled="">&nbsp;Configure Amazon CloudTrail to receive custom logs, use EMR to apply heuristics the logs<br><b>D</b>. <input type="radio" disabled="">&nbsp;Setup an Auto Scaling group of EC2 syslogd servers, store the logs on S3 use EMR to apply heuristics on the logs <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Send all the log events to Amazon Kinesis develop a client process to apply heuristics on the logs<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Key point here is requiring real time analytics and ability to go back to data samples from last 12 hours </p><p>Correct answer is <strong>B</strong> as Kinesis can perform real time analysis and stores data for 24 hours which can be extended to 7 days. Also data is not removed from Kinesis till 24 hours default, can be extended, and can be used to retrieve past data </p><p>Option A is wrong as SQS is not for real time ingestion of data. Also, once the message is consume it would be deleted and not available. </p><p><em>SQS minimum message size is 1,024 bytes (1 KB). The maximum is 262,144 bytes (256 KB). While Kinesis can store upload 1MB</em><br> </p><p>Option C is wrong as CloudTrail is only for auditing and CloudWatch can be used to collect logs </p><p>Option D is wrong as EMR is for batch analysis </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121215">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 17 of 50 </b><small>- Multiple Answer</small> <span style="float:right;" class="grad_5251482"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You company has launched an EMR cluster to support their big data analytics requirements. They are planning to build an application running on EMR which supports both OLTP and operational analytics allowing you to use standard SQL queries and JDBC APIs to work with an Apache HBase backing store. Also data transfer tool between Amazon S3, Hadoop, HDFS, and RDBMS databases. Which EMR Hadoop ecosystem fulfils the requirements? (Select TWO)<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="checkbox" checked="true" disabled="">&nbsp;Apache Hue</span><br><b>B</b>. <input type="checkbox" disabled="">&nbsp;Apache Flink<br><b>C</b>. <input type="checkbox" disabled="">&nbsp;Apache Phoenix<br><b>D</b>. <input type="checkbox" disabled="">&nbsp;Apache Sqoop<br><b>E</b>. <input type="checkbox" disabled="">&nbsp;Apache Ganglia <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Apache Phoenix<br><b>D</b>. Apache Sqoop<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answers are <strong>C &amp; D</strong> </p><p>Option C as <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-phoenix.html" target="_blank">Apache Phoenix</a> is used for OLTP and operational analytics, allowing you to use standard SQL queries and JDBC APIs to work with an Apache HBase backing store. </p><p>Option D as <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-sqoop.html" target="_blank">Apache Sqoop</a> is a tool for transferring data between Amazon S3, Hadoop, HDFS, and RDBMS databases. </p><p>Option A is wrong as <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hue.html" target="_blank">Hue</a> (Hadoop User Experience) is an open-source, web-based, graphical user interface for use with Amazon EMR and Apache Hadoop. </p><p>Option B is wrong as <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-flink.html" target="_blank">Apache Flink</a> is a streaming dataflow engine that you can use to run real-time stream processing on high-throughput data sources. </p><p>Option E is wrong as <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-ganglia.html" target="_blank">Apache Ganglia</a> open source project is a scalable, distributed system designed to monitor clusters and grids while minimizing the impact on their performance. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%203%20-%20%23125302">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 18 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251483"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company develops a tool whose coverage includes blogs, news sites, forums, videos, reviews, images and social networks such as Twitter and Facebook. Users can search data by using Text and Image Search, and use charting, categorization, sentiment analysis and other features to provide further information and analysis. They have access to over 80 million sources. They want to provide Image and text analysis capabilities to the applications which includes identify objects, people, text, scenes, and activities and also provides highly accurate facial analysis and facial recognition. What service can provide this capability?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Amazon Comprehend</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Amazon Rekognition<br><b>C</b>. <input type="radio" disabled="">&nbsp;Amazon Polly<br><b>D</b>. <input type="radio" disabled="">&nbsp;Amazon SageMaker <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Amazon Rekognition<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as Amazon <a href="https://docs.aws.amazon.com/rekognition/latest/dg/what-is.html" target="_blank">Rekognition</a> makes it easy to add image and video analysis to your applications. You just provide an image or video to the Rekognition API, and the service can identify objects, people, text, scenes, and activities. It can detect any inappropriate content as well. Amazon Rekognition also provides highly accurate facial analysis and facial recognition. You can detect, analyze, and compare faces for a wide variety of use cases, including user verification, cataloging, people counting, and public safety. </p><p>Option A is wrong as Amazon <a href="https://docs.aws.amazon.com/comprehend/latest/dg/what-is.html" target="_blank">Comprehend</a> uses natural language processing (NLP) to extract insights about the content of documents. </p><p>Option C is wrong as Amazon <a href="https://docs.aws.amazon.com/polly/latest/dg/what-is.html" target="_blank">Polly</a> is a cloud service that converts text into lifelike speech. You can use Amazon Polly to develop applications that increase engagement and accessibility. </p><p>Option D is wrong as Amazon <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html" target="_blank">SageMaker</a> is a fully managed machine learning service. With Amazon SageMaker, data scientists and developers can quickly and easily build and train machine learning models, and then directly deploy them into a production-readyhosted environment.<br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%203%20-%20%23125349">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 19 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251484"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An administrator needs to manage a large catalog of items from various external sellers. The administrator needs to determine if the items should be identified as minimally dangerous, dangerous, or highly dangerous based on their textual descriptions. The administrator already has some items with the danger attribute, but receives hundreds of new item descriptions every day without such classification. The administrator has a system that captures dangerous goods reports from customer support team or from user feedback. What is a cost-effective architecture to solve this issue?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Build a set of regular expression rules that are based on the existing examples, and run them on the DynamoDB Streams as every new item description is added to the system.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Build a Kinesis Streams process that captures and marks the relevant items in the dangerous goods reports using a Lambda function, once more than two reports have been filed.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Build a machine learning model to properly classify dangerous goods and run it on the DynamoDB Streams as every new item description is added to the system.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Build a machine learning model with binary classification for dangerous goods and run it on the DynamoDB Streams as every new item description is added to the system. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Build a machine learning model to properly classify dangerous goods and run it on the DynamoDB Streams as every new item description is added to the system.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as there is data already to learn, a machine learning model can be developed to properly classify the goods and run it with DynamoDB Streams. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/blogs/machine-learning/anomaly-detection-on-amazon-dynamodb-streams-using-the-amazon-sagemaker-random-cut-forest-algorithm/" target="_blank">Sample Similar Implementation</a> </p><p>Option A is wrong as regular expressions would only do matches which are not reliable </p><p>Option B is wrong as the function would not work on new or similar data but only on data already reported. </p><p>Option D is wrong as binary classification would help classify into 1 or 0. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121334">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 20 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251485"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> Your client has a high-volume DynamoDB table that serves comment information to an internal API. Currently, the table allows you to query with a composite primary key with postId as a partition key and commentId as a sort key. Application validation ensures that each item has other fields including timestamp, userId, and sentimentScore. The client has several long-running users, and they would like to provide more effective ways of surfacing posts from them from different time frames. How might the client enable this sort of functionality?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Create a Global Secondary Index with a partition key of userId and a sort key of timestamp.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Create a Local Secondary Index with a partition key of timestamp and a sort key of userId.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Create a Local Secondary Index with a partition key of userId and a sort key of timestamp.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Create a Global Secondary Index with a partition key of timestamp and a sort key of userId. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Create a Global Secondary Index with a partition key of userId and a sort key of timestamp.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as they want to query on users in different times, it would be best to create a Global Secondary Index using userId as partition key and timestamp as sort key. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html#HowItWorks.CoreComponents.SecondaryIndexes" target="_blank">DynamoDB Secondary Indexes</a> </p><p>Options B &amp; C are wrong as Local Secondary Index needs to have the same partition key as the base table. Also, Local Secondary Index can be created only during table creation. </p><p>Option D is wrong as the GSI needs to be created using userId as partition key and timestamp as sort key. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121753">AWS BDS-C00 Question feedback</a></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 21 of 50 </b><small>- Multiple Answer</small> <span style="float:right;" class="grad_5251486"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A Solutions Architect is designing a weather forecast application. Every hour, the application will receive a new set of raw data from weather stations. The application will analyze this data and produce a set of local weather forecasts available for users to download. The analysis takes 50 minutes to run on 2,000 vCPUs. The analysis must complete before the next set of data is available. Each local weather forecast is typically 10 GB in size. The forecasts are accessed heavily during the first hour they are available, with usage dropping rapidly as newer forecasts become available. Which combination of steps is the MOST cost-effective architecture? (Select TWO.)<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="checkbox" checked="true" disabled="">&nbsp;Conduct the analysis on an Amazon EC2-based cluster using 1-hour Spot blocks in multiple AWS Regions.</span><br><b>B</b>. <input type="checkbox" disabled="">&nbsp;Conduct the analysis on a cluster of Amazon EC2 instances using Reserved Instances in a single AWS Region.<br><b>C</b>. <input type="checkbox" disabled="">&nbsp;Store weather forecast data in Amazon S3 Standard. Configure a lifecycle policy to transition the data to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.<br><b>D</b>. <input type="checkbox" disabled="">&nbsp;Store weather forecast data in Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA). Configure a lifecycle policy to transition the data to Amazon Glacier after 90 days.<br><b>E</b>. <input type="checkbox" disabled="">&nbsp;Store weather forecast data in Amazon S3 Standard-Infrequent Access (S3 Standard-IA). Configure a lifecycle policy to transition the data to Amazon Glacier after 90 days. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Conduct the analysis on a cluster of Amazon EC2 instances using Reserved Instances in a single AWS Region.<br><b>D</b>. Store weather forecast data in Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA). Configure a lifecycle policy to transition the data to Amazon Glacier after 90 days.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answers are <strong>B &amp; D</strong></p><p>Option B as the job runs for 50 mins within an hour with a sustained usage of around 20 hours of the 24 hours. </p><p>Option D as the focus is on most cost effective architecture, S3 One Zone-IA would be an ideal option as the data is used only during the first hour. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/s3/storage-classes/" target="_blank">S3 Storage Classes</a> &amp; <a href="https://aws.amazon.com/s3/pricing/" target="_blank">Pricing</a> </p><p>Option A is wrong as the usage is sustained Reserved instances would be more reliable and cost effective as compared to Spot blocks. </p><p>Option C is wrong as Standard and S3 Standard-IA would not provide the most cost effective option. </p><p>Option E is wrong as Standard-IA would not provide the most cost effective option as compared to S3 One Zone-IA </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%203%20-%20%23125353">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 22 of 50 </b><small>- Multiple Answer</small> <span style="float:right;" class="grad_5251487"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An administrator needs to design a distribution strategy for a star schema in a Redshift cluster. The administrator needs to determine the optimal distribution style for the tables in the Redshift schema. In which three circumstances would choosing key-based distribution be most appropriate? (Select three.)<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="checkbox" checked="true" disabled="">&nbsp;When the administrator needs to optimize a large, slowly changing dimension table.</span><br><b>B</b>. <input type="checkbox" disabled="">&nbsp;When the administrator needs to reduce cross-node traffic.<br><b>C</b>. <input type="checkbox" disabled="">&nbsp;When the administrator needs to optimize the fact table for parity with the number of slices.<br><b>D</b>. <input type="checkbox" disabled="">&nbsp;When the administrator needs to balance data distribution and collocation data.<br><b>E</b>. <input type="checkbox" disabled="">&nbsp;When the administrator needs to take advantage of data locality on a local node for joins and aggregates. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. When the administrator needs to reduce cross-node traffic.<br><b>D</b>. When the administrator needs to balance data distribution and collocation data.<br><b>E</b>. When the administrator needs to take advantage of data locality on a local node for joins and aggregates.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answers are <strong>B, D &amp; E. </strong>With key-based distribution, the rows are distributed according to the values in one column. The leader node places matching values on the same node slice. If you distribute a pair of tables on the joining keys, the leader node collocates the rows on the slices according to the values in the joining columns so that matching values from the common columns are physically stored together. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/blogs/big-data/amazon-redshift-engineerings-advanced-table-design-playbook-distribution-styles-and-distribution-keys/" target="_blank">Redshift Distribution Styles</a> </p><p>Option A is wrong as DISTSTYLE ALL tables are most appropriate for smaller, slowly changing dimension tables. </p><p>Option C is wrong as key distribution impact the number of slices. KEY requires a single column to be defined as a DISTKEY. On ingest, Amazon Redshift hashes each DISTKEY column value, and route hashes to the same slice consistently. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121361">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 23 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251488"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You need to analyze clickstream data on your website from multiple applications. You want to analyze the pattern of pages a consumer clicks on and in what order. You need to be able to use the data in real time and want to manage as little infrastructure as possible. Which option would meet this requirement?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 3 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Publish web clicks by session to an Amazon SQS.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use Elastic MapReduce to ingest the data and analyze it.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Use Amazon Kinesis with a worker to process the data received from the Kinesis stream.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Send click events directly to Amazon Redshift and then analyze them with SQL. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Use Amazon Kinesis with a worker to process the data received from the Kinesis stream.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as the click streams can be captured using Kinesis with a Kinesis client to process the data. Kinesis Streams is a completely managed service. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/kinesis/data-streams/faqs/" target="_blank">Kinesis Streams FAQ</a> </p><p><em>Amazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs. You can continuously add various types of data such as clickstreams, application logs, and social media to an Amazon Kinesis data stream from hundreds of thousands of sources. Within seconds, the data will be available for your Amazon Kinesis Applications to read and process from the stream.</em><em></em><br> </p><p><em>Amazon Kinesis Data Streams manages the infrastructure, storage, networking, and configuration needed to stream your data at the level of your data throughput. You do not have to worry about provisioning, deployment, ongoing-maintenance of hardware, software, or other services for your data streams. In addition, Amazon Kinesis Data Streams synchronously replicates data across three availability zones, providing high availability and data durability.<br></em> </p><p>Options A, B &amp; D are wrong as they do not provide real time handling. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121744">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 24 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251489"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You have been asked to ensure that all AWS API calls are collected across your company's AWS account and that they are kept around for 90 days for analysis. After that, they must be able to be restored for 3 years. How can you meet these needs in a scalable, cost-effective way?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Enable AWS CloudTrail logging across all accounts to a centralized Amazon S3 bucket with versioning enabled. Set a lifecycle policy to move the data to Amazon Glacier daily, and expire the data after 90 days.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Enable CloudTrail logging to a centralized S3 bucket, set a lifecycle policy to move the data to Glacier after 90 days, and expire the data after 3 years.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Enable CloudTrail logging to Glacier, and set a lifecycle policy to expire the data after 3 years.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Enable CloudTrail logging in all accounts into S3 buckets, and set a lifecycle policy to expire the data in each bucket after 3 years. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Enable CloudTrail logging to a centralized S3 bucket, set a lifecycle policy to move the data to Glacier after 90 days, and expire the data after 3 years.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as the CloudTrail logging can be directed to a centralized S3 bucket. Lifecycle policies on the bucket can help to transition the data to low cost archival storage i.e. Glacier after 90 days and expire after 3 years. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html" target="_blank">S3 Object Lifecycle Management</a> &amp; <a href="https://aws.amazon.com/cloudtrail/faqs/" target="_blank">CloudTrail FAQs</a> </p><p><em>AWS CloudTrail is a web service that records activity made on your account and delivers log files to your Amazon S3 bucket.<br></em> </p><p><em>Applying a trail to all regions refers to creating a trail that will record AWS account activity in all regions. This setting also applies to any new regions that are added. <br></em> </p><p><em>You can create and manage a trail across all regions in the partition in one API call or few clicks. You will receive a record of account activity made in your AWS account across all regions to one S3 bucket or CloudWatch logs log group. When AWS launches a new region, you will receive the log files containing event history for the new region without taking any action.</em><em></em><br> </p><p><em>To manage your objects so that they are stored cost effectively throughout their lifecycle, configure their lifecycle. A lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions:</em> </p><ul> <li><em><strong>Transition actions</strong>—Define when objects transition to another <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html">storage class</a>. For example, you might choose to transition objects to the STANDARD_IA storage class 30 days after you created them, or archive objects to the GLACIER storage class one year after creating them.There are costs associated with the lifecycle transition requests.</em></li> <li><em><strong>Expiration actions</strong>—Define when objects expire. Amazon S3 deletes expired objects on your behalf.</em></li> </ul><p>Option A is wrong as the lifecycle policies do not meet the requirement. </p><p>Option C is wrong as CloudTrail cannot deliver to Glacier directly.<br> </p><p>Option D is wrong as the data can be kept into different S3 per region, however it is more cumbersome to manage and also its not most cost-effective as data can be moved to Glacier after 90 days.<br> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%203%20-%20%23125224">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 25 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251490"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company has two different types of repotting needs on their 200-GB data warehouse;– Data scientists run a small number of concurrent adhoc SQL queries that can take several minutes each to run.– Display screens throughout the company run many fast SQL queries to populate dashboards,Which design would meet these requirements with the LEAST cost?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Replicate relevant data between Amazon Redshift and Amazon DynamoDB. Data scientists use Redshift. Dashboards use DynamoDB</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Configure auto-replication between Amazon Redshift and Amazon RDS. Data scientists use Redshift and Dashboards use RDS<br><b>C</b>. <input type="radio" disabled="">&nbsp;Use Amazon Redshift for both requirements, with separate query queues configured in workload management.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use Amazon Redshift for Data Scientists; Run automated dashboard queries against Redshift and store the results in Amazon ElastiCache, Dashboards query ElastiCache. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Use Amazon Redshift for both requirements, with separate query queues configured in workload management.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as Redshift provides workload management which can help prioritize the interactive and long running jobs. Storing the data in a single storage service would also help keep the costs to minimum. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/blogs/big-data/run-mixed-workloads-with-amazon-redshift-workload-management/" target="_blank">Mixed workload with Redshift</a> </p><p><em>Mixed workloads run batch and interactive workloads (short-running and long-running queries or reports) concurrently to support business needs or demand. Typically, managing and configuring mixed workloads requires a thorough understanding of access patterns, how the system resources are being used and performance requirements.</em> </p><p><em>It’s common for mixed workloads to have some processes that require higher priority than others. Sometimes, this means a certain job must complete within a given SLA. Other times, this means you only want to prevent a non-critical reporting workload from consuming too many cluster resources at any one time.</em> </p><p><em>Without workload management (WLM), each query is prioritized equally, which can cause a person, team, or workload to consume excessive cluster resources for a process which isn’t as valuable as other more business-critical jobs.</em> </p><p><em>You can use WLM to define the separation of business concerns and to prioritize the different types of concurrently running queries in the system:</em> </p><ul> <li><em>Interactive: Software that accepts input from humans as it runs. Interactive software includes most popular programs, such as BI tools or reporting applications.</em> <ul> <li><em>Short-running, read-only user queries such as Tableau dashboard query with low latency requirements.</em></li> <li><em>Long-running, read-only user queries such as a complex structured report that aggregates the last 10 years of sales data.</em></li> </ul></li> <li><em>Batch: Execution of a job series in a server program without manual intervention (non-interactive). The execution of a series of programs, on a set or “batch” of inputs, rather than a single input, would instead be a custom job.</em> <ul> <li><em>Batch queries includes bulk INSERT, UPDATE, and DELETE transactions, for example, ETL or ELT programs.</em></li> </ul></li> </ul><p>Options A &amp; B are wrong as it would result in duplication of data and with 2 services costs would not be minimal. </p><p>Option D is wrong as ElastiCache would not provide the latest data. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%203%20-%20%23125354">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 26 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251491"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You are provisioning an application using EMR. You have requested 100 instances. You are charged $0.015 per hour, per instance. In the first 10 minutes after your launch request, Amazon EMR starts your cluster. 90 of your instances are available. It takes your cluster one hour to complete. How much will you be charged for this EMR usage for the first hour?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;$0.015</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;$1.50 per hour<br><b>C</b>. <input type="radio" disabled="">&nbsp;$1.35 per hour<br><b>D</b>. <input type="radio" disabled="">&nbsp;$0 <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. $1.35 per hour<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as EMR starts charging when 90% of the capacity is available, which is $1.35 (90 * <em>$0.015 per hour</em>) </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/emr/faqs/" target="_blank">EMR FAQs</a> </p><p><em>Q: When does billing of my Amazon EMR cluster begin and end?</em> </p><p><em>Billing commences when Amazon EMR starts running your cluster. You are only charged for the resources actually consumed. For example, let’s say you launched 100 Amazon EC2 Standard Small instances for an Amazon EMR cluster, where the Amazon EMR cost is an incremental $0.015 per hour. The Amazon EC2 instances will begin booting immediately, but they won’t necessarily all start at the same moment. Amazon EMR will track when each instance starts and will check it into the cluster so that it can accept processing tasks.</em> </p><p><em>In the first 10 minutes after your launch request, Amazon EMR either starts your cluster (if all of your instances are available) or checks in as many instances as possible. Once the 10 minute mark has passed, Amazon EMR will start processing (and charging for) your cluster as soon as 90% of your requested instances are available. As the remaining 10% of your requested instances check in, Amazon EMR starts charging for those instances as well.</em> </p><p><em>So, in the above example, if all 100 of your requested instances are available 10 minutes after you kick off a launch request, you’ll be charged $1.50 per hour (100 * $0.015) for as long as the cluster takes to complete. If only 90 of your requested instances were available at the 10 minute mark, you’d be charged $1.35 per hour (90 * $0.015) for as long as this was the number of instances running your cluster. When the remaining 10 instances checked in, you’d be charged $1.50 per hour (100 * $0.015) for as long as the balance of the cluster takes to complete.</em> </p><p><em>Each cluster will run until one of the following occurs: you terminate the cluster with the TerminateJobFlows API call (or an equivalent tool), the cluster shuts itself down, or the cluster is terminated due to software or hardware failure.</em> </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%203%20-%20%23125174">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 27 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251492"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company’s social media manager requests more staff on the weekends to handle an increase in customer contacts from a particular region. The company needs a report to visualize the trends on weekends over the past 6 months using QuickSight. How should the data be represented?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;A line graph plotting customer contacts vs. time, with a line for each region</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;A pie chart per region plotting customer contacts per day of week<br><b>C</b>. <input type="radio" disabled="">&nbsp;A map of regions with a heatmap overlay to show the volume of customer contacts<br><b>D</b>. <input type="radio" disabled="">&nbsp;A bar graph plotting region vs. volume of social media contacts <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. A line graph plotting customer contacts vs. time, with a line for each region<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as the main requirement is to track the customer contacts over a period of 6 months, a line graph with customer contacts vs. time would meet the requirement. </p><p>Option B is wrong as pie chart is not a right representation as they do not show changes over time. </p><p>Option C is wrong as it does not provide the region and time.<br> </p><p>Option D is wrong as it just provides a volume but does not provide it over time. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121411">AWS BDS-C00 Question feedback</a></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 28 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251493"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An administrator has a 500-GB file in Amazon S3. The administrator runs a nightly COPY command into a 10-node Amazon Redshift cluster. The administrator wants to prepare the data to optimize performance of the COPY command. How should the administrator prepare the data?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Compress the file using gz compression.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Split the file into 500 smaller files.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Convert the file format to AVRO.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Split the file into 10 files of equal size. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Split the file into 500 smaller files.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is B as the critical aspect of this question is running the COPY command with the maximum amount of parallelism. It will have a greater effect because it will allow Amazon Redshift to load multiple files per instance in parallel (COPY can process one file per slice on each node) </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/redshift/latest/dg/c_loading-data-best-practices.html" target="_blank">Redshift Best Practices - Loading Data</a> </p><p><em><span class="redactor-invisible-space"><strong><em>Split Your Load Data into Multiple Files</em><span class="redactor-invisible-space"></span></strong> - </span>The COPY command loads the data in parallel from multiple files, dividing the workload among the nodes in your cluster. When you load all the data from a single large file, Amazon Redshift is forced to perform a serialized load, which is much slower. Split your load data files so that the files are about equal size, between 1 MB and 1 GB after compression. For optimum parallelism, the ideal size is between 1 MB and 125 MB after compression. The number of files should be a multiple of the number of slices in your cluster.</em><em></em><br> </p><p>Options A &amp; C are wrong as Compressing the files is a recommended practice and will also increase performance, but not to the same extent as loading multiple files in parallel. </p><p>Option D is wrong as even though it will also load files in parallel i.e. one file per node, which will increase performance, however not as much as Option B.</p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%203%20-%20%23121316">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 29 of 50 </b><small>- Multiple Answer</small> <span style="float:right;" class="grad_5251494"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You manage a web advertising platform on a single AWS account. This platform produces real-time ad-click data that you store as objects in an Amazon S3 bucket called "click-data". Your advertising partners want to use Amazon Elastic MapReduce in their own AWS accounts to do analytics on the ad-click data. They've asked for immediate access to the ad-dick data so that they can run analytics. Which two choices are required to facilitate secure access to this data? Choose 2 answers.<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="checkbox" checked="true" disabled="">&nbsp;Create a cross-account IAM role with a trust policy that contains partner AWS account IDs and a unique external ID</span><br><b>B</b>. <input type="checkbox" disabled="">&nbsp;Create a new IAM group for AWS Data Pipeline users with a trust policy that contains partner AWS account IDs. <br><b>C</b>. <input type="checkbox" disabled="">&nbsp;Configure an Amazon S3 bucket policy for the "click-data" bucket that allows Read-Only access to the objects and associate this policy with an IAM role. <br><b>D</b>. <input type="checkbox" disabled="">&nbsp;Configure the Amazon S3 bucket access control list to allow access to the partners Amazon Elastic MapReduce cluster.<br><b>E</b>. <input type="checkbox" disabled="">&nbsp;Configure AWS Data Pipeline to transfer the data from the ''click-data" bucket to the partner's Amazon Elastic MapReduce cluster.<br><b>F</b>. <input type="checkbox" disabled="">&nbsp;Configure AWS Data Pipeline in the partner AWS accounts to use the web Identity Federation API to access data in the "click-data" bucket. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Create a cross-account IAM role with a trust policy that contains partner AWS account IDs and a unique external ID<br><b>C</b>. Configure an Amazon S3 bucket policy for the "click-data" bucket that allows Read-Only access to the objects and associate this policy with an IAM role. <br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer are <strong>A &amp; C.</strong> As the access needs to secure, the data should be sent to the partner account. An IAM cross account role can be created for the AWS partner account with a external ID for security and an S3 bucket policy can be created to allow only read access and associate it with an IAM role. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%203%20-%20%23125352">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 30 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251495"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company is centralizing a large number of unencrypted small files from multiple Amazon S3 buckets. The company needs to verify that the files contain the same data after centralization. Which method meets the requirements?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Compare the S3 Etags from the source and destination objects.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Call the S3 CompareObjects API for the source and destination objects.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Place a HEAD request against the source and destination objects comparing SIG v4.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Compare the size of the source and destination objects. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Compare the S3 Etags from the source and destination objects.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as S3 stores the MD5 digest of the object data which can be verify to ensure the object contents have not changed. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/AmazonS3/latest/API/RESTCommonResponseHeaders.html" target="_blank">S3 Response Headers</a> &amp; <a href="https://aws.amazon.com/premiumsupport/knowledge-center/data-integrity-s3/" target="_blank">S3 Data Integrity</a> </p><table> <tbody> <tr> <td> </td> <td><code>ETag</code> </td> <td><p><em>The entity tag is a hash of the object. The ETag reflects changes only to the contents of an object, not its metadata. The ETag may or may not be an MD5 digest of the object data. Whether or not it is depends on how the object was created and how it is encrypted as described below:</em> </p><ul> <li><em>Objects created by the PUT Object, POST Object, or Copy operation, or through the AWS Management Console, and are encrypted by SSE-S3 or plaintext, have ETags that are an MD5 digest of their object data.</em></li> <li><em>Objects created by the PUT Object, POST Object, or Copy operation, or through the AWS Management Console, and are encrypted by SSE-C or SSE-KMS, have ETags that are not an MD5 digest of their object data.</em></li> <li><em>If an object is created by either the Multipart Upload or Part Copy operation, the ETag is not an MD5 digest, regardless of the method of encryption.</em></li> </ul> </td> </tr> </tbody> </table><p>Options B, C &amp; D are wrong as they do not enable verify the object data integrity. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121402">AWS BDS-C00 Question feedback</a> </p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 31 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251496"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You have a JSON data file in S3 that you are attempting to load into a JavaScript visualization you are writing locally. This visualization makes an HTTP GET request to the S3 location that fails. However, when you attempt to visit the URL being requested by the JavaScript directly from inside your browser, it seems to be loading fine. You are also using a private/incognito window and are not signed into the AWS console. What is the most likely issue?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;The CORS settings are preventing the JavaScript from loading the file.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;The ACLs on the bucket are preventing the JavaScript from loading the file.<br><b>C</b>. <input type="radio" disabled="">&nbsp;The bucket policies are preventing the JavaScript from loading the file.<br><b>D</b>. <input type="radio" disabled="">&nbsp;The IAM role you used to create and upload the JSON data in the S3 bucket is preventing the JavaScript from loading the file. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. The CORS settings are preventing the JavaScript from loading the file.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as CORS in S3 needs to be enabled for the application to be able to access the files. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html" target="_blank">S3 CORS</a> </p><p><em>Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources.</em><br> </p><p>Options B, C &amp; D are wrong as the file is accessible from the browser the ACLs and Bucket Policies cannot be an issue </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23123026">AWS BDS-C00 Question feedback</a></p></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 32 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251497"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You require the ability to analyze a customer’s clickstream data on a website so they can do behavioral analysis. Your customer needs to know what sequence of pages and ads their customer clicked on. This data will be used in real time to modify the page layouts as customers click through the site to increase stickiness and advertising click-through. Which option meets the requirements for capturing and analyzing this data?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Log clicks in weblogs by URL store to Amazon S3, and then analyze with Elastic MapReduce</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Push web clicks by session to Amazon Kinesis and analyze behavior using Kinesis workers<br><b>C</b>. <input type="radio" disabled="">&nbsp;Write click events directly to Amazon Redshift and then analyze with SQL<br><b>D</b>. <input type="radio" disabled="">&nbsp;Publish web clicks by session to an Amazon SQS queue and periodically drain these events to Amazon RDS and analyze with SQL <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Push web clicks by session to Amazon Kinesis and analyze behavior using Kinesis workers<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Key point here is real time data capture and analytics </p><p>Correct answer is <strong>B</strong> as Kinesis helps to collect real time data capture and analyze using kinesis workers </p><p>Option A is wrong as S3 &amp; EMR is not ideal for real time data ingestion and analytics </p><p>Option C is wrong as Redshift is not suitable for real time data ingestion and only allows jdbc/odbc data connection </p><p>Option D is wrong as SQS is not ideal for real time data ingestion. Also periodical analytics is not real time to be able to modify the behavior </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121239">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 33 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251498"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A web-hosting company is building a web analytics tool to capture clickstream data from all of the websites hosted within its platform and to provide near-real-time business intelligence. This entire system is built on AWS services. The web-hosting company is interested in using Amazon Kinesis to collect this data and perform sliding window analytics. What is the most reliable and fault-tolerant technique to get each website to send data to Amazon Kinesis with every click?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;After receiving a request, each web server sends it to Amazon Kinesis using the Amazon Kinesis PutRecord API. Use the sessionID as a partition key and set up a loop to retry until a success response is received.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;After receiving a request, each web server sends it to Amazon Kinesis using the Amazon Kinesis Producer Library addRecords method.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Each web server buffers the requests until the count reaches 500 and sends them to Amazon Kinesis using the Amazon Kinesis PutRecord API.<br><b>D</b>. <input type="radio" disabled="">&nbsp;After receiving a request, each web server sends it to Amazon Kinesis using the Amazon Kinesis PutRecord API. Use the exponential back-off algorithm for retries until a successful response is received. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. After receiving a request, each web server sends it to Amazon Kinesis using the Amazon Kinesis PutRecord API. Use the exponential back-off algorithm for retries until a successful response is received.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as you can use Kinesis <a href="https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecord.html" target="_blank">PutRecord</a> to insert data into Kinesis. To handle failure of PutRecord, AWS recommends using <a href="https://docs.aws.amazon.com/general/latest/gr/api-retries.html">Error Retries and Exponential Backoff in AWS</a>. </p><p><em>The request rate for the stream is too high, or the requested data is too large for the available throughput. Reduce the frequency or size of your requests. For more information, see <a href="https://docs.aws.amazon.com/kinesis/latest/dev/service-sizes-and-limits.html">Streams Limits</a> in the Amazon Kinesis Data Streams Developer Guide, and <a href="https://docs.aws.amazon.com/general/latest/gr/api-retries.html">Error Retries and Exponential Backoff in AWS</a> in the AWS General Reference.</em><em></em><br> </p><p>Option A is wrong as AWS recommends using exponential backoff instead of loop retries. </p><p>Option B is wrong as KPL addRecords does not exist.<br> </p><p>Option C is wrong as with batching you need to use <a href="https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html" target="_blank">PutRecords</a>.<br> </p><p><em>Writes multiple data records into a Kinesis data stream in a single call (also referred to as a <code>PutRecords</code> request). Use this operation to send data into the stream for data ingestion and processing.</em> </p><p><em>Each <code>PutRecords</code> request can support up to 500 records. Each record in the request can be as large as 1 MiB, up to a limit of 5 MiB for the entire request, including partition keys. Each shard can support writes up to 1,000 records per second, up to a maximum data write total of 1 MiB per second.</em> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121415">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 34 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251499"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A medical record filing system for a government medical fund is using an Amazon S3 bucket to archive documents related to patients. Every patient visit to a physician creates a new file, which can add up millions of files each month. Collection of these files from each physician is handled via a batch process that runs everу night using AWS Data Pipeline. This is sensitive data, so the data and any associated metadata must be encrypted at rest. Auditors review some files on a quarterly basis to see whether the records are maintained according to regulations. Auditors must be able to locate any physical file in the S3 bucket for a given date, patient, or physician. Auditors spend a significant amount of time location such files. What is the most cost and time efficient collection methodology in this situation?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Use Amazon Kinesis to get the data feeds directly from physicians, batch them using a Spark application on Amazon Elastic MapReduce (EMR), and then store them in Amazon S3 with folders separated per physician.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use Amazon API Gateway to get the data feeds directly from physicians, batch them using a Spark application on Amazon Elastic MapReduce (EMR), and then store them in Amazon S3 with folders separated per physician.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Use Amazon S3 event notification to populate an Amazon DynamoDB table with metadata about every file loaded to Amazon S3, and partition them based on the month and year of the file.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use Amazon S3 event notification to populate an Amazon Redshift table with metadata about every file loaded to Amazon S3, and partition them based on the month and year of the file. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Use Amazon S3 event notification to populate an Amazon DynamoDB table with metadata about every file loaded to Amazon S3, and partition them based on the month and year of the file.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as the S3 even notification can be used to populate DynamoDB with the metadata of the file like physician, patient and date. This does not impact the current process and provides and easy way for the auditors to query the data. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/blogs/big-data/building-and-maintaining-an-amazon-s3-metadata-index-without-servers/" target="_blank">S3 Maintain Metadata</a> </p><p><em>Amazon S3 is a simple key-based object store whose scalability and low cost make it ideal for storing large datasets. Its design enables S3 to provide excellent performance for storing and retrieving objects based on a known key. Finding objects based on other attributes, however, requires doing a linear search using the LIST operation. Because each listing can return at most 1000 keys, it may require many requests before finding the object. Because of these additional requests, implementing attribute-based queries in S3 alone can be challenging.<br> </em> </p><p><em>A common solution is to build an external index that maps queryable attributes to the S3 object key. This index can leverage data repositories that are built for fast lookups but might not be great at storing large data blobs. These types of indexes provide an entry point to your data that can be used by a variety of systems. </em> </p><p>Option A is wrong as it is not a time efficient way and would need new development with EMR cost included. </p><p>Option B is wrong as API Gateway is not an ideal interface with Spark applications.<br> </p><p>Option D is wrong as Redshift is more an option for analytics and would not be as cost efficient as compared for DynamoDB as it is a managed service.<br> </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%203%20-%20%23121345">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 35 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251500"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You are designing a service that aggregates clickstream data in batch and delivers reports to subscribers via email only once per week. Data is extremely spikey, geographically distributed, high-scale, and unpredictable. How should you design this system?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Use a large Redshift cluster to perform the analysis, and a fleet of Lambdas to perform record inserts into the Redshift tables. Lambda will scale rapidly enough for the traffic spikes.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use a CloudFront distribution with access log delivery to S3. Clicks should be recorded as query string GETs to the distribution. Reports are built and sent by periodically running EMR jobs over the access logs in S3.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Use API Gateway invoking Lambdas which PutRecords into Kinesis, and EMR running Spark performing GetRecords on Kinesis to scale with spikes. Spark on EMR outputs the analysis to S3, which are sent out via email.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use AWS Elasticsearch service and EC2 Auto Scaling groups. The Autoscaling groups scale based on click throughput and stream into the Elasticsearch domain, which is also scalable. Use Kibana to generate reports periodically. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Use a CloudFront distribution with access log delivery to S3. Clicks should be recorded as query string GETs to the distribution. Reports are built and sent by periodically running EMR jobs over the access logs in S3.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Key point here is clickstream batch analysis for large global data only once per week </p><p>Correct answer is <strong>B</strong> as CloudFront is a Gigabit-Scale HTTP(S) global request distribution service and works fine with peaks higher than 10 Gbps or 15,000 RPS. It can handle scale, geo-spread, spikes, and unpredictability. Access Logs will contain the GET data. EMR can be used for batch analysis </p><p>Other streaming options are expensive as not required as the need is to batch analyze </p><p>Option A is wrong as Redshift is more of Data Warehousing solution and with lambda not needed as it is more of an batch analytics solution </p><p>Option C is wrong as Lambdas with Kinesis is more for streaming real time data. </p><p>Option D is wrong as Elasticsearch is more of an search solution and not batch analytics </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121271">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 36 of 50 </b><small>- Multiple Answer</small> <span style="float:right;" class="grad_5251501"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You have to design an EMR system where you will be processing highly confidential data. What can you do to ensure encryption of data at rest?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="checkbox" checked="true" disabled="">&nbsp;SSE-KMS</span><br><b>B</b>. <input type="checkbox" disabled="">&nbsp;VPN<br><b>C</b>. <input type="checkbox" disabled="">&nbsp;TLS<br><b>D</b>. <input type="checkbox" disabled="">&nbsp;LUKS <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. SSE-KMS<br><b>D</b>. LUKS<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answers are <strong>A &amp; D</strong> as SSE-KMS and LUKS can be used for implemented encryption at rest. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html" target="_blank">EMR Encryption</a> </p><p><img src="./bds3_files/emr-encryption.png"><br><em>Amazon S3 encryption works with EMR File System (EMRFS) objects read from and written to Amazon S3. You specify Amazon S3 server-side encryption (SSE) or client-side encryption (CSE) when you enable at-rest encryption. Amazon S3 SSE and CSE encryption with EMRFS are mutually exclusive; you can choose either but not both. Regardless of whether Amazon S3 encryption is enabled, Transport Layer Security (TLS) encrypts the EMRFS objects in-transit between EMR cluster nodes and Amazon S3. <br></em> </p><p><em><strong>SSE-KMS</strong>: You use an AWS KMS customer master key (CMK) set up with policies suitable for Amazon EMR.<br></em> </p><p><em><strong>LUKS</strong>. In addition to HDFS encryption, the Amazon EC2 instance store volumes and the attached Amazon EBS volumes of cluster instances are encrypted using LUKS.</em><br> </p><p>Option B is wrong as VPN just provides connectivity between On-premises and AWS. </p><p>Option C is wrong TLS provides in transit encryption </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23123035">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 37 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251502"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An Amazon EMR cluster using EMRFS has access to petabytes of data on Amazon S3, originating from multiple unique data sources. The customer needs to query common fields across some of the data sets to be able to perform interactive joins and then display results quickly. Which technology is most appropriate to enable this capability?<br><div class="pull-left"><div class="label-success label" style="padding: .2em .6em .3em;"> Correct </div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Presto</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;MicroStrategy<br><b>C</b>. <input type="radio" disabled="">&nbsp;Pig<br><b>D</b>. <input type="radio" disabled="">&nbsp;R Studio <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>A</b>. Presto<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>A</strong> as Presto can help perform interactive analysis and its performance is much better than Pig as it uses a custom query execution engine </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/emr/features/presto/" target="_blank">EMR Presto</a> </p><p><em><a href="https://aws.amazon.com/big-data/what-is-presto/">Presto</a> is an open-source distributed SQL query engine optimized for low-latency, ad-hoc analysis of data. It supports the ANSI SQL standard, including complex queries, aggregations, joins, and window functions. Presto can process data from multiple data sources including the Hadoop Distributed File System (HDFS) and Amazon S3.<br></em> </p><p><em>Presto uses a custom query execution engine with operators designed to support SQL semantics. Different from Hive/MapReduce, Presto executes queries in memory, pipelined across the network between stages, thus avoiding unnecessary I/O. The pipelined execution model runs multiple stages in parallel and streams data from one stage to the next as it becomes available. </em><em></em><br> </p><p>Options C is wrong Pig is based on Map Reduce execution and more ideal for batch processing. </p><p>Options B &amp; D are wrong as they do not run with EMR. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121386">AWS BDS-C00 Question feedback</a></div><p><b>Points : </b> 5 out of 5 </p></div></div><div class="question_info"><div class="question_no"><b>Question : 38 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251503"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A customer needs to determine the optimal distribution strategy for the ORDERS fact table in its Redshift schema. The ORDERS table has foreign key relationships with multiple dimension tables in this schema. How should the company determine the most appropriate distribution key for the ORDERS table?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Identify the largest and most frequently joined dimension table and ensure that it and the ORDERS table both have EVEN distribution.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Identify the largest dimension table and designate the key of this dimension table as the distribution key of the ORDERS table.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Identify the smallest dimension table and designate the key of this dimension table as the distribution key of the ORDERS table.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Identify the largest and the most frequently joined dimension table and designate the key of this dimension table as the distribution key of the ORDERS table. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Identify the largest and the most frequently joined dimension table and designate the key of this dimension table as the distribution key of the ORDERS table.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as you should choose the largest and the most frequently joined dimension table and use the key of the dimension table as the distribution key. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/blogs/big-data/optimizing-for-star-schemas-and-interleaved-sorting-on-amazon-redshift/">Redshift Optimizing Star Schema</a> </p><p><em>Using distribution keys is a good way to optimize the performance of Amazon Redshift when you use a star schema. With an EVEN distribution, data is spread equally across all nodes in the cluster to ensure balanced processing. In many cases, simply distributing data equally using EVEN does not optimize performance as the data rows on a node for the table do not have any affinity with each other. Take an example of a fact table for ORDERS where a distribution style of EVEN is chosen. In this case, the orders for a specific customer are potentially spread across many compute nodes in the cluster. However, if the table had a distribution style of KEY and a DISTKEY of customer_id was chosen, then all of the orders for a particular customer would be stored on the same compute node. Using a distribution style of EVEN can lead to more cross-node traffic.</em> </p><p><em>A good selection for a distribution key distributes data relatively evenly across nodes while collocating related data on a compute node used in joins or aggregates. When you perform a join on a column that is a distribution key for both tables, Amazon Redshift is able to run the join locally on each node with no inter-node data movement; this is because rows with the same distribution key value reside on the same node for both tables in the join. Similarly, aggregating on a distribution key performs better because the data for the aggregate column value is local to the compute node.</em> </p><p><em>In a typical star schema, the fact table has foreign key relationships with multiple dimension tables, so you need to choose one of the dimensions. You would choose the foreign key for the largest frequently joined dimension as a distribution key in the fact table and the primary key in the dimension table. Make sure that the distribution keys chosen result in relatively even distribution for both tables, and if the distribution is skewed, use a different dimension. Then analyze the remaining dimensions to determine if a distribution style of ALL, KEY, or EVEN is appropriate.</em> </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121392">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 39 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251504"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An organization uses a custom map reduce application to build monthly reports based on many small data files in an Amazon S3 bucket. The data is submitted from various business units on a frequent but unpredictable schedule. As the dataset continues to grow, it becomes increasingly difficult to process all of the data in one day. The organization has scaled up its Amazon EMR cluster, but other optimizations could improve performance. The organization needs to improve performance with minimal changes to existing processes and applications. What action should the organization take?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Use Amazon S3 Event Notifications and AWS Lambda to create a quick search file index in DynamoDB.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Add Spark to the Amazon EMR cluster and utilize Resilient Distributed Datasets in-memory.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Use Amazon S3 Event Notifications and AWS Lambda to index each file into an Amazon Elasticsearch Service cluster.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Schedule a daily AWS Data Pipeline process that aggregates content into larger files using S3DistCp.<br><b>E</b>. <input type="radio" disabled="">&nbsp;Have business units submit data via Amazon Kinesis Firehose to aggregate data hourly into Amazon S3. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Schedule a daily AWS Data Pipeline process that aggregates content into larger files using S3DistCp.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as the focus is to improve performance with minimal changes. S3DistCp can be used to aggregate smaller files to large ones without any change to the existing applications and processes. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/blogs/big-data/seven-tips-for-using-s3distcp-on-amazon-emr-to-move-data-efficiently-between-hdfs-and-amazon-s3/#5" target="_blank">S3DistCp Best Practices</a> </p><p><em>Hadoop is optimized for reading a fewer number of large files rather than many small files, whether from S3 or HDFS. You can use S3DistCp to aggregate small files into fewer large files of a size that you choose, which can optimize your analysis for both performance and cost.</em><br> </p><p>Options A &amp; C are wrong as it would not help to improve the performance </p><p>Option B is wrong as it would need a change in the current processing logic. </p><p>Option E is wrong as it might work, however, you would need to change the applications to submit data to Kinesis Firehose. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121340">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 40 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251505"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company uses Amazon Redshift for its enterprise data warehouse. A new on-premises PostgreSQL OLTP DB must be integrated into the data warehouse. Each table in the PostgreSQL DB has an indexed last_modified timestamp column. The data warehouse has a staging layer to load source data into the data warehouse environment for further processing. The data lag between the source PostgreSQL DB and the Amazon Redshift staging layer should NOT exceed four hours. What is the most efficient technique to meet these requirements?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 9 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Create a DBLINK on the source DB to connect to Amazon Redshift. Use a PostgreSQL trigger on the source table to capture the new insert/update/delete event and execute the event on the Amazon Redshift staging table.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use a PostgreSQL trigger on the source table to capture the new insert/update/delete event and write it to Amazon Kinesis Streams. Use a KCL application to execute the event on the Amazon Redshift staging table.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Extract the incremental changes periodically using a SQL query. Upload the changes to multiple Amazon Simple Storage Service (S3) objects, and run the COPY command to load to the Amazon Redshift staging layer.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Extract the incremental changes periodically using a SQL query. Upload the changes to a single Amazon Simple Storage Service (S3) object, and run the COPY command to load to the Amazon Redshift staging layer. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Extract the incremental changes periodically using a SQL query. Upload the changes to multiple Amazon Simple Storage Service (S3) objects, and run the COPY command to load to the Amazon Redshift staging layer.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as the requirement is not to have a real time change replication, the incremental data can be retrieved and upload to S3 as multiple objects. COPY commands would help load the data into Redshift staging layer taking advantage of parallelism with multiple files. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/redshift/latest/dg/t_Loading-data-from-S3.html" target="_blank">Redshift Loading data from S3</a> </p><p><em>COPY command leverages the Amazon Redshift massively parallel processing (MPP) architecture to read and load data in parallel from files in an Amazon S3 bucket. You can take maximum advantage of parallel processing by splitting your data into multiple files and by setting distribution keys on your tables. </em><br> </p><p>Option D is wrong as a single file would not support parallelism. </p><p>Options A &amp; B are wrong as there is no need for load near time data as the lag needs be less than 4 hrs and teh DB table support last_modified column. Also, <a href="https://forums.aws.amazon.com/message.jspa?messageID=807709" target="_blank">DBLink works good for select but not much with writes</a>. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121347">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 41 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251506"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> An organization needs a data store to handle the following data types and access patterns: Faceting Search Flexible schema (JSON) and fixed schema Noise word elimination Which data store should the organization choose? <br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Amazon DynamoDB</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Amazon Relational Database Service (RDS)<br><b>C</b>. <input type="radio" disabled="">&nbsp;Amazon Elasticsearch Service<br><b>D</b>. <input type="radio" disabled="">&nbsp;Amazon Redshift <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. Amazon Elasticsearch Service<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as Elasticsearch provides all the listed features. It provides full text search, faceting, noise words or also known as stopwords and a flexible schema option. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/elasticsearch-service/" target="_blank">Elasticsearch</a> </p><p><em>Amazon Elasticsearch Service is a fully managed service that makes it easy for you to deploy, secure, and operate Elasticsearch at scale with zero down time.<br></em> </p><p><em>Provide a low-latency, high-throughput, personalized search experience for your users across e-commerce applications, website, data lake catalogs, and other curated application data. Amazon Elasticsearch Service provides direct access to all of Elasticsearch’s rich search APIs, supporting natural language search across free text, Boolean combinations of text and metadata search, auto-completion, faceted search, location-aware search, and much more.</em><em></em><br> </p><p>Options A, B &amp; D are wrong as they do not cover all the requirements. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%203%20-%20%23121359">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 42 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251507"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A data engineer is running a DWH on a 25-node Redshift cluster of a SaaS service. The data engineer needs to build a dashboard that will be used by customers. Five big customers represent 80% of usage, and there is a long tail of dozens of smaller customers. The data engineer has selected the dashboarding tool. How should the data engineer make sure that the larger customer workloads do NOT interfere with the smaller customer workloads?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 6 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <b>A</b>. <input type="radio" disabled="">&nbsp;Apply query filters based on customer-id that can NOT be changed by the user and apply distribution keys on customer-id.<br><b>B</b>. <input type="radio" disabled="">&nbsp;Place the largest customers into a single user group with a dedicated query queue and place the rest of the customers into a different query queue. <br><b>C</b>. <input type="radio" disabled="">&nbsp;Push aggregations into an RDS for Aurora instance. Connect the dashboard application to Aurora rather than Redshift for faster queries.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Route the largest customers to a dedicated Redshift cluster. Raise the concurrency of the multi-tenant Redshift cluster to accommodate the remaining customers. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Place the largest customers into a single user group with a dedicated query queue and place the rest of the customers into a different query queue. <br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as Redshift allows Workload Management (WLM) to help define queues. In this case, a dedicated Queue can be defined for large customer and other for rest of customers ensuring the queries from them do not interfere with large customers. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/redshift/latest/dg/cm-c-implementing-workload-management.html" target="_blank">Redshift Workload Management</a> </p><p><em>You can use workload management (WLM) to define multiple query queues and to route queries to the appropriate queues at run time.</em> </p><p><em>In some cases, you might have multiple sessions or users running queries at the same time. In these cases, some queries might consume cluster resources for long periods of time and affect the performance of other queries. For example, suppose that one group of users submits occasional complex, long-running queries that select and sort rows from several large tables. Another group frequently submits short queries that select only a few rows from one or two tables and run in a few seconds. In this situation, the short-running queries might have to wait in a queue for a long-running query to complete.</em> </p><p><em>Alternatively, you can manage system performance and your users' experience by modifying your WLM configuration to create separate queues for the long-running queries and the short-running queries. At run time, you can route queries to these queues according to user groups or query groups. You can enable this manual configuration using the Amazon Redshift console by switching to <strong>Manual WLM</strong>. With this choice, you specify the queues used to manage queries, and the <strong>Memory</strong> and <strong>Concurrency on main</strong> field values.</em> </p><p><em>With a manual configuration, you can configure up to eight query queues and set the number of queries that can run in each of those queues concurrently. You can set up rules to route queries to particular queues based on the user running the query or labels that you specify. You can also configure the amount of memory allocated to each queue, so that large queries run in queues with more memory than other queues. You can also configure the WLM timeout property to limit long-running queries.</em> </p><p>Options A &amp; C are wrong as it does not prevent the queries from interfering with each other. </p><p>Option D is wrong as it would lead to duplication with a dedicated Redshift cluster. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121389">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 43 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251508"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A new algorithm has been written in Python to identify SPAM e-mails. The algorithm analyzes the free text contained within a sample set of 1 million e-mails stored on Amazon S3. The algorithm must be scaled across a production dataset of 5 PB, which also resides in Amazon S3 storage. Which AWS service strategy is best for this use case?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Copy the data into Amazon ElastiCache to perform text analysis on the in-memory data and export the results of the model into Amazon Machine Learning.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use Amazon EMR to parallelize the text analysis tasks across the cluster using a streaming program step.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Use Amazon Elasticsearch Service to store the text and then use the Python Elasticsearch Client to run analysis against the text index.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Initiate a Python job from AWS Data Pipeline to run directly against the Amazon S3 text files. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Use Amazon EMR to parallelize the text analysis tasks across the cluster using a streaming program step.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as the data is huge EMR can be used to parallelly analyse the data using Streaming program which supports python. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/UseCase_Streaming.html" target="_blank">EMR Process Data with Streaming</a> </p><p><em>A Streaming application reads input from standard input and then runs a script or executable (called a mapper) against each input. The result from each of the inputs is saved locally, typically on a Hadoop Distributed File System (HDFS) partition. After all the input is processed by the mapper, a second script or executable (called a reducer) processes the mapper results. The results from the reducer are sent to standard output. You can chain together a series of Streaming steps, where the output of one step becomes the input of another step.<br> </em> </p><p><em>The mapper and the reducer can each be referenced as a file or you can supply a Java class. You can implement the mapper and reducer in any of the supported languages, including Ruby, Perl, Python, PHP, or Bash. </em> </p><p>Option A is wrong as ElastiCache is for caching and does not perform text analysis </p><p>Option C is wrong as the data would be needed to be loaded into Elasticsearch and would take a long time </p><p>Option D is wrong as AWS Data Pipeline cannot run job directly on S3 text files.<br> </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121331">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 44 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251509"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A customer needs to load a 550-GB data file into an Amazon Redshift cluster from Amazon S3, using the COPY command. The input file has both known and unknown issues that will probably cause the load process to fail. The customer needs the most efficient way to detect load errors without performing any cleanup if the load process fails. Which technique should the customer use?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 6 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Split the input file into 50-GB blocks and load them separately.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use COPY with NOLOAD parameter.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Write a script to delete the data from the tables in case of errors.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Compress the input file before running COPY. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Use COPY with NOLOAD parameter.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as <em>NOLOAD checks the integrity of all of the data without loading it into the database. The NOLOAD option displays any errors that would occur if you had attempted to load the data. All other options will require subsequent processing on the cluster which will consume resources.</em> </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-load.html#copy-noload" target="_blank">Data Load Copy Parameters</a> </p><p><em>If you want to validate your data without actually loading the table, use the NOLOAD option with the <a href="https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html">COPY</a> command.</em><br> </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121317">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 45 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251510"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You've been asked to select a tool that can easily visualize sales data that comes in as JSON to S3, occasionally as ad-hoc CSV files, and even from the Amazon Redshift data warehouse. The solution must allow multiple users from the finance department to easily access it and occasionally upload their own Excel spreadsheets to compare with existing data. What solution do you recommend?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Use Kibana and a combination of an S3 bucket that accepts the XLSX downloads and processes them with Lambda to transform them into JSON and index them in Elasticsearch.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use Kibana and Amazon Athena to process the S3 data and XLSX files before indexing them in Elasticsearch.<br><b>C</b>. <input type="radio" disabled="">&nbsp;QuickSight and a combination of data source connections with the Redshift cluster and existing S3 JSON documents while still allowing finance to upload Excel files directly.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use QuickSight and a combination of data source connections with the Redshift cluster and existing S3 JSON documents along with a Lambda function to process the XLSX files and transform them into a QuickSight-readable format. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>C</b>. QuickSight and a combination of data source connections with the Redshift cluster and existing S3 JSON documents while still allowing finance to upload Excel files directly.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>C</strong> as QuickSight can provide visualization with out of box integration with Redshift and S3 JSON documents as well as handle Excel files. </p><p>Refer AWS documentation - <a href="https://aws.amazon.com/quicksight/features/" target="_blank">QuickSight</a> </p><p><em>QuickSight allows you to directly connect to and import data from a wide variety of cloud and on-premises data sources. These include SaaS applications such as Salesforce, Square, ServiceNow, Twitter, Github, and JIRA; 3rd party databases such as Teradata, MySQL, Postgres, and SQL Server; native AWS services such as Redshift, Athena, S3, RDS, and Aurora; and private VPC subnets. You can also upload a variety of file types including Excel, CSV, JSON, and Presto.</em><span class="redactor-invisible-space"><em></em><br></span> </p><p>Options A &amp; B are wrong as Kibana is not a managed service and needs to be maintained. Also, the integrations with other AWS services need to be checked. </p><p>Option D is wrong as QuickSight can handle Excel files directly and would not need any Lambda functions. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23123012">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 46 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251511"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A customer is collecting clickstream data using Amazon Kinesis and is grouping the events by IP address into 5-minute chunks stored in Amazon S3. Many analysts in the company use Hive on Amazon EMR to analyze this data. Their queries always reference a single IP address. Data must be optimized for querying based on IP address using Hive running on Amazon EMR. What is the most efficient method to query the data with Hive?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Store an index of the files by IP address in the Amazon DynamoDB metadata store for EMRFS.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Store the Amazon S3 objects with the following naming scheme bucket_name/source=ip_address/year=yy/month=mm/day=dd/hour=hh/filename. <br><b>C</b>. <input type="radio" disabled="">&nbsp;Store the data in an HBase table with the IP address as the row key.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Store the events for an IP address as a single file in Amazon S3 and add metadata with keys:Hive_Partitioned_IPAddress. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Store the Amazon S3 objects with the following naming scheme bucket_name/source=ip_address/year=yy/month=mm/day=dd/hour=hh/filename. <br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as you can create an external table with dynamic partitioning enabled and point to S3. Partitioning on ip_address and dates would help in efficient queries.<br></p><p>Option A is wrong as EMR hive only supports RDS MySQL, Aurora and Glue Data Catalog as external metadata store. Refer <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-glue.html" target="_blank">EMR Hive Metastore</a></p><p>Option C is wrong as HBase on S3 was not supported before and has been supported only lately.<br></p><p>Option D is wrong as data cannot be stored as a single file in S3. Also, adding metadata does not improve the querying performance. </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121337">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 47 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251512"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A clinical trial will rely on medical sensors to remotely assess patient health. Each physician who participates in the trial requires visual reports each morning. The reports are built from aggregations of all the sensor data taken each minute. What is the most cost-effective solution for creating this visualization each day?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 6 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Use Kinesis Aggregators Library to generate reports for reviewing the patient sensor data and generate a QuickSight visualization on the new data each morning for the physician to review.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Use a transient EMR cluster that shuts down after use to aggregate the patient sensor data each night and generate a QuickSight visualization on the new data each morning for the physician to review.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Use Spark streaming on EMR to aggregate the patient sensor data in every 15 minutes and generate a QuickSight visualization on the new data each morning for the physician to review.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use an EMR cluster to aggregate the patient sensor data each night and provide Zeppelin notebooks that look at the new data residing on the cluster each morning for the physician to review. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Use a transient EMR cluster that shuts down after use to aggregate the patient sensor data each night and generate a QuickSight visualization on the new data each morning for the physician to review.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as a transient cluster can be used to aggregate the data and use QuickSight for visualization.<br></p><p>Option A is wrong as <a href="https://github.com/awslabs/amazon-kinesis-aggregators" target="_blank">Kinesis Aggregators</a> runs on EB, EC2 and Kinesis enabled application which would need to be managed and scaled. Also, Amazon Kinesis Aggregators is a Java framework that enables the automatic creation of real-time aggregated time series data from Amazon Kinesis streams.</p><p>Option C is wrong as running every 15 minutes would not be cost-efficient. Also the EMR cluster is not transient.</p><p>Option D is wrong as it does not mention its a transient cluster and would be expensive to have a persistent cluster.</p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121405">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 48 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251513"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> A company is looking to collect and process the log files in near real time that are generated from thousands of applications in their AWS cloud. They are also collecting stock pricing information from stock price publishing data providers and using the information to recommend stocks to customers. They are looking at querying streams and using Kinesis Analytics application to process all the stocks for recommendation if price changes greater than 10 percent. What kind of Queries will help fulfil the requirement?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 5 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Stagger Windows queries</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Tumbling Windows queries<br><b>C</b>. <input type="radio" disabled="">&nbsp;Sliding windows queries<br><b>D</b>. <input type="radio" disabled="">&nbsp;Continuous queries <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Continuous queries<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as <a href="https://docs.aws.amazon.com/kinesisanalytics/latest/dev/continuous-queries-%20concepts.html" target="_blank">Continuous Query</a> can provide the ability to monitor, query and generate alerts on a stream. Continuous Query<span class="redactor-invisible-space"> is a query over a stream executes continuously over streaming data. This continuous execution enables scenarios, such as the ability for applications to continuously query a stream and generate alerts. </span> </p><p>Option A is wrong as <a href="https://docs.aws.amazon.com/kinesisanalytics/latest/dev/stagger-window-%20concepts.html" target="_blank">Stagger windows</a> query aggregates data using keyed time-based windows that open as data arrives. The keys allow for multiple overlapping windows. This is the recommended way to aggregate data using time-based windows </p><p>Option B is wrong as <a href="https://docs.aws.amazon.com/kinesisanalytics/latest/dev/tumbling-window-%20concepts.html" target="_blank">Tumbling Windows</a> query, A query that aggregates data using distinct time-based windows that open and close at regular intervals. </p><p>Option C is wrong as <a href="https://docs.aws.amazon.com/kinesisanalytics/latest/dev/sliding-window-concepts.html" target="_blank">Sliding windows</a> query aggregates data continuously, using a fixed time or row count interval. </p> <a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%203%20-%20%23125347">AWS BDS-C00 Question feedback</a></div></div></div><div class="question_info"><div class="question_no"><b>Question : 49 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251514"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You have been asked to cost optimize a business critical and long-running EMR cluster. The EMR cluster is currently on-demand for the master nodes, core nodes and task nodes. The costs for running the cluster have been steadily increasing as nodes have been added and resized. What would you suggest the business does to reduce the costs without requiring any long-term commitment?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 4 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <span class="bgcolor"><b>A</b>. <input type="radio" checked="true" disabled="">&nbsp;Recreate the cluster using spot instances for the master, core and task nodes.</span><br><b>B</b>. <input type="radio" disabled="">&nbsp;Leave the master node to use on-demand and change the core and task nodes to spot<br><b>C</b>. <input type="radio" disabled="">&nbsp;Leave all nodes running on-demand instances, the cluster is already cost optimized.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Leave the master and core nodes as on-demand and use spot instances for the task nodes <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>D</b>. Leave the master and core nodes as on-demand and use spot instances for the task nodes<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>D</strong> as AWS recommends using reserved or on-demand instances for Master and Core nodes. Task nodes can use spot instances to improve performance and reduce cost. </p><p>Refer AWS documentation - <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html" target="_blank">EMR Plan Instances</a> </p><p><em>The master node controls and directs the cluster. When it terminates, the cluster ends, so you should only launch the master node as a Spot Instance if you are running a cluster where sudden termination is acceptable. This might be the case if you are testing a new application, have a cluster that periodically persists data to an external store such as Amazon S3, or are running a cluster where cost is more important than ensuring the cluster’s completion.<br></em> </p><p><em>Core nodes process data and store information using HDFS. Terminating a core instance risks data loss. For this reason, you should only run core nodes on Spot Instances when partial HDFS data loss is tolerable.<br></em> </p><p><em>The task nodes process data but do not hold persistent data in HDFS. If they terminate because the Spot price has risen above your maximum Spot price, no data is lost and the effect on your cluster is minimal.</em><br> </p><p>Option A is wrong as running all on spot instances would result in data loss and cluster availability issues. </p><p>Option B is wrong as Core nodes should not be run on spot instances as there is a data loss issue.<br> </p><p>Option C is wrong as On-demand nodes for all would not provide the best cost optimization.<br> </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%201%20-%20%23121264">AWS BDS-C00 Question feedback</a></p></div></div></div><div class="question_info"><div class="question_no"><b>Question : 50 of 50 </b><small>- Multiple Choice</small> <span style="float:right;" class="grad_5251515"> <span class="label label-success">Graded</span> </span> </div><div class="question_detail"> You need to design a solution that can return user profile data to your application with millisecond latency or better and that can store this information for two million users. Currently, user profile information is capped at 15 KB and typically doesn't exceed 3 KB. It is very important that this profile information always reflects the most recent changes when read. You also want a way to process changes from user profiles and potentially send out emails when specific types of changes are made but the amount of compute capacity required for this is highly sporadic. What could you use to help build a cost-effective solution to your application?<br><div class="pull-left"><div class="label-danger label" style="padding: .2em .6em .3em;">Incorrect</div></div><div class="pull-right"><b style="font-size: 15px;">Time spent : 6 sec </b></div><br><br> <b>Your answer</b><br><div style="font-size: 15px;"> <b>A</b>. <input type="radio" disabled="">&nbsp;Use separate S3 objects to store user profile information and run ECS jobs periodically hash the files and compare them to a RDS database of object hashes to determine if they need to be processed and emails might need to be sent out using SES.<br><b>B</b>. <input type="radio" disabled="">&nbsp;Use a DynamoDB table to store profile information as items and then enable DynamoDB streams on the table. Whenever changes are made, evaluate them with a Lambda function and send an email with SES if that is appropriate.<br><b>C</b>. <input type="radio" disabled="">&nbsp;Use a DynamoDB table to store profile information as items and then enable DynamoDB streams on the table. Write an application that sits on a cluster of EC2 Reserved Instances and use that to process the DynamoDB streams data and send emails using SES when appropriate.<br><b>D</b>. <input type="radio" disabled="">&nbsp;Use separate S3 objects to store user profile information and use AWS Lambda functions that trigger whenever objects are updated. Then send the customized HTML emails out from SNS to users if appropriate. <br><br> </div> <b>Correct Answer</b><br><div style="font-size: 15px;"> <b>B</b>. Use a DynamoDB table to store profile information as items and then enable DynamoDB streams on the table. Whenever changes are made, evaluate them with a Lambda function and send an email with SES if that is appropriate.<br> <br> </div> <b>Explanation</b><br><div style="font-size: 15px;"><p>Correct answer is <strong>B</strong> as DynamoDB can store the data and provide low latency and strongly consistent reads of the user profiles data. DynamoDB streams can help detect any changes on the data and process them using Lambda and notify. </p><p><img src="./bds3_files/125235.png"><br> </p><p><span class="redactor-invisible-space"></span> </p><p><span class="redactor-invisible-space">Option C is wrong as using EC2 Reserved Instances needs long term commitment and would not be as cost-effective as Lambda.</span> </p><p>Options A &amp; D are wrong as <span class="redactor-invisible-space">S3 is not an ideal storage option for user profile data and it would not provide low latency access nor the ability to update the data.</span> </p><p><a href="mailto:certification.exam.tests@gmail.com?subject=AWS%20BDS-C00%20Practice%20Test%203%20-%20%23125235">AWS BDS-C00 Question feedback</a> </p></div></div></div><div class="col-sm-12 well"><h4 class="sm" style="color:#333;">9/50 Questions right</h4></div></div><style type="text/css">
span.label.label-danger,span.label.label-success {
    padding: .2em .6em .3em;
}
.saved{color:red; }
.question_info{
  margin-bottom: 40px;
  border: solid 1px #ccc;
  border-radius: 5px;
  overflow: hidden;
}
.question_no {
    background: #f4f4f4;
    padding: 0 15px;
    line-height: 40px;
    border-bottom: solid 1px #ccc;
}

.question_detail {
    padding: 10px;
}
.hide{
  display: none;
}
input[type="radio"]{
  -webkit-appearance: radio;
}
input[type="checkbox"]{
  -webkit-appearance: checkbox;
}
span.bgcolor {
    background: yellow;
    padding: 5px;
    margin-left: -5px;
}
</style><link href="./bds3_files/mcoursestyle.css" rel="stylesheet"></div> <script type="text/javascript" src="./bds3_files/bc-course.min_031117.js.下载"></script> <div class="overlayForm" style=""></div></div></div></div></div></div><div class="overlayForm"></div></div><iframe style="position:absolute;left:-999px;top:-999px;visibility:hidden" src="./bds3_files/saved_resource.html"></iframe><iframe style="display: none; visibility: hidden;" src="./bds3_files/saved_resource(1).html"></iframe><div style="position:absolute;top:-300px;left:-40px;width:1px;height:1px;"><iframe id="__l1__l" style="top:-300px;left:-400px;border:0px solid transparent !important;width:1px !important;height:1px !important;display:none;" seamless="" height="1px" border="0px" width="1px" src="./bds3_files/sto.html"></iframe></div></body></html>